{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggrHC9NPMIaa"
      },
      "source": [
        "#Imports and Data Classes for all Parts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tDCs6Hfo1nA7",
        "outputId": "9546c0aa-c05d-4eb7-a6f2-6a0d31f4d3d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.10/dist-packages (1.4.0.post0)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.25.2)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (24.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.3.0+cu121)\n",
            "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (0.11.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (67.7.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.12.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.14.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->torchmetrics) (12.5.40)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->torchmetrics) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->torchmetrics) (1.3.0)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#Imports\n",
        "\n",
        "import time\n",
        "import os\n",
        "import random\n",
        "import types\n",
        "from matplotlib import pyplot as plt\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "# -----------------------------------------------------------------\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "# -----------------------------------------------------------------\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import ToTensor\n",
        "# -----------------------------------------------------------------\n",
        "!pip install torchmetrics\n",
        "from torchmetrics import F1Score\n",
        "from torchmetrics import ConfusionMatrix\n",
        "from torchmetrics import Accuracy\n",
        "# -----------------------------------------------------------------\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWml-o8l6N82",
        "outputId": "9cee29e3-e44c-4ef9-b793-df1c8218b481"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "device = cpu\n"
          ]
        }
      ],
      "source": [
        "#Data in CPU for questions 1-5\n",
        "device=\"cpu\"\n",
        "print (\"device =\",device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "SAv7kFWWyimV"
      },
      "outputs": [],
      "source": [
        "#Dataset Class to process our input files for use with the neural network\n",
        "\n",
        "class dataset(Dataset):\n",
        "  def __init__(self,data, labels,labels_map,transform=None):\n",
        "    self.transform = transform\n",
        "    self.labels = []\n",
        "    for l in labels:\n",
        "      self.labels.append(labels_map[l])\n",
        "    self.labels = np.array(self.labels)\n",
        "    self.data = data\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.labels)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    sound = self.data[index]\n",
        "    label = self.labels[index]\n",
        "    if self.transform:\n",
        "      sound = self.transform(sound)\n",
        "    return sound, label\n",
        "\n",
        "labels_map = {\n",
        "    \"classical\":0,\n",
        "    \"blues\":1,\n",
        "    \"rock_metal_hardrock\":2,\n",
        "    \"hiphop\":3,\n",
        "}\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhq81urW10VQ"
      },
      "source": [
        "# Part 1 Feedforward Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5Y9kSKcOAa1"
      },
      "source": [
        "## Step 1 Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "p7Zvho8WMDPe"
      },
      "outputs": [],
      "source": [
        "\n",
        "mfcctraindata = np.load('/content/drive/MyDrive/Machine_Learning/music_genre_data_di/train/mfccs/X.npy')\n",
        "mfcctrainlabels = np.load('/content/drive/MyDrive/Machine_Learning/music_genre_data_di/train/mfccs/labels.npy')\n",
        "\n",
        "mfcctestdata = np.load('/content/drive/MyDrive/Machine_Learning/music_genre_data_di/test/mfccs/X.npy')\n",
        "mfcctestlabels = np.load('/content/drive/MyDrive/Machine_Learning/music_genre_data_di/test/mfccs/labels.npy')\n",
        "\n",
        "mfccvaldata = np.load('/content/drive/MyDrive/Machine_Learning/music_genre_data_di/val/mfccs/X.npy')\n",
        "mfccvallabels = np.load('/content/drive/MyDrive/Machine_Learning/music_genre_data_di/val/mfccs/labels.npy')\n",
        "\n",
        "\n",
        "training_data = dataset(mfcctraindata, mfcctrainlabels, labels_map, torch.tensor)\n",
        "val_data = dataset(mfccvaldata, mfccvallabels, labels_map, torch.tensor)\n",
        "test_data = dataset(mfcctestdata, mfcctestlabels, labels_map, torch.tensor)\n",
        "\n",
        "\n",
        "train_dataloader = DataLoader(training_data, batch_size=16,shuffle=True )\n",
        "val_dataloader =  DataLoader(val_data, batch_size=16,shuffle=True )\n",
        "test_dataloader =  DataLoader(test_data, batch_size=16,shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RO11CHrryvhV",
        "outputId": "e793355f-8620-4eb6-8da1-56bab3b7b28d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Element 1\n",
            "Input: tensor([-22.9744,   1.9012,   0.1679,   0.5512,   0.3211,   0.5997,   0.0674,\n",
            "          0.0578,   0.0309,   0.0858,   0.2841,   0.4549,   0.1126,   1.2083,\n",
            "          0.4164,   0.6556,   0.3223,   0.3863,   0.4391,   0.2785,   0.2911,\n",
            "          0.3201,   0.2940,   0.3278,   0.3299,   0.2015], dtype=torch.float64)\n",
            "Label: tensor(3)\n",
            "\n",
            "Element 2\n",
            "Input: tensor([-2.3234e+01,  1.6790e+00,  8.4396e-01,  5.8634e-01,  3.0448e-01,\n",
            "         6.4591e-01, -1.8444e-01,  6.9171e-03,  3.0031e-01,  3.9149e-01,\n",
            "         4.0412e-01, -1.1271e-02, -1.5429e-01,  1.3284e+00,  5.4626e-01,\n",
            "         3.1021e-01,  4.3165e-01,  2.2652e-01,  2.4103e-01,  3.5091e-01,\n",
            "         1.9282e-01,  1.9751e-01,  1.8797e-01,  1.9818e-01,  2.0764e-01,\n",
            "         1.6100e-01], dtype=torch.float64)\n",
            "Label: tensor(2)\n",
            "\n",
            "Element 3\n",
            "Input: tensor([-24.7860,   2.3513,  -2.0920,   0.3254,   0.6009,   0.2253,  -0.3357,\n",
            "         -0.1756,  -0.4013,  -0.1950,   0.3568,   0.1239,  -0.3383,   0.5490,\n",
            "          0.4892,   0.4613,   0.3006,   0.3627,   0.2777,   0.3959,   0.2043,\n",
            "          0.4764,   0.2289,   0.3429,   0.2256,   0.2568], dtype=torch.float64)\n",
            "Label: tensor(0)\n",
            "\n",
            "Element 4\n",
            "Input: tensor([-22.6897,   2.5243,  -0.7066,   0.7444,   0.8749,   0.2845,  -0.6611,\n",
            "         -0.1243,   0.4774,   0.2677,  -0.1964,  -0.1334,   0.0863,   0.7149,\n",
            "          0.3502,   0.2643,   0.3253,   0.2360,   0.2021,   0.1468,   0.1733,\n",
            "          0.2108,   0.3020,   0.1338,   0.1765,   0.1103], dtype=torch.float64)\n",
            "Label: tensor(0)\n",
            "\n",
            "Element 5\n",
            "Input: tensor([-2.2919e+01,  1.9555e+00, -9.4302e-01, -4.9726e-01, -1.0121e-01,\n",
            "         1.6656e-01, -3.6380e-01, -3.1035e-01,  2.0463e-01,  4.2223e-01,\n",
            "         1.1086e-01, -1.3605e-01,  1.6419e-02,  1.5012e+00,  5.7781e-01,\n",
            "         5.8024e-01,  2.9833e-01,  3.4607e-01,  2.6119e-01,  3.3779e-01,\n",
            "         3.2571e-01,  3.3495e-01,  2.6423e-01,  2.3667e-01,  2.6843e-01,\n",
            "         2.7994e-01], dtype=torch.float64)\n",
            "Label: tensor(1)\n",
            "\n",
            "Element 6\n",
            "Input: tensor([-25.1157,   1.9686,   0.2859,   0.1027,  -0.0577,  -0.0344,  -0.0898,\n",
            "          0.1044,  -0.0556,   0.1338,  -0.2460,  -0.1957,  -0.4256,   2.1351,\n",
            "          0.8322,   0.5662,   0.3582,   0.3966,   0.2657,   0.2457,   0.2570,\n",
            "          0.1997,   0.2513,   0.2586,   0.2345,   0.2650], dtype=torch.float64)\n",
            "Label: tensor(3)\n",
            "\n",
            "Element 7\n",
            "Input: tensor([-22.7258,   1.5784,  -0.7861,  -0.2145,   0.0563,  -0.1674,  -0.0261,\n",
            "         -0.0238,  -0.0292,  -0.1492,  -0.4365,  -0.2017,  -0.2571,   0.8889,\n",
            "          1.2056,   1.0922,   0.6001,   0.3896,   0.3391,   0.3980,   0.3782,\n",
            "          0.2530,   0.3170,   0.3479,   0.3736,   0.3757], dtype=torch.float64)\n",
            "Label: tensor(3)\n",
            "\n",
            "Element 8\n",
            "Input: tensor([-21.8762,   1.4922,   0.1722,   0.2032,   0.1188,   0.2078,   0.1495,\n",
            "          0.3271,   0.1951,   0.1605,   0.0517,   0.0413,  -0.0621,   1.0153,\n",
            "          0.4956,   0.3381,   0.2662,   0.2130,   0.2260,   0.2702,   0.2665,\n",
            "          0.3096,   0.2621,   0.3482,   0.2207,   0.1865], dtype=torch.float64)\n",
            "Label: tensor(3)\n",
            "\n",
            "Element 9\n",
            "Input: tensor([-2.1207e+01,  1.2044e+00,  8.3057e-01, -2.7941e-02,  3.7245e-01,\n",
            "         4.5423e-01, -7.1347e-02, -8.1715e-02,  4.1302e-02,  2.6881e-02,\n",
            "         8.1577e-02, -1.7939e-02,  2.9720e-02,  3.3939e-01,  2.4366e-01,\n",
            "         3.0510e-01,  2.5375e-01,  2.6228e-01,  3.6317e-01,  2.7265e-01,\n",
            "         2.7041e-01,  2.6893e-01,  2.9718e-01,  2.5669e-01,  2.6477e-01,\n",
            "         2.4560e-01], dtype=torch.float64)\n",
            "Label: tensor(2)\n",
            "\n",
            "Element 10\n",
            "Input: tensor([-24.4441,   2.2272,  -0.0986,   0.1711,  -0.0479,   0.1062,   0.6476,\n",
            "          0.4263,  -0.1218,  -0.2329,   0.2841,   0.1631,  -0.0482,   1.6456,\n",
            "          0.5999,   0.4016,   0.2822,   0.4003,   0.2814,   0.2510,   0.2724,\n",
            "          0.2196,   0.2553,   0.2701,   0.2370,   0.1432], dtype=torch.float64)\n",
            "Label: tensor(3)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def print_first_n_elements(dataloader, n=10):\n",
        "    count = 0\n",
        "    for batch in dataloader:\n",
        "        for i in range(len(batch[0])):\n",
        "            if count >= n:\n",
        "                return\n",
        "            X, y = batch[0][i], batch[1][i]\n",
        "            print(f\"Element {count + 1}\")\n",
        "            print(\"Input:\", X)\n",
        "            print(\"Label:\", y)\n",
        "            print()\n",
        "            count += 1\n",
        "\n",
        "dataloader = train_dataloader\n",
        "\n",
        "print_first_n_elements(dataloader, 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DF7BFpOAO-gT"
      },
      "source": [
        "## Step 2 Fully Connected Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M1WembwpPEv0",
        "outputId": "39b52aba-98e7-4fa2-b166-9643e08df8d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NeuralNetwork(\n",
            "  (layer1): Linear(in_features=26, out_features=128, bias=True)\n",
            "  (layer2): Linear(in_features=128, out_features=32, bias=True)\n",
            "  (layer3): Linear(in_features=32, out_features=4, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "  def __init__(self,activation_function=None):\n",
        "      super(NeuralNetwork, self).__init__()\n",
        "      self.layer1 = nn.Linear(26, 128)\n",
        "      self.layer2 = nn.Linear(128,32)\n",
        "      self.layer3 = nn.Linear(32,4)\n",
        "      self.activation_function = activation_function\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.layer1(x)\n",
        "    if self.activation_function:\n",
        "      x = self.activation_function(x)\n",
        "    x = self.layer2(x)\n",
        "    if self.activation_function:\n",
        "      x = self.activation_function(x)\n",
        "    logits = self.layer3(x)\n",
        "    return logits\n",
        "#No activation function in this step\n",
        "model = NeuralNetwork().to(device)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ga18rEYwTUCj"
      },
      "source": [
        "## Step 3 Training Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "2FC75CDETjrC"
      },
      "outputs": [],
      "source": [
        "def train(num_epochs, optimizer, dataloader,cost_func,model, device):\n",
        "  model.to(device)\n",
        "  model.train()\n",
        "  size = len(dataloader.dataset)\n",
        "  for epoch in range(num_epochs):\n",
        "    print(f\"current epoch: {epoch}\\n\")\n",
        "    for batch, (X,y) in enumerate(dataloader):\n",
        "      X = X.to(device)\n",
        "      y = y.to(device)\n",
        "      pred = model(X.float())\n",
        "      loss = cost_func(pred,y)\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      loss, current = loss.item(), batch * len(X)\n",
        "      print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2us28e2oX1jZ"
      },
      "source": [
        "## Step 4 Evaluation Function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ygK2FEBAvjyB"
      },
      "outputs": [],
      "source": [
        "def evaluate(dataloader, cost_func, model, device):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    test_loss = 0.0\n",
        "    test_f1 = 0.0\n",
        "    test_acc = 0.0\n",
        "    correct = 0\n",
        "    size = len(dataloader.dataset)\n",
        "    accuracy = Accuracy(task=\"multiclass\", num_classes=4).to(device)\n",
        "    f1 = F1Score(task=\"multiclass\", num_classes=4, average=\"macro\").to(device)\n",
        "    confmatrix = ConfusionMatrix(task=\"multiclass\", num_classes=4).to(device)\n",
        "\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch, (X, y) in enumerate(dataloader):\n",
        "            X = X.to(device).float()\n",
        "            y = y.to(device)\n",
        "\n",
        "            pred = model(X)\n",
        "\n",
        "            all_preds.append(pred)\n",
        "            all_labels.append(y)\n",
        "\n",
        "            test_loss += cost_func(pred, y).item() * X.size(0)\n",
        "            test_acc += accuracy(pred, y).item() * X.size(0)\n",
        "            test_f1 += f1(pred, y).item() * X.size(0)\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    test_loss /= size\n",
        "    test_acc /= size\n",
        "    test_f1 /= size\n",
        "\n",
        "    all_preds = torch.cat(all_preds)\n",
        "    all_labels = torch.cat(all_labels)\n",
        "    confusion_matrix = confmatrix(all_preds.argmax(dim=1), all_labels)\n",
        "\n",
        "    print(f\"Avg Accuracy: {100*test_acc:>8f}%, Avg loss: {test_loss:>8f}\")\n",
        "    print(f\"F1 score is: {test_f1}\")\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrix.cpu().numpy())\n",
        "\n",
        "    return test_loss, test_f1, test_acc, confusion_matrix\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPMafl3SctIg"
      },
      "source": [
        "## Step 5 Training and Evaluation of our Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SmXdvq-kcqOx",
        "outputId": "dfe206c4-e428-4ad8-acae-19ef22c9451c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "loss: 1.344077  [  912/ 3200]\n",
            "loss: 1.227578  [  928/ 3200]\n",
            "loss: 1.304826  [  944/ 3200]\n",
            "loss: 1.306929  [  960/ 3200]\n",
            "loss: 1.316983  [  976/ 3200]\n",
            "loss: 1.302292  [  992/ 3200]\n",
            "loss: 1.300383  [ 1008/ 3200]\n",
            "loss: 1.238822  [ 1024/ 3200]\n",
            "loss: 1.282813  [ 1040/ 3200]\n",
            "loss: 1.195735  [ 1056/ 3200]\n",
            "loss: 1.332905  [ 1072/ 3200]\n",
            "loss: 1.325178  [ 1088/ 3200]\n",
            "loss: 1.304537  [ 1104/ 3200]\n",
            "loss: 1.204636  [ 1120/ 3200]\n",
            "loss: 1.235811  [ 1136/ 3200]\n",
            "loss: 1.425579  [ 1152/ 3200]\n",
            "loss: 1.320343  [ 1168/ 3200]\n",
            "loss: 1.321758  [ 1184/ 3200]\n",
            "loss: 1.341453  [ 1200/ 3200]\n",
            "loss: 1.296938  [ 1216/ 3200]\n",
            "loss: 1.315947  [ 1232/ 3200]\n",
            "loss: 1.327361  [ 1248/ 3200]\n",
            "loss: 1.267840  [ 1264/ 3200]\n",
            "loss: 1.296741  [ 1280/ 3200]\n",
            "loss: 1.210579  [ 1296/ 3200]\n",
            "loss: 1.363716  [ 1312/ 3200]\n",
            "loss: 1.292179  [ 1328/ 3200]\n",
            "loss: 1.378983  [ 1344/ 3200]\n",
            "loss: 1.253724  [ 1360/ 3200]\n",
            "loss: 1.353701  [ 1376/ 3200]\n",
            "loss: 1.304173  [ 1392/ 3200]\n",
            "loss: 1.285486  [ 1408/ 3200]\n",
            "loss: 1.276108  [ 1424/ 3200]\n",
            "loss: 1.286466  [ 1440/ 3200]\n",
            "loss: 1.251372  [ 1456/ 3200]\n",
            "loss: 1.244325  [ 1472/ 3200]\n",
            "loss: 1.292240  [ 1488/ 3200]\n",
            "loss: 1.335015  [ 1504/ 3200]\n",
            "loss: 1.400789  [ 1520/ 3200]\n",
            "loss: 1.319340  [ 1536/ 3200]\n",
            "loss: 1.165312  [ 1552/ 3200]\n",
            "loss: 1.359251  [ 1568/ 3200]\n",
            "loss: 1.279930  [ 1584/ 3200]\n",
            "loss: 1.212632  [ 1600/ 3200]\n",
            "loss: 1.271106  [ 1616/ 3200]\n",
            "loss: 1.238443  [ 1632/ 3200]\n",
            "loss: 1.217177  [ 1648/ 3200]\n",
            "loss: 1.294211  [ 1664/ 3200]\n",
            "loss: 1.168927  [ 1680/ 3200]\n",
            "loss: 1.339265  [ 1696/ 3200]\n",
            "loss: 1.283090  [ 1712/ 3200]\n",
            "loss: 1.286814  [ 1728/ 3200]\n",
            "loss: 1.258534  [ 1744/ 3200]\n",
            "loss: 1.279302  [ 1760/ 3200]\n",
            "loss: 1.356821  [ 1776/ 3200]\n",
            "loss: 1.266538  [ 1792/ 3200]\n",
            "loss: 1.302124  [ 1808/ 3200]\n",
            "loss: 1.315282  [ 1824/ 3200]\n",
            "loss: 1.337170  [ 1840/ 3200]\n",
            "loss: 1.344170  [ 1856/ 3200]\n",
            "loss: 1.242158  [ 1872/ 3200]\n",
            "loss: 1.250990  [ 1888/ 3200]\n",
            "loss: 1.247334  [ 1904/ 3200]\n",
            "loss: 1.273420  [ 1920/ 3200]\n",
            "loss: 1.308864  [ 1936/ 3200]\n",
            "loss: 1.326409  [ 1952/ 3200]\n",
            "loss: 1.188223  [ 1968/ 3200]\n",
            "loss: 1.214484  [ 1984/ 3200]\n",
            "loss: 1.257191  [ 2000/ 3200]\n",
            "loss: 1.261612  [ 2016/ 3200]\n",
            "loss: 1.176509  [ 2032/ 3200]\n",
            "loss: 1.368146  [ 2048/ 3200]\n",
            "loss: 1.354979  [ 2064/ 3200]\n",
            "loss: 1.234978  [ 2080/ 3200]\n",
            "loss: 1.221875  [ 2096/ 3200]\n",
            "loss: 1.141316  [ 2112/ 3200]\n",
            "loss: 1.352950  [ 2128/ 3200]\n",
            "loss: 1.272147  [ 2144/ 3200]\n",
            "loss: 1.187993  [ 2160/ 3200]\n",
            "loss: 1.200761  [ 2176/ 3200]\n",
            "loss: 1.418015  [ 2192/ 3200]\n",
            "loss: 1.296060  [ 2208/ 3200]\n",
            "loss: 1.276067  [ 2224/ 3200]\n",
            "loss: 1.341860  [ 2240/ 3200]\n",
            "loss: 1.348497  [ 2256/ 3200]\n",
            "loss: 1.272261  [ 2272/ 3200]\n",
            "loss: 1.392578  [ 2288/ 3200]\n",
            "loss: 1.273162  [ 2304/ 3200]\n",
            "loss: 1.278902  [ 2320/ 3200]\n",
            "loss: 1.216290  [ 2336/ 3200]\n",
            "loss: 1.275439  [ 2352/ 3200]\n",
            "loss: 1.258501  [ 2368/ 3200]\n",
            "loss: 1.270114  [ 2384/ 3200]\n",
            "loss: 1.323304  [ 2400/ 3200]\n",
            "loss: 1.250504  [ 2416/ 3200]\n",
            "loss: 1.272521  [ 2432/ 3200]\n",
            "loss: 1.308759  [ 2448/ 3200]\n",
            "loss: 1.237481  [ 2464/ 3200]\n",
            "loss: 1.202649  [ 2480/ 3200]\n",
            "loss: 1.268599  [ 2496/ 3200]\n",
            "loss: 1.265656  [ 2512/ 3200]\n",
            "loss: 1.287679  [ 2528/ 3200]\n",
            "loss: 1.255453  [ 2544/ 3200]\n",
            "loss: 1.252922  [ 2560/ 3200]\n",
            "loss: 1.270653  [ 2576/ 3200]\n",
            "loss: 1.314759  [ 2592/ 3200]\n",
            "loss: 1.231619  [ 2608/ 3200]\n",
            "loss: 1.193362  [ 2624/ 3200]\n",
            "loss: 1.334415  [ 2640/ 3200]\n",
            "loss: 1.231465  [ 2656/ 3200]\n",
            "loss: 1.189866  [ 2672/ 3200]\n",
            "loss: 1.173874  [ 2688/ 3200]\n",
            "loss: 1.268166  [ 2704/ 3200]\n",
            "loss: 1.141169  [ 2720/ 3200]\n",
            "loss: 1.281991  [ 2736/ 3200]\n",
            "loss: 1.178996  [ 2752/ 3200]\n",
            "loss: 1.171561  [ 2768/ 3200]\n",
            "loss: 1.310735  [ 2784/ 3200]\n",
            "loss: 1.212915  [ 2800/ 3200]\n",
            "loss: 1.211596  [ 2816/ 3200]\n",
            "loss: 1.475383  [ 2832/ 3200]\n",
            "loss: 1.270883  [ 2848/ 3200]\n",
            "loss: 1.233176  [ 2864/ 3200]\n",
            "loss: 1.200196  [ 2880/ 3200]\n",
            "loss: 1.160712  [ 2896/ 3200]\n",
            "loss: 1.261883  [ 2912/ 3200]\n",
            "loss: 1.349479  [ 2928/ 3200]\n",
            "loss: 1.305393  [ 2944/ 3200]\n",
            "loss: 1.338074  [ 2960/ 3200]\n",
            "loss: 1.270729  [ 2976/ 3200]\n",
            "loss: 1.285437  [ 2992/ 3200]\n",
            "loss: 1.362934  [ 3008/ 3200]\n",
            "loss: 1.301596  [ 3024/ 3200]\n",
            "loss: 1.308699  [ 3040/ 3200]\n",
            "loss: 1.272365  [ 3056/ 3200]\n",
            "loss: 1.311325  [ 3072/ 3200]\n",
            "loss: 1.237002  [ 3088/ 3200]\n",
            "loss: 1.247558  [ 3104/ 3200]\n",
            "loss: 1.209561  [ 3120/ 3200]\n",
            "loss: 1.232628  [ 3136/ 3200]\n",
            "loss: 1.262607  [ 3152/ 3200]\n",
            "loss: 1.260338  [ 3168/ 3200]\n",
            "loss: 1.308227  [ 3184/ 3200]\n",
            "current epoch: 6\n",
            "\n",
            "loss: 1.245528  [    0/ 3200]\n",
            "loss: 1.188854  [   16/ 3200]\n",
            "loss: 1.265198  [   32/ 3200]\n",
            "loss: 1.306217  [   48/ 3200]\n",
            "loss: 1.213741  [   64/ 3200]\n",
            "loss: 1.230166  [   80/ 3200]\n",
            "loss: 1.216463  [   96/ 3200]\n",
            "loss: 1.287844  [  112/ 3200]\n",
            "loss: 1.257851  [  128/ 3200]\n",
            "loss: 1.254520  [  144/ 3200]\n",
            "loss: 1.172987  [  160/ 3200]\n",
            "loss: 1.311528  [  176/ 3200]\n",
            "loss: 1.256362  [  192/ 3200]\n",
            "loss: 1.345315  [  208/ 3200]\n",
            "loss: 1.296750  [  224/ 3200]\n",
            "loss: 1.225640  [  240/ 3200]\n",
            "loss: 1.275451  [  256/ 3200]\n",
            "loss: 1.212015  [  272/ 3200]\n",
            "loss: 1.381885  [  288/ 3200]\n",
            "loss: 1.308591  [  304/ 3200]\n",
            "loss: 1.269389  [  320/ 3200]\n",
            "loss: 1.302668  [  336/ 3200]\n",
            "loss: 1.238246  [  352/ 3200]\n",
            "loss: 1.264924  [  368/ 3200]\n",
            "loss: 1.251262  [  384/ 3200]\n",
            "loss: 1.211386  [  400/ 3200]\n",
            "loss: 1.238530  [  416/ 3200]\n",
            "loss: 1.258056  [  432/ 3200]\n",
            "loss: 1.210095  [  448/ 3200]\n",
            "loss: 1.307604  [  464/ 3200]\n",
            "loss: 1.269986  [  480/ 3200]\n",
            "loss: 1.242006  [  496/ 3200]\n",
            "loss: 1.272418  [  512/ 3200]\n",
            "loss: 1.223485  [  528/ 3200]\n",
            "loss: 1.206368  [  544/ 3200]\n",
            "loss: 1.380217  [  560/ 3200]\n",
            "loss: 1.240816  [  576/ 3200]\n",
            "loss: 1.205974  [  592/ 3200]\n",
            "loss: 1.311780  [  608/ 3200]\n",
            "loss: 1.260396  [  624/ 3200]\n",
            "loss: 1.279327  [  640/ 3200]\n",
            "loss: 1.242206  [  656/ 3200]\n",
            "loss: 1.295782  [  672/ 3200]\n",
            "loss: 1.299904  [  688/ 3200]\n",
            "loss: 1.248875  [  704/ 3200]\n",
            "loss: 1.285962  [  720/ 3200]\n",
            "loss: 1.315839  [  736/ 3200]\n",
            "loss: 1.308553  [  752/ 3200]\n",
            "loss: 1.167391  [  768/ 3200]\n",
            "loss: 1.267227  [  784/ 3200]\n",
            "loss: 1.373171  [  800/ 3200]\n",
            "loss: 1.239630  [  816/ 3200]\n",
            "loss: 1.277764  [  832/ 3200]\n",
            "loss: 1.230694  [  848/ 3200]\n",
            "loss: 1.350743  [  864/ 3200]\n",
            "loss: 1.251078  [  880/ 3200]\n",
            "loss: 1.320458  [  896/ 3200]\n",
            "loss: 1.138205  [  912/ 3200]\n",
            "loss: 1.279242  [  928/ 3200]\n",
            "loss: 1.316328  [  944/ 3200]\n",
            "loss: 1.260002  [  960/ 3200]\n",
            "loss: 1.297812  [  976/ 3200]\n",
            "loss: 1.276560  [  992/ 3200]\n",
            "loss: 1.144572  [ 1008/ 3200]\n",
            "loss: 1.260205  [ 1024/ 3200]\n",
            "loss: 1.191489  [ 1040/ 3200]\n",
            "loss: 1.316009  [ 1056/ 3200]\n",
            "loss: 1.326149  [ 1072/ 3200]\n",
            "loss: 1.189836  [ 1088/ 3200]\n",
            "loss: 1.259431  [ 1104/ 3200]\n",
            "loss: 1.215659  [ 1120/ 3200]\n",
            "loss: 1.224279  [ 1136/ 3200]\n",
            "loss: 1.330038  [ 1152/ 3200]\n",
            "loss: 1.300567  [ 1168/ 3200]\n",
            "loss: 1.233195  [ 1184/ 3200]\n",
            "loss: 1.202276  [ 1200/ 3200]\n",
            "loss: 1.321741  [ 1216/ 3200]\n",
            "loss: 1.168584  [ 1232/ 3200]\n",
            "loss: 1.243018  [ 1248/ 3200]\n",
            "loss: 1.327375  [ 1264/ 3200]\n",
            "loss: 1.241117  [ 1280/ 3200]\n",
            "loss: 1.257845  [ 1296/ 3200]\n",
            "loss: 1.186462  [ 1312/ 3200]\n",
            "loss: 1.227052  [ 1328/ 3200]\n",
            "loss: 1.227451  [ 1344/ 3200]\n",
            "loss: 1.255791  [ 1360/ 3200]\n",
            "loss: 1.375715  [ 1376/ 3200]\n",
            "loss: 1.331095  [ 1392/ 3200]\n",
            "loss: 1.218473  [ 1408/ 3200]\n",
            "loss: 1.190687  [ 1424/ 3200]\n",
            "loss: 1.327723  [ 1440/ 3200]\n",
            "loss: 1.236497  [ 1456/ 3200]\n",
            "loss: 1.298898  [ 1472/ 3200]\n",
            "loss: 1.287671  [ 1488/ 3200]\n",
            "loss: 1.213722  [ 1504/ 3200]\n",
            "loss: 1.232042  [ 1520/ 3200]\n",
            "loss: 1.243461  [ 1536/ 3200]\n",
            "loss: 1.342371  [ 1552/ 3200]\n",
            "loss: 1.237647  [ 1568/ 3200]\n",
            "loss: 1.250437  [ 1584/ 3200]\n",
            "loss: 1.242803  [ 1600/ 3200]\n",
            "loss: 1.295877  [ 1616/ 3200]\n",
            "loss: 1.180176  [ 1632/ 3200]\n",
            "loss: 1.242396  [ 1648/ 3200]\n",
            "loss: 1.296792  [ 1664/ 3200]\n",
            "loss: 1.150970  [ 1680/ 3200]\n",
            "loss: 1.273290  [ 1696/ 3200]\n",
            "loss: 1.247567  [ 1712/ 3200]\n",
            "loss: 1.179212  [ 1728/ 3200]\n",
            "loss: 1.281465  [ 1744/ 3200]\n",
            "loss: 1.319559  [ 1760/ 3200]\n",
            "loss: 1.210690  [ 1776/ 3200]\n",
            "loss: 1.180653  [ 1792/ 3200]\n",
            "loss: 1.228606  [ 1808/ 3200]\n",
            "loss: 1.177802  [ 1824/ 3200]\n",
            "loss: 1.304398  [ 1840/ 3200]\n",
            "loss: 1.232383  [ 1856/ 3200]\n",
            "loss: 1.263521  [ 1872/ 3200]\n",
            "loss: 1.150177  [ 1888/ 3200]\n",
            "loss: 1.272330  [ 1904/ 3200]\n",
            "loss: 1.264579  [ 1920/ 3200]\n",
            "loss: 1.230581  [ 1936/ 3200]\n",
            "loss: 1.338954  [ 1952/ 3200]\n",
            "loss: 1.230328  [ 1968/ 3200]\n",
            "loss: 1.230373  [ 1984/ 3200]\n",
            "loss: 1.330238  [ 2000/ 3200]\n",
            "loss: 1.310862  [ 2016/ 3200]\n",
            "loss: 1.313135  [ 2032/ 3200]\n",
            "loss: 1.219311  [ 2048/ 3200]\n",
            "loss: 1.188762  [ 2064/ 3200]\n",
            "loss: 1.307107  [ 2080/ 3200]\n",
            "loss: 1.253983  [ 2096/ 3200]\n",
            "loss: 1.311102  [ 2112/ 3200]\n",
            "loss: 1.187715  [ 2128/ 3200]\n",
            "loss: 1.281548  [ 2144/ 3200]\n",
            "loss: 1.271008  [ 2160/ 3200]\n",
            "loss: 1.240071  [ 2176/ 3200]\n",
            "loss: 1.201775  [ 2192/ 3200]\n",
            "loss: 1.304208  [ 2208/ 3200]\n",
            "loss: 1.226325  [ 2224/ 3200]\n",
            "loss: 1.367052  [ 2240/ 3200]\n",
            "loss: 1.191847  [ 2256/ 3200]\n",
            "loss: 1.258162  [ 2272/ 3200]\n",
            "loss: 1.220983  [ 2288/ 3200]\n",
            "loss: 1.233355  [ 2304/ 3200]\n",
            "loss: 1.178444  [ 2320/ 3200]\n",
            "loss: 1.286911  [ 2336/ 3200]\n",
            "loss: 1.207947  [ 2352/ 3200]\n",
            "loss: 1.202377  [ 2368/ 3200]\n",
            "loss: 1.209226  [ 2384/ 3200]\n",
            "loss: 1.207910  [ 2400/ 3200]\n",
            "loss: 1.234150  [ 2416/ 3200]\n",
            "loss: 1.193547  [ 2432/ 3200]\n",
            "loss: 1.318868  [ 2448/ 3200]\n",
            "loss: 1.223319  [ 2464/ 3200]\n",
            "loss: 1.112861  [ 2480/ 3200]\n",
            "loss: 1.066030  [ 2496/ 3200]\n",
            "loss: 1.398105  [ 2512/ 3200]\n",
            "loss: 1.263393  [ 2528/ 3200]\n",
            "loss: 1.238155  [ 2544/ 3200]\n",
            "loss: 1.308855  [ 2560/ 3200]\n",
            "loss: 1.245199  [ 2576/ 3200]\n",
            "loss: 1.279230  [ 2592/ 3200]\n",
            "loss: 1.349887  [ 2608/ 3200]\n",
            "loss: 1.125838  [ 2624/ 3200]\n",
            "loss: 1.211566  [ 2640/ 3200]\n",
            "loss: 1.323473  [ 2656/ 3200]\n",
            "loss: 1.258758  [ 2672/ 3200]\n",
            "loss: 1.280534  [ 2688/ 3200]\n",
            "loss: 1.204590  [ 2704/ 3200]\n",
            "loss: 1.249964  [ 2720/ 3200]\n",
            "loss: 1.331754  [ 2736/ 3200]\n",
            "loss: 1.260139  [ 2752/ 3200]\n",
            "loss: 1.335852  [ 2768/ 3200]\n",
            "loss: 1.354130  [ 2784/ 3200]\n",
            "loss: 1.281354  [ 2800/ 3200]\n",
            "loss: 1.326327  [ 2816/ 3200]\n",
            "loss: 1.234487  [ 2832/ 3200]\n",
            "loss: 1.245149  [ 2848/ 3200]\n",
            "loss: 1.306942  [ 2864/ 3200]\n",
            "loss: 1.250295  [ 2880/ 3200]\n",
            "loss: 1.256552  [ 2896/ 3200]\n",
            "loss: 1.164989  [ 2912/ 3200]\n",
            "loss: 1.169358  [ 2928/ 3200]\n",
            "loss: 1.297092  [ 2944/ 3200]\n",
            "loss: 1.273634  [ 2960/ 3200]\n",
            "loss: 1.335554  [ 2976/ 3200]\n",
            "loss: 1.350364  [ 2992/ 3200]\n",
            "loss: 1.239761  [ 3008/ 3200]\n",
            "loss: 1.326962  [ 3024/ 3200]\n",
            "loss: 1.254609  [ 3040/ 3200]\n",
            "loss: 1.181731  [ 3056/ 3200]\n",
            "loss: 1.106428  [ 3072/ 3200]\n",
            "loss: 1.342272  [ 3088/ 3200]\n",
            "loss: 1.187976  [ 3104/ 3200]\n",
            "loss: 1.308313  [ 3120/ 3200]\n",
            "loss: 1.268605  [ 3136/ 3200]\n",
            "loss: 1.189430  [ 3152/ 3200]\n",
            "loss: 1.193606  [ 3168/ 3200]\n",
            "loss: 1.226491  [ 3184/ 3200]\n",
            "current epoch: 7\n",
            "\n",
            "loss: 1.297938  [    0/ 3200]\n",
            "loss: 1.117246  [   16/ 3200]\n",
            "loss: 1.301590  [   32/ 3200]\n",
            "loss: 1.182809  [   48/ 3200]\n",
            "loss: 1.312896  [   64/ 3200]\n",
            "loss: 1.104546  [   80/ 3200]\n",
            "loss: 1.272640  [   96/ 3200]\n",
            "loss: 1.195020  [  112/ 3200]\n",
            "loss: 1.235958  [  128/ 3200]\n",
            "loss: 1.047331  [  144/ 3200]\n",
            "loss: 1.354749  [  160/ 3200]\n",
            "loss: 1.195133  [  176/ 3200]\n",
            "loss: 1.170415  [  192/ 3200]\n",
            "loss: 1.201609  [  208/ 3200]\n",
            "loss: 1.102522  [  224/ 3200]\n",
            "loss: 1.134506  [  240/ 3200]\n",
            "loss: 1.273589  [  256/ 3200]\n",
            "loss: 1.214674  [  272/ 3200]\n",
            "loss: 1.223449  [  288/ 3200]\n",
            "loss: 1.084733  [  304/ 3200]\n",
            "loss: 1.124757  [  320/ 3200]\n",
            "loss: 1.275319  [  336/ 3200]\n",
            "loss: 1.388926  [  352/ 3200]\n",
            "loss: 1.261339  [  368/ 3200]\n",
            "loss: 1.301344  [  384/ 3200]\n",
            "loss: 1.307211  [  400/ 3200]\n",
            "loss: 1.151855  [  416/ 3200]\n",
            "loss: 1.323826  [  432/ 3200]\n",
            "loss: 1.268681  [  448/ 3200]\n",
            "loss: 1.297621  [  464/ 3200]\n",
            "loss: 1.213809  [  480/ 3200]\n",
            "loss: 1.240691  [  496/ 3200]\n",
            "loss: 1.247902  [  512/ 3200]\n",
            "loss: 1.260428  [  528/ 3200]\n",
            "loss: 1.225854  [  544/ 3200]\n",
            "loss: 1.325609  [  560/ 3200]\n",
            "loss: 1.225042  [  576/ 3200]\n",
            "loss: 1.253782  [  592/ 3200]\n",
            "loss: 1.244679  [  608/ 3200]\n",
            "loss: 1.263973  [  624/ 3200]\n",
            "loss: 1.259028  [  640/ 3200]\n",
            "loss: 1.283247  [  656/ 3200]\n",
            "loss: 1.242725  [  672/ 3200]\n",
            "loss: 1.231590  [  688/ 3200]\n",
            "loss: 1.246046  [  704/ 3200]\n",
            "loss: 1.331682  [  720/ 3200]\n",
            "loss: 1.292305  [  736/ 3200]\n",
            "loss: 1.164364  [  752/ 3200]\n",
            "loss: 1.245601  [  768/ 3200]\n",
            "loss: 1.234369  [  784/ 3200]\n",
            "loss: 1.270785  [  800/ 3200]\n",
            "loss: 1.371013  [  816/ 3200]\n",
            "loss: 1.215600  [  832/ 3200]\n",
            "loss: 1.185599  [  848/ 3200]\n",
            "loss: 1.189171  [  864/ 3200]\n",
            "loss: 1.300666  [  880/ 3200]\n",
            "loss: 1.108905  [  896/ 3200]\n",
            "loss: 1.269694  [  912/ 3200]\n",
            "loss: 1.219878  [  928/ 3200]\n",
            "loss: 1.275612  [  944/ 3200]\n",
            "loss: 1.209534  [  960/ 3200]\n",
            "loss: 1.130233  [  976/ 3200]\n",
            "loss: 1.264659  [  992/ 3200]\n",
            "loss: 1.280836  [ 1008/ 3200]\n",
            "loss: 1.299369  [ 1024/ 3200]\n",
            "loss: 1.117489  [ 1040/ 3200]\n",
            "loss: 1.141304  [ 1056/ 3200]\n",
            "loss: 1.220489  [ 1072/ 3200]\n",
            "loss: 1.199623  [ 1088/ 3200]\n",
            "loss: 1.205887  [ 1104/ 3200]\n",
            "loss: 1.249252  [ 1120/ 3200]\n",
            "loss: 1.205586  [ 1136/ 3200]\n",
            "loss: 1.269345  [ 1152/ 3200]\n",
            "loss: 1.167324  [ 1168/ 3200]\n",
            "loss: 1.164124  [ 1184/ 3200]\n",
            "loss: 1.252030  [ 1200/ 3200]\n",
            "loss: 1.226543  [ 1216/ 3200]\n",
            "loss: 1.233077  [ 1232/ 3200]\n",
            "loss: 1.138134  [ 1248/ 3200]\n",
            "loss: 1.246197  [ 1264/ 3200]\n",
            "loss: 1.287896  [ 1280/ 3200]\n",
            "loss: 1.252772  [ 1296/ 3200]\n",
            "loss: 1.211936  [ 1312/ 3200]\n",
            "loss: 1.214766  [ 1328/ 3200]\n",
            "loss: 1.231196  [ 1344/ 3200]\n",
            "loss: 1.331893  [ 1360/ 3200]\n",
            "loss: 1.191956  [ 1376/ 3200]\n",
            "loss: 1.254949  [ 1392/ 3200]\n",
            "loss: 1.175471  [ 1408/ 3200]\n",
            "loss: 1.320270  [ 1424/ 3200]\n",
            "loss: 1.278383  [ 1440/ 3200]\n",
            "loss: 1.159365  [ 1456/ 3200]\n",
            "loss: 1.248579  [ 1472/ 3200]\n",
            "loss: 1.397243  [ 1488/ 3200]\n",
            "loss: 1.279435  [ 1504/ 3200]\n",
            "loss: 1.244010  [ 1520/ 3200]\n",
            "loss: 1.314778  [ 1536/ 3200]\n",
            "loss: 1.241352  [ 1552/ 3200]\n",
            "loss: 1.169165  [ 1568/ 3200]\n",
            "loss: 1.258222  [ 1584/ 3200]\n",
            "loss: 1.206725  [ 1600/ 3200]\n",
            "loss: 1.257972  [ 1616/ 3200]\n",
            "loss: 1.227823  [ 1632/ 3200]\n",
            "loss: 1.326942  [ 1648/ 3200]\n",
            "loss: 1.164115  [ 1664/ 3200]\n",
            "loss: 1.241945  [ 1680/ 3200]\n",
            "loss: 1.250160  [ 1696/ 3200]\n",
            "loss: 1.300617  [ 1712/ 3200]\n",
            "loss: 1.237888  [ 1728/ 3200]\n",
            "loss: 1.234135  [ 1744/ 3200]\n",
            "loss: 1.266451  [ 1760/ 3200]\n",
            "loss: 1.230783  [ 1776/ 3200]\n",
            "loss: 1.203803  [ 1792/ 3200]\n",
            "loss: 1.263599  [ 1808/ 3200]\n",
            "loss: 1.288897  [ 1824/ 3200]\n",
            "loss: 1.235203  [ 1840/ 3200]\n",
            "loss: 1.359596  [ 1856/ 3200]\n",
            "loss: 1.200714  [ 1872/ 3200]\n",
            "loss: 1.203768  [ 1888/ 3200]\n",
            "loss: 1.306297  [ 1904/ 3200]\n",
            "loss: 1.135413  [ 1920/ 3200]\n",
            "loss: 1.228800  [ 1936/ 3200]\n",
            "loss: 1.177795  [ 1952/ 3200]\n",
            "loss: 1.323132  [ 1968/ 3200]\n",
            "loss: 1.178298  [ 1984/ 3200]\n",
            "loss: 1.249534  [ 2000/ 3200]\n",
            "loss: 1.193324  [ 2016/ 3200]\n",
            "loss: 1.199083  [ 2032/ 3200]\n",
            "loss: 1.123344  [ 2048/ 3200]\n",
            "loss: 1.226339  [ 2064/ 3200]\n",
            "loss: 1.270238  [ 2080/ 3200]\n",
            "loss: 1.195291  [ 2096/ 3200]\n",
            "loss: 1.032777  [ 2112/ 3200]\n",
            "loss: 1.150322  [ 2128/ 3200]\n",
            "loss: 1.308826  [ 2144/ 3200]\n",
            "loss: 1.233054  [ 2160/ 3200]\n",
            "loss: 1.187691  [ 2176/ 3200]\n",
            "loss: 1.199120  [ 2192/ 3200]\n",
            "loss: 1.325214  [ 2208/ 3200]\n",
            "loss: 1.099078  [ 2224/ 3200]\n",
            "loss: 1.234359  [ 2240/ 3200]\n",
            "loss: 1.133230  [ 2256/ 3200]\n",
            "loss: 1.187544  [ 2272/ 3200]\n",
            "loss: 1.275980  [ 2288/ 3200]\n",
            "loss: 1.250621  [ 2304/ 3200]\n",
            "loss: 1.141602  [ 2320/ 3200]\n",
            "loss: 1.321189  [ 2336/ 3200]\n",
            "loss: 1.179179  [ 2352/ 3200]\n",
            "loss: 1.200010  [ 2368/ 3200]\n",
            "loss: 1.285887  [ 2384/ 3200]\n",
            "loss: 1.060770  [ 2400/ 3200]\n",
            "loss: 1.212118  [ 2416/ 3200]\n",
            "loss: 1.266542  [ 2432/ 3200]\n",
            "loss: 1.207350  [ 2448/ 3200]\n",
            "loss: 1.261679  [ 2464/ 3200]\n",
            "loss: 1.104959  [ 2480/ 3200]\n",
            "loss: 1.236682  [ 2496/ 3200]\n",
            "loss: 1.207016  [ 2512/ 3200]\n",
            "loss: 1.219366  [ 2528/ 3200]\n",
            "loss: 1.242939  [ 2544/ 3200]\n",
            "loss: 1.148970  [ 2560/ 3200]\n",
            "loss: 1.242204  [ 2576/ 3200]\n",
            "loss: 1.302932  [ 2592/ 3200]\n",
            "loss: 1.187395  [ 2608/ 3200]\n",
            "loss: 1.093170  [ 2624/ 3200]\n",
            "loss: 1.344599  [ 2640/ 3200]\n",
            "loss: 1.143386  [ 2656/ 3200]\n",
            "loss: 1.150226  [ 2672/ 3200]\n",
            "loss: 1.226361  [ 2688/ 3200]\n",
            "loss: 1.217876  [ 2704/ 3200]\n",
            "loss: 1.235100  [ 2720/ 3200]\n",
            "loss: 1.288948  [ 2736/ 3200]\n",
            "loss: 1.244789  [ 2752/ 3200]\n",
            "loss: 1.257692  [ 2768/ 3200]\n",
            "loss: 1.180974  [ 2784/ 3200]\n",
            "loss: 1.221741  [ 2800/ 3200]\n",
            "loss: 1.159500  [ 2816/ 3200]\n",
            "loss: 1.270724  [ 2832/ 3200]\n",
            "loss: 1.089788  [ 2848/ 3200]\n",
            "loss: 1.184229  [ 2864/ 3200]\n",
            "loss: 1.297113  [ 2880/ 3200]\n",
            "loss: 1.198515  [ 2896/ 3200]\n",
            "loss: 1.272248  [ 2912/ 3200]\n",
            "loss: 1.400143  [ 2928/ 3200]\n",
            "loss: 1.315305  [ 2944/ 3200]\n",
            "loss: 1.243265  [ 2960/ 3200]\n",
            "loss: 1.300342  [ 2976/ 3200]\n",
            "loss: 1.166697  [ 2992/ 3200]\n",
            "loss: 1.326382  [ 3008/ 3200]\n",
            "loss: 1.164222  [ 3024/ 3200]\n",
            "loss: 1.225573  [ 3040/ 3200]\n",
            "loss: 1.070003  [ 3056/ 3200]\n",
            "loss: 1.076975  [ 3072/ 3200]\n",
            "loss: 1.288510  [ 3088/ 3200]\n",
            "loss: 1.235352  [ 3104/ 3200]\n",
            "loss: 1.145150  [ 3120/ 3200]\n",
            "loss: 1.183468  [ 3136/ 3200]\n",
            "loss: 1.191011  [ 3152/ 3200]\n",
            "loss: 1.230183  [ 3168/ 3200]\n",
            "loss: 1.323243  [ 3184/ 3200]\n",
            "current epoch: 8\n",
            "\n",
            "loss: 1.269725  [    0/ 3200]\n",
            "loss: 1.130568  [   16/ 3200]\n",
            "loss: 1.148991  [   32/ 3200]\n",
            "loss: 1.242567  [   48/ 3200]\n",
            "loss: 1.288421  [   64/ 3200]\n",
            "loss: 1.316805  [   80/ 3200]\n",
            "loss: 1.210412  [   96/ 3200]\n",
            "loss: 1.204704  [  112/ 3200]\n",
            "loss: 1.212564  [  128/ 3200]\n",
            "loss: 1.325779  [  144/ 3200]\n",
            "loss: 1.315628  [  160/ 3200]\n",
            "loss: 1.281865  [  176/ 3200]\n",
            "loss: 1.258031  [  192/ 3200]\n",
            "loss: 1.136328  [  208/ 3200]\n",
            "loss: 1.284174  [  224/ 3200]\n",
            "loss: 1.281978  [  240/ 3200]\n",
            "loss: 1.109067  [  256/ 3200]\n",
            "loss: 1.262635  [  272/ 3200]\n",
            "loss: 1.298526  [  288/ 3200]\n",
            "loss: 1.321801  [  304/ 3200]\n",
            "loss: 1.171365  [  320/ 3200]\n",
            "loss: 1.199970  [  336/ 3200]\n",
            "loss: 1.326686  [  352/ 3200]\n",
            "loss: 1.229827  [  368/ 3200]\n",
            "loss: 1.313503  [  384/ 3200]\n",
            "loss: 1.167913  [  400/ 3200]\n",
            "loss: 1.202546  [  416/ 3200]\n",
            "loss: 1.309469  [  432/ 3200]\n",
            "loss: 1.198571  [  448/ 3200]\n",
            "loss: 1.220380  [  464/ 3200]\n",
            "loss: 1.128780  [  480/ 3200]\n",
            "loss: 1.224595  [  496/ 3200]\n",
            "loss: 1.267385  [  512/ 3200]\n",
            "loss: 1.209941  [  528/ 3200]\n",
            "loss: 1.197425  [  544/ 3200]\n",
            "loss: 1.216977  [  560/ 3200]\n",
            "loss: 1.203477  [  576/ 3200]\n",
            "loss: 1.234274  [  592/ 3200]\n",
            "loss: 1.205250  [  608/ 3200]\n",
            "loss: 1.091718  [  624/ 3200]\n",
            "loss: 1.321202  [  640/ 3200]\n",
            "loss: 1.213418  [  656/ 3200]\n",
            "loss: 1.190133  [  672/ 3200]\n",
            "loss: 1.280698  [  688/ 3200]\n",
            "loss: 1.172592  [  704/ 3200]\n",
            "loss: 1.222426  [  720/ 3200]\n",
            "loss: 1.275450  [  736/ 3200]\n",
            "loss: 1.273671  [  752/ 3200]\n",
            "loss: 1.145470  [  768/ 3200]\n",
            "loss: 1.234341  [  784/ 3200]\n",
            "loss: 1.284910  [  800/ 3200]\n",
            "loss: 1.319319  [  816/ 3200]\n",
            "loss: 1.112383  [  832/ 3200]\n",
            "loss: 1.370986  [  848/ 3200]\n",
            "loss: 1.168006  [  864/ 3200]\n",
            "loss: 1.146402  [  880/ 3200]\n",
            "loss: 1.227082  [  896/ 3200]\n",
            "loss: 1.188033  [  912/ 3200]\n",
            "loss: 0.987263  [  928/ 3200]\n",
            "loss: 1.357377  [  944/ 3200]\n",
            "loss: 1.114511  [  960/ 3200]\n",
            "loss: 1.150682  [  976/ 3200]\n",
            "loss: 1.152374  [  992/ 3200]\n",
            "loss: 1.154960  [ 1008/ 3200]\n",
            "loss: 1.210280  [ 1024/ 3200]\n",
            "loss: 1.124682  [ 1040/ 3200]\n",
            "loss: 1.334218  [ 1056/ 3200]\n",
            "loss: 1.191668  [ 1072/ 3200]\n",
            "loss: 1.077425  [ 1088/ 3200]\n",
            "loss: 1.173878  [ 1104/ 3200]\n",
            "loss: 1.243711  [ 1120/ 3200]\n",
            "loss: 1.221737  [ 1136/ 3200]\n",
            "loss: 1.141069  [ 1152/ 3200]\n",
            "loss: 1.128581  [ 1168/ 3200]\n",
            "loss: 1.345481  [ 1184/ 3200]\n",
            "loss: 1.310423  [ 1200/ 3200]\n",
            "loss: 1.251218  [ 1216/ 3200]\n",
            "loss: 1.231540  [ 1232/ 3200]\n",
            "loss: 1.184240  [ 1248/ 3200]\n",
            "loss: 1.218082  [ 1264/ 3200]\n",
            "loss: 1.083070  [ 1280/ 3200]\n",
            "loss: 1.227383  [ 1296/ 3200]\n",
            "loss: 1.151771  [ 1312/ 3200]\n",
            "loss: 1.206464  [ 1328/ 3200]\n",
            "loss: 1.210193  [ 1344/ 3200]\n",
            "loss: 1.235819  [ 1360/ 3200]\n",
            "loss: 1.132721  [ 1376/ 3200]\n",
            "loss: 1.286940  [ 1392/ 3200]\n",
            "loss: 1.068118  [ 1408/ 3200]\n",
            "loss: 1.154490  [ 1424/ 3200]\n",
            "loss: 1.232785  [ 1440/ 3200]\n",
            "loss: 1.329028  [ 1456/ 3200]\n",
            "loss: 1.239479  [ 1472/ 3200]\n",
            "loss: 1.242168  [ 1488/ 3200]\n",
            "loss: 1.142056  [ 1504/ 3200]\n",
            "loss: 1.209676  [ 1520/ 3200]\n",
            "loss: 1.181572  [ 1536/ 3200]\n",
            "loss: 1.145640  [ 1552/ 3200]\n",
            "loss: 1.198193  [ 1568/ 3200]\n",
            "loss: 1.169661  [ 1584/ 3200]\n",
            "loss: 1.324538  [ 1600/ 3200]\n",
            "loss: 1.294016  [ 1616/ 3200]\n",
            "loss: 1.147791  [ 1632/ 3200]\n",
            "loss: 1.303330  [ 1648/ 3200]\n",
            "loss: 1.290997  [ 1664/ 3200]\n",
            "loss: 1.213667  [ 1680/ 3200]\n",
            "loss: 1.127560  [ 1696/ 3200]\n",
            "loss: 1.196877  [ 1712/ 3200]\n",
            "loss: 1.222299  [ 1728/ 3200]\n",
            "loss: 1.163406  [ 1744/ 3200]\n",
            "loss: 1.180075  [ 1760/ 3200]\n",
            "loss: 1.221480  [ 1776/ 3200]\n",
            "loss: 1.198672  [ 1792/ 3200]\n",
            "loss: 1.194692  [ 1808/ 3200]\n",
            "loss: 1.202328  [ 1824/ 3200]\n",
            "loss: 1.156956  [ 1840/ 3200]\n",
            "loss: 1.079857  [ 1856/ 3200]\n",
            "loss: 1.179935  [ 1872/ 3200]\n",
            "loss: 1.203761  [ 1888/ 3200]\n",
            "loss: 1.121381  [ 1904/ 3200]\n",
            "loss: 1.106539  [ 1920/ 3200]\n",
            "loss: 1.151026  [ 1936/ 3200]\n",
            "loss: 1.140757  [ 1952/ 3200]\n",
            "loss: 1.305451  [ 1968/ 3200]\n",
            "loss: 1.261922  [ 1984/ 3200]\n",
            "loss: 1.299755  [ 2000/ 3200]\n",
            "loss: 1.296237  [ 2016/ 3200]\n",
            "loss: 1.084097  [ 2032/ 3200]\n",
            "loss: 1.233022  [ 2048/ 3200]\n",
            "loss: 1.211943  [ 2064/ 3200]\n",
            "loss: 1.267515  [ 2080/ 3200]\n",
            "loss: 1.361870  [ 2096/ 3200]\n",
            "loss: 1.084826  [ 2112/ 3200]\n",
            "loss: 1.100977  [ 2128/ 3200]\n",
            "loss: 1.126896  [ 2144/ 3200]\n",
            "loss: 1.205083  [ 2160/ 3200]\n",
            "loss: 1.202135  [ 2176/ 3200]\n",
            "loss: 1.126409  [ 2192/ 3200]\n",
            "loss: 1.107617  [ 2208/ 3200]\n",
            "loss: 1.279930  [ 2224/ 3200]\n",
            "loss: 1.275398  [ 2240/ 3200]\n",
            "loss: 1.191655  [ 2256/ 3200]\n",
            "loss: 1.174767  [ 2272/ 3200]\n",
            "loss: 1.117442  [ 2288/ 3200]\n",
            "loss: 1.066869  [ 2304/ 3200]\n",
            "loss: 1.048632  [ 2320/ 3200]\n",
            "loss: 1.304288  [ 2336/ 3200]\n",
            "loss: 1.284685  [ 2352/ 3200]\n",
            "loss: 1.204024  [ 2368/ 3200]\n",
            "loss: 1.047136  [ 2384/ 3200]\n",
            "loss: 1.255141  [ 2400/ 3200]\n",
            "loss: 1.303109  [ 2416/ 3200]\n",
            "loss: 1.130413  [ 2432/ 3200]\n",
            "loss: 1.050303  [ 2448/ 3200]\n",
            "loss: 1.236481  [ 2464/ 3200]\n",
            "loss: 1.266374  [ 2480/ 3200]\n",
            "loss: 1.101353  [ 2496/ 3200]\n",
            "loss: 1.229130  [ 2512/ 3200]\n",
            "loss: 1.290703  [ 2528/ 3200]\n",
            "loss: 1.133489  [ 2544/ 3200]\n",
            "loss: 1.187787  [ 2560/ 3200]\n",
            "loss: 1.210648  [ 2576/ 3200]\n",
            "loss: 1.153174  [ 2592/ 3200]\n",
            "loss: 1.344604  [ 2608/ 3200]\n",
            "loss: 1.159145  [ 2624/ 3200]\n",
            "loss: 1.304420  [ 2640/ 3200]\n",
            "loss: 1.205029  [ 2656/ 3200]\n",
            "loss: 1.213284  [ 2672/ 3200]\n",
            "loss: 1.316064  [ 2688/ 3200]\n",
            "loss: 1.206859  [ 2704/ 3200]\n",
            "loss: 1.123601  [ 2720/ 3200]\n",
            "loss: 1.088132  [ 2736/ 3200]\n",
            "loss: 0.973675  [ 2752/ 3200]\n",
            "loss: 1.019056  [ 2768/ 3200]\n",
            "loss: 1.227052  [ 2784/ 3200]\n",
            "loss: 1.138245  [ 2800/ 3200]\n",
            "loss: 1.219945  [ 2816/ 3200]\n",
            "loss: 1.243446  [ 2832/ 3200]\n",
            "loss: 1.190185  [ 2848/ 3200]\n",
            "loss: 1.111869  [ 2864/ 3200]\n",
            "loss: 1.137758  [ 2880/ 3200]\n",
            "loss: 1.237644  [ 2896/ 3200]\n",
            "loss: 1.117684  [ 2912/ 3200]\n",
            "loss: 1.294279  [ 2928/ 3200]\n",
            "loss: 1.165174  [ 2944/ 3200]\n",
            "loss: 1.263593  [ 2960/ 3200]\n",
            "loss: 1.278112  [ 2976/ 3200]\n",
            "loss: 1.130574  [ 2992/ 3200]\n",
            "loss: 1.150180  [ 3008/ 3200]\n",
            "loss: 1.277497  [ 3024/ 3200]\n",
            "loss: 1.152138  [ 3040/ 3200]\n",
            "loss: 1.181532  [ 3056/ 3200]\n",
            "loss: 1.209430  [ 3072/ 3200]\n",
            "loss: 1.187418  [ 3088/ 3200]\n",
            "loss: 1.203331  [ 3104/ 3200]\n",
            "loss: 1.190505  [ 3120/ 3200]\n",
            "loss: 1.174396  [ 3136/ 3200]\n",
            "loss: 1.175133  [ 3152/ 3200]\n",
            "loss: 1.148202  [ 3168/ 3200]\n",
            "loss: 1.196748  [ 3184/ 3200]\n",
            "current epoch: 9\n",
            "\n",
            "loss: 1.201741  [    0/ 3200]\n",
            "loss: 1.097452  [   16/ 3200]\n",
            "loss: 1.191902  [   32/ 3200]\n",
            "loss: 1.209511  [   48/ 3200]\n",
            "loss: 1.144288  [   64/ 3200]\n",
            "loss: 1.239386  [   80/ 3200]\n",
            "loss: 1.280649  [   96/ 3200]\n",
            "loss: 1.050229  [  112/ 3200]\n",
            "loss: 1.149969  [  128/ 3200]\n",
            "loss: 1.352744  [  144/ 3200]\n",
            "loss: 1.269192  [  160/ 3200]\n",
            "loss: 1.135327  [  176/ 3200]\n",
            "loss: 1.142451  [  192/ 3200]\n",
            "loss: 1.095415  [  208/ 3200]\n",
            "loss: 1.004705  [  224/ 3200]\n",
            "loss: 1.192653  [  240/ 3200]\n",
            "loss: 1.105035  [  256/ 3200]\n",
            "loss: 1.080531  [  272/ 3200]\n",
            "loss: 1.255247  [  288/ 3200]\n",
            "loss: 1.105205  [  304/ 3200]\n",
            "loss: 1.195027  [  320/ 3200]\n",
            "loss: 1.288546  [  336/ 3200]\n",
            "loss: 1.180984  [  352/ 3200]\n",
            "loss: 1.099740  [  368/ 3200]\n",
            "loss: 1.130475  [  384/ 3200]\n",
            "loss: 1.329634  [  400/ 3200]\n",
            "loss: 1.097592  [  416/ 3200]\n",
            "loss: 1.121682  [  432/ 3200]\n",
            "loss: 1.079698  [  448/ 3200]\n",
            "loss: 1.243350  [  464/ 3200]\n",
            "loss: 1.008384  [  480/ 3200]\n",
            "loss: 1.145294  [  496/ 3200]\n",
            "loss: 1.222249  [  512/ 3200]\n",
            "loss: 1.249412  [  528/ 3200]\n",
            "loss: 1.136754  [  544/ 3200]\n",
            "loss: 1.122024  [  560/ 3200]\n",
            "loss: 1.262733  [  576/ 3200]\n",
            "loss: 1.137144  [  592/ 3200]\n",
            "loss: 1.232483  [  608/ 3200]\n",
            "loss: 1.178632  [  624/ 3200]\n",
            "loss: 1.155437  [  640/ 3200]\n",
            "loss: 1.102500  [  656/ 3200]\n",
            "loss: 1.157768  [  672/ 3200]\n",
            "loss: 1.171882  [  688/ 3200]\n",
            "loss: 1.211449  [  704/ 3200]\n",
            "loss: 1.138782  [  720/ 3200]\n",
            "loss: 1.246797  [  736/ 3200]\n",
            "loss: 1.094133  [  752/ 3200]\n",
            "loss: 1.184444  [  768/ 3200]\n",
            "loss: 1.129324  [  784/ 3200]\n",
            "loss: 1.198902  [  800/ 3200]\n",
            "loss: 1.347637  [  816/ 3200]\n",
            "loss: 1.199219  [  832/ 3200]\n",
            "loss: 1.121418  [  848/ 3200]\n",
            "loss: 1.154135  [  864/ 3200]\n",
            "loss: 1.266398  [  880/ 3200]\n",
            "loss: 1.165093  [  896/ 3200]\n",
            "loss: 1.241015  [  912/ 3200]\n",
            "loss: 1.039793  [  928/ 3200]\n",
            "loss: 1.250393  [  944/ 3200]\n",
            "loss: 1.247037  [  960/ 3200]\n",
            "loss: 1.284785  [  976/ 3200]\n",
            "loss: 1.298223  [  992/ 3200]\n",
            "loss: 1.222916  [ 1008/ 3200]\n",
            "loss: 1.195132  [ 1024/ 3200]\n",
            "loss: 1.194001  [ 1040/ 3200]\n",
            "loss: 1.180511  [ 1056/ 3200]\n",
            "loss: 1.015526  [ 1072/ 3200]\n",
            "loss: 1.304003  [ 1088/ 3200]\n",
            "loss: 1.159576  [ 1104/ 3200]\n",
            "loss: 1.240143  [ 1120/ 3200]\n",
            "loss: 1.119485  [ 1136/ 3200]\n",
            "loss: 1.101340  [ 1152/ 3200]\n",
            "loss: 1.099924  [ 1168/ 3200]\n",
            "loss: 1.169504  [ 1184/ 3200]\n",
            "loss: 1.302794  [ 1200/ 3200]\n",
            "loss: 1.203638  [ 1216/ 3200]\n",
            "loss: 1.303847  [ 1232/ 3200]\n",
            "loss: 1.122694  [ 1248/ 3200]\n",
            "loss: 1.141137  [ 1264/ 3200]\n",
            "loss: 1.250461  [ 1280/ 3200]\n",
            "loss: 1.126758  [ 1296/ 3200]\n",
            "loss: 1.137382  [ 1312/ 3200]\n",
            "loss: 1.187881  [ 1328/ 3200]\n",
            "loss: 1.144240  [ 1344/ 3200]\n",
            "loss: 1.142441  [ 1360/ 3200]\n",
            "loss: 1.085566  [ 1376/ 3200]\n",
            "loss: 1.351089  [ 1392/ 3200]\n",
            "loss: 1.061684  [ 1408/ 3200]\n",
            "loss: 1.186820  [ 1424/ 3200]\n",
            "loss: 1.276839  [ 1440/ 3200]\n",
            "loss: 1.189526  [ 1456/ 3200]\n",
            "loss: 1.256274  [ 1472/ 3200]\n",
            "loss: 1.200719  [ 1488/ 3200]\n",
            "loss: 1.182373  [ 1504/ 3200]\n",
            "loss: 1.241060  [ 1520/ 3200]\n",
            "loss: 1.112844  [ 1536/ 3200]\n",
            "loss: 1.179795  [ 1552/ 3200]\n",
            "loss: 1.166299  [ 1568/ 3200]\n",
            "loss: 1.046869  [ 1584/ 3200]\n",
            "loss: 1.309937  [ 1600/ 3200]\n",
            "loss: 1.299195  [ 1616/ 3200]\n",
            "loss: 1.161714  [ 1632/ 3200]\n",
            "loss: 1.120177  [ 1648/ 3200]\n",
            "loss: 1.177501  [ 1664/ 3200]\n",
            "loss: 1.147302  [ 1680/ 3200]\n",
            "loss: 1.170655  [ 1696/ 3200]\n",
            "loss: 1.302783  [ 1712/ 3200]\n",
            "loss: 1.394666  [ 1728/ 3200]\n",
            "loss: 1.156813  [ 1744/ 3200]\n",
            "loss: 1.120790  [ 1760/ 3200]\n",
            "loss: 1.107093  [ 1776/ 3200]\n",
            "loss: 1.096161  [ 1792/ 3200]\n",
            "loss: 1.120403  [ 1808/ 3200]\n",
            "loss: 1.071328  [ 1824/ 3200]\n",
            "loss: 1.170189  [ 1840/ 3200]\n",
            "loss: 1.264803  [ 1856/ 3200]\n",
            "loss: 1.187618  [ 1872/ 3200]\n",
            "loss: 1.300434  [ 1888/ 3200]\n",
            "loss: 1.240081  [ 1904/ 3200]\n",
            "loss: 1.249370  [ 1920/ 3200]\n",
            "loss: 1.082088  [ 1936/ 3200]\n",
            "loss: 1.014329  [ 1952/ 3200]\n",
            "loss: 1.217099  [ 1968/ 3200]\n",
            "loss: 1.125649  [ 1984/ 3200]\n",
            "loss: 1.417953  [ 2000/ 3200]\n",
            "loss: 1.390399  [ 2016/ 3200]\n",
            "loss: 1.063065  [ 2032/ 3200]\n",
            "loss: 1.157300  [ 2048/ 3200]\n",
            "loss: 1.287155  [ 2064/ 3200]\n",
            "loss: 1.229198  [ 2080/ 3200]\n",
            "loss: 1.253423  [ 2096/ 3200]\n",
            "loss: 1.188411  [ 2112/ 3200]\n",
            "loss: 1.259843  [ 2128/ 3200]\n",
            "loss: 1.259251  [ 2144/ 3200]\n",
            "loss: 1.081259  [ 2160/ 3200]\n",
            "loss: 1.278921  [ 2176/ 3200]\n",
            "loss: 1.121171  [ 2192/ 3200]\n",
            "loss: 1.064410  [ 2208/ 3200]\n",
            "loss: 1.107437  [ 2224/ 3200]\n",
            "loss: 1.069957  [ 2240/ 3200]\n",
            "loss: 1.149059  [ 2256/ 3200]\n",
            "loss: 1.202594  [ 2272/ 3200]\n",
            "loss: 1.260736  [ 2288/ 3200]\n",
            "loss: 1.226889  [ 2304/ 3200]\n",
            "loss: 1.159819  [ 2320/ 3200]\n",
            "loss: 1.194476  [ 2336/ 3200]\n",
            "loss: 1.049157  [ 2352/ 3200]\n",
            "loss: 1.296166  [ 2368/ 3200]\n",
            "loss: 1.219912  [ 2384/ 3200]\n",
            "loss: 1.139272  [ 2400/ 3200]\n",
            "loss: 1.236080  [ 2416/ 3200]\n",
            "loss: 1.221716  [ 2432/ 3200]\n",
            "loss: 1.135308  [ 2448/ 3200]\n",
            "loss: 1.212054  [ 2464/ 3200]\n",
            "loss: 1.216140  [ 2480/ 3200]\n",
            "loss: 1.055539  [ 2496/ 3200]\n",
            "loss: 1.078579  [ 2512/ 3200]\n",
            "loss: 1.097996  [ 2528/ 3200]\n",
            "loss: 1.236987  [ 2544/ 3200]\n",
            "loss: 1.200528  [ 2560/ 3200]\n",
            "loss: 1.125388  [ 2576/ 3200]\n",
            "loss: 1.444211  [ 2592/ 3200]\n",
            "loss: 1.075246  [ 2608/ 3200]\n",
            "loss: 1.110566  [ 2624/ 3200]\n",
            "loss: 1.102697  [ 2640/ 3200]\n",
            "loss: 1.138338  [ 2656/ 3200]\n",
            "loss: 1.202341  [ 2672/ 3200]\n",
            "loss: 1.160841  [ 2688/ 3200]\n",
            "loss: 1.282910  [ 2704/ 3200]\n",
            "loss: 1.179907  [ 2720/ 3200]\n",
            "loss: 1.210172  [ 2736/ 3200]\n",
            "loss: 1.271393  [ 2752/ 3200]\n",
            "loss: 1.064573  [ 2768/ 3200]\n",
            "loss: 0.936455  [ 2784/ 3200]\n",
            "loss: 1.170989  [ 2800/ 3200]\n",
            "loss: 1.129137  [ 2816/ 3200]\n",
            "loss: 1.177602  [ 2832/ 3200]\n",
            "loss: 1.152384  [ 2848/ 3200]\n",
            "loss: 1.031357  [ 2864/ 3200]\n",
            "loss: 1.243692  [ 2880/ 3200]\n",
            "loss: 1.280924  [ 2896/ 3200]\n",
            "loss: 1.186659  [ 2912/ 3200]\n",
            "loss: 1.073687  [ 2928/ 3200]\n",
            "loss: 1.169769  [ 2944/ 3200]\n",
            "loss: 1.155705  [ 2960/ 3200]\n",
            "loss: 1.144423  [ 2976/ 3200]\n",
            "loss: 1.118173  [ 2992/ 3200]\n",
            "loss: 1.096537  [ 3008/ 3200]\n",
            "loss: 1.167138  [ 3024/ 3200]\n",
            "loss: 0.951673  [ 3040/ 3200]\n",
            "loss: 1.129273  [ 3056/ 3200]\n",
            "loss: 1.357916  [ 3072/ 3200]\n",
            "loss: 1.107576  [ 3088/ 3200]\n",
            "loss: 1.112329  [ 3104/ 3200]\n",
            "loss: 1.044090  [ 3120/ 3200]\n",
            "loss: 1.046020  [ 3136/ 3200]\n",
            "loss: 1.119569  [ 3152/ 3200]\n",
            "loss: 1.205817  [ 3168/ 3200]\n",
            "loss: 1.182536  [ 3184/ 3200]\n",
            "current epoch: 10\n",
            "\n",
            "loss: 1.069287  [    0/ 3200]\n",
            "loss: 0.971088  [   16/ 3200]\n",
            "loss: 1.215100  [   32/ 3200]\n",
            "loss: 1.248073  [   48/ 3200]\n",
            "loss: 1.287144  [   64/ 3200]\n",
            "loss: 1.304893  [   80/ 3200]\n",
            "loss: 1.102263  [   96/ 3200]\n",
            "loss: 1.178168  [  112/ 3200]\n",
            "loss: 1.201588  [  128/ 3200]\n",
            "loss: 1.092571  [  144/ 3200]\n",
            "loss: 1.095002  [  160/ 3200]\n",
            "loss: 1.058286  [  176/ 3200]\n",
            "loss: 1.169499  [  192/ 3200]\n",
            "loss: 1.176825  [  208/ 3200]\n",
            "loss: 1.155265  [  224/ 3200]\n",
            "loss: 1.157222  [  240/ 3200]\n",
            "loss: 1.097761  [  256/ 3200]\n",
            "loss: 1.225060  [  272/ 3200]\n",
            "loss: 1.151217  [  288/ 3200]\n",
            "loss: 1.117743  [  304/ 3200]\n",
            "loss: 1.221819  [  320/ 3200]\n",
            "loss: 1.133375  [  336/ 3200]\n",
            "loss: 1.087535  [  352/ 3200]\n",
            "loss: 1.032957  [  368/ 3200]\n",
            "loss: 1.130487  [  384/ 3200]\n",
            "loss: 1.147398  [  400/ 3200]\n",
            "loss: 1.035889  [  416/ 3200]\n",
            "loss: 1.174737  [  432/ 3200]\n",
            "loss: 1.155920  [  448/ 3200]\n",
            "loss: 1.104225  [  464/ 3200]\n",
            "loss: 1.137155  [  480/ 3200]\n",
            "loss: 1.332813  [  496/ 3200]\n",
            "loss: 1.278908  [  512/ 3200]\n",
            "loss: 1.171447  [  528/ 3200]\n",
            "loss: 1.198279  [  544/ 3200]\n",
            "loss: 1.249674  [  560/ 3200]\n",
            "loss: 1.186681  [  576/ 3200]\n",
            "loss: 1.192061  [  592/ 3200]\n",
            "loss: 1.029716  [  608/ 3200]\n",
            "loss: 1.287480  [  624/ 3200]\n",
            "loss: 1.140790  [  640/ 3200]\n",
            "loss: 1.044384  [  656/ 3200]\n",
            "loss: 1.191856  [  672/ 3200]\n",
            "loss: 1.327498  [  688/ 3200]\n",
            "loss: 1.112928  [  704/ 3200]\n",
            "loss: 1.252465  [  720/ 3200]\n",
            "loss: 1.286707  [  736/ 3200]\n",
            "loss: 1.176387  [  752/ 3200]\n",
            "loss: 1.134819  [  768/ 3200]\n",
            "loss: 1.166049  [  784/ 3200]\n",
            "loss: 1.109249  [  800/ 3200]\n",
            "loss: 1.168365  [  816/ 3200]\n",
            "loss: 1.106772  [  832/ 3200]\n",
            "loss: 1.296247  [  848/ 3200]\n",
            "loss: 1.240925  [  864/ 3200]\n",
            "loss: 1.068074  [  880/ 3200]\n",
            "loss: 1.141880  [  896/ 3200]\n",
            "loss: 1.190352  [  912/ 3200]\n",
            "loss: 1.251826  [  928/ 3200]\n",
            "loss: 1.122349  [  944/ 3200]\n",
            "loss: 1.076722  [  960/ 3200]\n",
            "loss: 1.094731  [  976/ 3200]\n",
            "loss: 1.235632  [  992/ 3200]\n",
            "loss: 1.152853  [ 1008/ 3200]\n",
            "loss: 1.193920  [ 1024/ 3200]\n",
            "loss: 1.246993  [ 1040/ 3200]\n",
            "loss: 1.225341  [ 1056/ 3200]\n",
            "loss: 1.195187  [ 1072/ 3200]\n",
            "loss: 1.118328  [ 1088/ 3200]\n",
            "loss: 1.225188  [ 1104/ 3200]\n",
            "loss: 1.104390  [ 1120/ 3200]\n",
            "loss: 0.990254  [ 1136/ 3200]\n",
            "loss: 1.008348  [ 1152/ 3200]\n",
            "loss: 1.334957  [ 1168/ 3200]\n",
            "loss: 1.052687  [ 1184/ 3200]\n",
            "loss: 1.047490  [ 1200/ 3200]\n",
            "loss: 1.256136  [ 1216/ 3200]\n",
            "loss: 1.012475  [ 1232/ 3200]\n",
            "loss: 1.128846  [ 1248/ 3200]\n",
            "loss: 1.122242  [ 1264/ 3200]\n",
            "loss: 1.224870  [ 1280/ 3200]\n",
            "loss: 1.030470  [ 1296/ 3200]\n",
            "loss: 1.075880  [ 1312/ 3200]\n",
            "loss: 1.099427  [ 1328/ 3200]\n",
            "loss: 0.956469  [ 1344/ 3200]\n",
            "loss: 1.257769  [ 1360/ 3200]\n",
            "loss: 1.209219  [ 1376/ 3200]\n",
            "loss: 1.075335  [ 1392/ 3200]\n",
            "loss: 1.255481  [ 1408/ 3200]\n",
            "loss: 1.339942  [ 1424/ 3200]\n",
            "loss: 1.121301  [ 1440/ 3200]\n",
            "loss: 1.265740  [ 1456/ 3200]\n",
            "loss: 1.164143  [ 1472/ 3200]\n",
            "loss: 1.125199  [ 1488/ 3200]\n",
            "loss: 1.408378  [ 1504/ 3200]\n",
            "loss: 1.199441  [ 1520/ 3200]\n",
            "loss: 1.264650  [ 1536/ 3200]\n",
            "loss: 1.213943  [ 1552/ 3200]\n",
            "loss: 1.186216  [ 1568/ 3200]\n",
            "loss: 0.905560  [ 1584/ 3200]\n",
            "loss: 1.139438  [ 1600/ 3200]\n",
            "loss: 1.213327  [ 1616/ 3200]\n",
            "loss: 0.976237  [ 1632/ 3200]\n",
            "loss: 1.068279  [ 1648/ 3200]\n",
            "loss: 1.098028  [ 1664/ 3200]\n",
            "loss: 1.208640  [ 1680/ 3200]\n",
            "loss: 1.301414  [ 1696/ 3200]\n",
            "loss: 1.120985  [ 1712/ 3200]\n",
            "loss: 1.141348  [ 1728/ 3200]\n",
            "loss: 1.151211  [ 1744/ 3200]\n",
            "loss: 1.169057  [ 1760/ 3200]\n",
            "loss: 1.120434  [ 1776/ 3200]\n",
            "loss: 1.194101  [ 1792/ 3200]\n",
            "loss: 1.146203  [ 1808/ 3200]\n",
            "loss: 1.175200  [ 1824/ 3200]\n",
            "loss: 1.119243  [ 1840/ 3200]\n",
            "loss: 1.131255  [ 1856/ 3200]\n",
            "loss: 1.149469  [ 1872/ 3200]\n",
            "loss: 1.410110  [ 1888/ 3200]\n",
            "loss: 0.923259  [ 1904/ 3200]\n",
            "loss: 1.123897  [ 1920/ 3200]\n",
            "loss: 1.137699  [ 1936/ 3200]\n",
            "loss: 1.038422  [ 1952/ 3200]\n",
            "loss: 1.131815  [ 1968/ 3200]\n",
            "loss: 1.019331  [ 1984/ 3200]\n",
            "loss: 1.119172  [ 2000/ 3200]\n",
            "loss: 1.190533  [ 2016/ 3200]\n",
            "loss: 0.999069  [ 2032/ 3200]\n",
            "loss: 1.227149  [ 2048/ 3200]\n",
            "loss: 1.172679  [ 2064/ 3200]\n",
            "loss: 1.054740  [ 2080/ 3200]\n",
            "loss: 1.128282  [ 2096/ 3200]\n",
            "loss: 1.103183  [ 2112/ 3200]\n",
            "loss: 1.245319  [ 2128/ 3200]\n",
            "loss: 1.166955  [ 2144/ 3200]\n",
            "loss: 1.008529  [ 2160/ 3200]\n",
            "loss: 1.050932  [ 2176/ 3200]\n",
            "loss: 0.937467  [ 2192/ 3200]\n",
            "loss: 1.207232  [ 2208/ 3200]\n",
            "loss: 0.935652  [ 2224/ 3200]\n",
            "loss: 1.282430  [ 2240/ 3200]\n",
            "loss: 1.123532  [ 2256/ 3200]\n",
            "loss: 1.137276  [ 2272/ 3200]\n",
            "loss: 1.051929  [ 2288/ 3200]\n",
            "loss: 1.143102  [ 2304/ 3200]\n",
            "loss: 1.204312  [ 2320/ 3200]\n",
            "loss: 1.168851  [ 2336/ 3200]\n",
            "loss: 1.243079  [ 2352/ 3200]\n",
            "loss: 1.036919  [ 2368/ 3200]\n",
            "loss: 1.169613  [ 2384/ 3200]\n",
            "loss: 1.185938  [ 2400/ 3200]\n",
            "loss: 1.163402  [ 2416/ 3200]\n",
            "loss: 1.141344  [ 2432/ 3200]\n",
            "loss: 1.314065  [ 2448/ 3200]\n",
            "loss: 1.206373  [ 2464/ 3200]\n",
            "loss: 1.174473  [ 2480/ 3200]\n",
            "loss: 1.033168  [ 2496/ 3200]\n",
            "loss: 1.172641  [ 2512/ 3200]\n",
            "loss: 1.121816  [ 2528/ 3200]\n",
            "loss: 1.216143  [ 2544/ 3200]\n",
            "loss: 1.164316  [ 2560/ 3200]\n",
            "loss: 1.066026  [ 2576/ 3200]\n",
            "loss: 1.099637  [ 2592/ 3200]\n",
            "loss: 1.212463  [ 2608/ 3200]\n",
            "loss: 1.196555  [ 2624/ 3200]\n",
            "loss: 0.904314  [ 2640/ 3200]\n",
            "loss: 1.176400  [ 2656/ 3200]\n",
            "loss: 1.201278  [ 2672/ 3200]\n",
            "loss: 1.170807  [ 2688/ 3200]\n",
            "loss: 1.151869  [ 2704/ 3200]\n",
            "loss: 1.247008  [ 2720/ 3200]\n",
            "loss: 1.032754  [ 2736/ 3200]\n",
            "loss: 1.381492  [ 2752/ 3200]\n",
            "loss: 1.234900  [ 2768/ 3200]\n",
            "loss: 1.243086  [ 2784/ 3200]\n",
            "loss: 1.010515  [ 2800/ 3200]\n",
            "loss: 1.085098  [ 2816/ 3200]\n",
            "loss: 1.257844  [ 2832/ 3200]\n",
            "loss: 1.266477  [ 2848/ 3200]\n",
            "loss: 1.264792  [ 2864/ 3200]\n",
            "loss: 1.099048  [ 2880/ 3200]\n",
            "loss: 1.053008  [ 2896/ 3200]\n",
            "loss: 1.176758  [ 2912/ 3200]\n",
            "loss: 1.024008  [ 2928/ 3200]\n",
            "loss: 1.170677  [ 2944/ 3200]\n",
            "loss: 1.309869  [ 2960/ 3200]\n",
            "loss: 1.159004  [ 2976/ 3200]\n",
            "loss: 1.052484  [ 2992/ 3200]\n",
            "loss: 1.039201  [ 3008/ 3200]\n",
            "loss: 1.122759  [ 3024/ 3200]\n",
            "loss: 1.289733  [ 3040/ 3200]\n",
            "loss: 1.107701  [ 3056/ 3200]\n",
            "loss: 1.240855  [ 3072/ 3200]\n",
            "loss: 1.166227  [ 3088/ 3200]\n",
            "loss: 1.104784  [ 3104/ 3200]\n",
            "loss: 1.176178  [ 3120/ 3200]\n",
            "loss: 0.893898  [ 3136/ 3200]\n",
            "loss: 1.075446  [ 3152/ 3200]\n",
            "loss: 1.147708  [ 3168/ 3200]\n",
            "loss: 1.098619  [ 3184/ 3200]\n",
            "current epoch: 11\n",
            "\n",
            "loss: 1.163428  [    0/ 3200]\n",
            "loss: 1.174575  [   16/ 3200]\n",
            "loss: 1.228422  [   32/ 3200]\n",
            "loss: 1.128438  [   48/ 3200]\n",
            "loss: 0.962557  [   64/ 3200]\n",
            "loss: 1.238469  [   80/ 3200]\n",
            "loss: 1.058143  [   96/ 3200]\n",
            "loss: 1.045121  [  112/ 3200]\n",
            "loss: 1.113942  [  128/ 3200]\n",
            "loss: 1.123297  [  144/ 3200]\n",
            "loss: 0.924849  [  160/ 3200]\n",
            "loss: 1.316943  [  176/ 3200]\n",
            "loss: 1.122637  [  192/ 3200]\n",
            "loss: 1.146245  [  208/ 3200]\n",
            "loss: 1.096496  [  224/ 3200]\n",
            "loss: 1.097147  [  240/ 3200]\n",
            "loss: 1.246116  [  256/ 3200]\n",
            "loss: 1.133542  [  272/ 3200]\n",
            "loss: 1.009855  [  288/ 3200]\n",
            "loss: 1.039068  [  304/ 3200]\n",
            "loss: 1.155907  [  320/ 3200]\n",
            "loss: 1.229250  [  336/ 3200]\n",
            "loss: 1.299707  [  352/ 3200]\n",
            "loss: 1.194156  [  368/ 3200]\n",
            "loss: 1.075749  [  384/ 3200]\n",
            "loss: 1.198658  [  400/ 3200]\n",
            "loss: 1.284707  [  416/ 3200]\n",
            "loss: 1.080215  [  432/ 3200]\n",
            "loss: 1.291591  [  448/ 3200]\n",
            "loss: 1.202130  [  464/ 3200]\n",
            "loss: 1.086105  [  480/ 3200]\n",
            "loss: 0.915471  [  496/ 3200]\n",
            "loss: 1.164652  [  512/ 3200]\n",
            "loss: 0.943388  [  528/ 3200]\n",
            "loss: 1.175309  [  544/ 3200]\n",
            "loss: 1.100138  [  560/ 3200]\n",
            "loss: 1.135238  [  576/ 3200]\n",
            "loss: 1.101870  [  592/ 3200]\n",
            "loss: 0.998146  [  608/ 3200]\n",
            "loss: 1.070032  [  624/ 3200]\n",
            "loss: 1.034106  [  640/ 3200]\n",
            "loss: 1.209328  [  656/ 3200]\n",
            "loss: 1.241376  [  672/ 3200]\n",
            "loss: 1.039845  [  688/ 3200]\n",
            "loss: 0.989788  [  704/ 3200]\n",
            "loss: 1.331042  [  720/ 3200]\n",
            "loss: 1.274934  [  736/ 3200]\n",
            "loss: 1.145428  [  752/ 3200]\n",
            "loss: 1.101440  [  768/ 3200]\n",
            "loss: 1.081848  [  784/ 3200]\n",
            "loss: 1.155944  [  800/ 3200]\n",
            "loss: 1.152489  [  816/ 3200]\n",
            "loss: 1.130100  [  832/ 3200]\n",
            "loss: 1.254624  [  848/ 3200]\n",
            "loss: 1.102102  [  864/ 3200]\n",
            "loss: 1.027163  [  880/ 3200]\n",
            "loss: 1.296448  [  896/ 3200]\n",
            "loss: 1.160352  [  912/ 3200]\n",
            "loss: 1.115540  [  928/ 3200]\n",
            "loss: 1.055797  [  944/ 3200]\n",
            "loss: 1.168024  [  960/ 3200]\n",
            "loss: 1.097129  [  976/ 3200]\n",
            "loss: 1.212142  [  992/ 3200]\n",
            "loss: 1.281256  [ 1008/ 3200]\n",
            "loss: 1.179688  [ 1024/ 3200]\n",
            "loss: 1.070449  [ 1040/ 3200]\n",
            "loss: 1.189566  [ 1056/ 3200]\n",
            "loss: 1.225685  [ 1072/ 3200]\n",
            "loss: 1.149523  [ 1088/ 3200]\n",
            "loss: 1.137565  [ 1104/ 3200]\n",
            "loss: 1.164238  [ 1120/ 3200]\n",
            "loss: 1.214344  [ 1136/ 3200]\n",
            "loss: 1.028349  [ 1152/ 3200]\n",
            "loss: 1.133935  [ 1168/ 3200]\n",
            "loss: 1.093402  [ 1184/ 3200]\n",
            "loss: 1.109567  [ 1200/ 3200]\n",
            "loss: 1.057474  [ 1216/ 3200]\n",
            "loss: 1.272303  [ 1232/ 3200]\n",
            "loss: 1.163979  [ 1248/ 3200]\n",
            "loss: 1.269373  [ 1264/ 3200]\n",
            "loss: 1.221176  [ 1280/ 3200]\n",
            "loss: 1.049377  [ 1296/ 3200]\n",
            "loss: 1.257289  [ 1312/ 3200]\n",
            "loss: 1.232903  [ 1328/ 3200]\n",
            "loss: 1.106563  [ 1344/ 3200]\n",
            "loss: 1.120282  [ 1360/ 3200]\n",
            "loss: 1.196919  [ 1376/ 3200]\n",
            "loss: 1.254656  [ 1392/ 3200]\n",
            "loss: 1.168296  [ 1408/ 3200]\n",
            "loss: 1.279690  [ 1424/ 3200]\n",
            "loss: 0.961010  [ 1440/ 3200]\n",
            "loss: 1.112808  [ 1456/ 3200]\n",
            "loss: 1.033907  [ 1472/ 3200]\n",
            "loss: 1.151123  [ 1488/ 3200]\n",
            "loss: 1.299938  [ 1504/ 3200]\n",
            "loss: 1.037771  [ 1520/ 3200]\n",
            "loss: 1.153533  [ 1536/ 3200]\n",
            "loss: 1.225584  [ 1552/ 3200]\n",
            "loss: 1.134133  [ 1568/ 3200]\n",
            "loss: 1.027947  [ 1584/ 3200]\n",
            "loss: 1.288974  [ 1600/ 3200]\n",
            "loss: 1.129310  [ 1616/ 3200]\n",
            "loss: 1.132751  [ 1632/ 3200]\n",
            "loss: 1.389606  [ 1648/ 3200]\n",
            "loss: 1.172991  [ 1664/ 3200]\n",
            "loss: 1.098334  [ 1680/ 3200]\n",
            "loss: 1.071666  [ 1696/ 3200]\n",
            "loss: 1.156118  [ 1712/ 3200]\n",
            "loss: 0.922166  [ 1728/ 3200]\n",
            "loss: 1.082160  [ 1744/ 3200]\n",
            "loss: 1.233413  [ 1760/ 3200]\n",
            "loss: 1.339127  [ 1776/ 3200]\n",
            "loss: 1.010685  [ 1792/ 3200]\n",
            "loss: 1.129845  [ 1808/ 3200]\n",
            "loss: 1.222332  [ 1824/ 3200]\n",
            "loss: 0.968066  [ 1840/ 3200]\n",
            "loss: 1.109424  [ 1856/ 3200]\n",
            "loss: 1.131824  [ 1872/ 3200]\n",
            "loss: 1.109774  [ 1888/ 3200]\n",
            "loss: 1.025342  [ 1904/ 3200]\n",
            "loss: 0.910848  [ 1920/ 3200]\n",
            "loss: 1.148886  [ 1936/ 3200]\n",
            "loss: 1.062425  [ 1952/ 3200]\n",
            "loss: 1.058058  [ 1968/ 3200]\n",
            "loss: 1.405287  [ 1984/ 3200]\n",
            "loss: 1.059993  [ 2000/ 3200]\n",
            "loss: 1.024730  [ 2016/ 3200]\n",
            "loss: 1.199041  [ 2032/ 3200]\n",
            "loss: 1.162901  [ 2048/ 3200]\n",
            "loss: 1.021830  [ 2064/ 3200]\n",
            "loss: 1.220453  [ 2080/ 3200]\n",
            "loss: 1.099607  [ 2096/ 3200]\n",
            "loss: 1.199337  [ 2112/ 3200]\n",
            "loss: 1.057820  [ 2128/ 3200]\n",
            "loss: 0.985332  [ 2144/ 3200]\n",
            "loss: 1.239160  [ 2160/ 3200]\n",
            "loss: 1.088717  [ 2176/ 3200]\n",
            "loss: 1.005989  [ 2192/ 3200]\n",
            "loss: 1.237444  [ 2208/ 3200]\n",
            "loss: 1.172222  [ 2224/ 3200]\n",
            "loss: 1.048689  [ 2240/ 3200]\n",
            "loss: 1.058113  [ 2256/ 3200]\n",
            "loss: 1.012416  [ 2272/ 3200]\n",
            "loss: 1.198014  [ 2288/ 3200]\n",
            "loss: 0.997284  [ 2304/ 3200]\n",
            "loss: 1.082710  [ 2320/ 3200]\n",
            "loss: 1.064119  [ 2336/ 3200]\n",
            "loss: 1.157057  [ 2352/ 3200]\n",
            "loss: 1.135150  [ 2368/ 3200]\n",
            "loss: 1.215731  [ 2384/ 3200]\n",
            "loss: 1.067942  [ 2400/ 3200]\n",
            "loss: 1.042879  [ 2416/ 3200]\n",
            "loss: 0.934106  [ 2432/ 3200]\n",
            "loss: 1.223859  [ 2448/ 3200]\n",
            "loss: 1.180525  [ 2464/ 3200]\n",
            "loss: 1.145546  [ 2480/ 3200]\n",
            "loss: 1.052556  [ 2496/ 3200]\n",
            "loss: 1.213502  [ 2512/ 3200]\n",
            "loss: 1.076919  [ 2528/ 3200]\n",
            "loss: 1.001606  [ 2544/ 3200]\n",
            "loss: 1.154535  [ 2560/ 3200]\n",
            "loss: 1.052814  [ 2576/ 3200]\n",
            "loss: 1.028700  [ 2592/ 3200]\n",
            "loss: 1.049675  [ 2608/ 3200]\n",
            "loss: 1.044299  [ 2624/ 3200]\n",
            "loss: 1.308957  [ 2640/ 3200]\n",
            "loss: 1.200527  [ 2656/ 3200]\n",
            "loss: 1.153315  [ 2672/ 3200]\n",
            "loss: 1.088287  [ 2688/ 3200]\n",
            "loss: 1.128894  [ 2704/ 3200]\n",
            "loss: 1.191074  [ 2720/ 3200]\n",
            "loss: 1.110730  [ 2736/ 3200]\n",
            "loss: 1.112865  [ 2752/ 3200]\n",
            "loss: 1.182013  [ 2768/ 3200]\n",
            "loss: 1.078065  [ 2784/ 3200]\n",
            "loss: 1.041530  [ 2800/ 3200]\n",
            "loss: 1.077067  [ 2816/ 3200]\n",
            "loss: 1.301964  [ 2832/ 3200]\n",
            "loss: 1.127927  [ 2848/ 3200]\n",
            "loss: 1.015026  [ 2864/ 3200]\n",
            "loss: 1.021117  [ 2880/ 3200]\n",
            "loss: 1.055836  [ 2896/ 3200]\n",
            "loss: 1.054480  [ 2912/ 3200]\n",
            "loss: 1.085829  [ 2928/ 3200]\n",
            "loss: 0.860448  [ 2944/ 3200]\n",
            "loss: 0.992387  [ 2960/ 3200]\n",
            "loss: 0.868216  [ 2976/ 3200]\n",
            "loss: 1.198028  [ 2992/ 3200]\n",
            "loss: 1.318671  [ 3008/ 3200]\n",
            "loss: 1.156492  [ 3024/ 3200]\n",
            "loss: 1.091752  [ 3040/ 3200]\n",
            "loss: 1.323107  [ 3056/ 3200]\n",
            "loss: 1.231247  [ 3072/ 3200]\n",
            "loss: 1.207569  [ 3088/ 3200]\n",
            "loss: 1.212621  [ 3104/ 3200]\n",
            "loss: 1.117491  [ 3120/ 3200]\n",
            "loss: 1.155509  [ 3136/ 3200]\n",
            "loss: 1.218828  [ 3152/ 3200]\n",
            "loss: 0.849549  [ 3168/ 3200]\n",
            "loss: 1.205601  [ 3184/ 3200]\n",
            "current epoch: 12\n",
            "\n",
            "loss: 1.004920  [    0/ 3200]\n",
            "loss: 1.170291  [   16/ 3200]\n",
            "loss: 1.114748  [   32/ 3200]\n",
            "loss: 1.224013  [   48/ 3200]\n",
            "loss: 1.101820  [   64/ 3200]\n",
            "loss: 1.289523  [   80/ 3200]\n",
            "loss: 1.155653  [   96/ 3200]\n",
            "loss: 1.007531  [  112/ 3200]\n",
            "loss: 1.038181  [  128/ 3200]\n",
            "loss: 1.011173  [  144/ 3200]\n",
            "loss: 1.196124  [  160/ 3200]\n",
            "loss: 1.144690  [  176/ 3200]\n",
            "loss: 1.045161  [  192/ 3200]\n",
            "loss: 0.880710  [  208/ 3200]\n",
            "loss: 1.110086  [  224/ 3200]\n",
            "loss: 0.956870  [  240/ 3200]\n",
            "loss: 1.157865  [  256/ 3200]\n",
            "loss: 1.015900  [  272/ 3200]\n",
            "loss: 1.257409  [  288/ 3200]\n",
            "loss: 1.111991  [  304/ 3200]\n",
            "loss: 1.110309  [  320/ 3200]\n",
            "loss: 1.096334  [  336/ 3200]\n",
            "loss: 1.142576  [  352/ 3200]\n",
            "loss: 1.188081  [  368/ 3200]\n",
            "loss: 1.286305  [  384/ 3200]\n",
            "loss: 0.992218  [  400/ 3200]\n",
            "loss: 1.189304  [  416/ 3200]\n",
            "loss: 0.998838  [  432/ 3200]\n",
            "loss: 1.075359  [  448/ 3200]\n",
            "loss: 1.039977  [  464/ 3200]\n",
            "loss: 1.153330  [  480/ 3200]\n",
            "loss: 1.239597  [  496/ 3200]\n",
            "loss: 0.972003  [  512/ 3200]\n",
            "loss: 1.340616  [  528/ 3200]\n",
            "loss: 1.017698  [  544/ 3200]\n",
            "loss: 1.001314  [  560/ 3200]\n",
            "loss: 1.106418  [  576/ 3200]\n",
            "loss: 0.967531  [  592/ 3200]\n",
            "loss: 1.256269  [  608/ 3200]\n",
            "loss: 1.249393  [  624/ 3200]\n",
            "loss: 1.115519  [  640/ 3200]\n",
            "loss: 0.992222  [  656/ 3200]\n",
            "loss: 1.148643  [  672/ 3200]\n",
            "loss: 1.024909  [  688/ 3200]\n",
            "loss: 1.232085  [  704/ 3200]\n",
            "loss: 1.074513  [  720/ 3200]\n",
            "loss: 1.104270  [  736/ 3200]\n",
            "loss: 1.193866  [  752/ 3200]\n",
            "loss: 1.130318  [  768/ 3200]\n",
            "loss: 0.934184  [  784/ 3200]\n",
            "loss: 1.046263  [  800/ 3200]\n",
            "loss: 1.187009  [  816/ 3200]\n",
            "loss: 0.976574  [  832/ 3200]\n",
            "loss: 1.040870  [  848/ 3200]\n",
            "loss: 1.144591  [  864/ 3200]\n",
            "loss: 1.129756  [  880/ 3200]\n",
            "loss: 1.126821  [  896/ 3200]\n",
            "loss: 1.259308  [  912/ 3200]\n",
            "loss: 1.149826  [  928/ 3200]\n",
            "loss: 0.988489  [  944/ 3200]\n",
            "loss: 1.156207  [  960/ 3200]\n",
            "loss: 1.360973  [  976/ 3200]\n",
            "loss: 1.129045  [  992/ 3200]\n",
            "loss: 1.190673  [ 1008/ 3200]\n",
            "loss: 1.091562  [ 1024/ 3200]\n",
            "loss: 1.330153  [ 1040/ 3200]\n",
            "loss: 1.051242  [ 1056/ 3200]\n",
            "loss: 1.050743  [ 1072/ 3200]\n",
            "loss: 0.991967  [ 1088/ 3200]\n",
            "loss: 1.113131  [ 1104/ 3200]\n",
            "loss: 1.243404  [ 1120/ 3200]\n",
            "loss: 1.151605  [ 1136/ 3200]\n",
            "loss: 1.089535  [ 1152/ 3200]\n",
            "loss: 1.210517  [ 1168/ 3200]\n",
            "loss: 1.212114  [ 1184/ 3200]\n",
            "loss: 1.232067  [ 1200/ 3200]\n",
            "loss: 1.143891  [ 1216/ 3200]\n",
            "loss: 1.004051  [ 1232/ 3200]\n",
            "loss: 1.047996  [ 1248/ 3200]\n",
            "loss: 1.104165  [ 1264/ 3200]\n",
            "loss: 1.068146  [ 1280/ 3200]\n",
            "loss: 0.940547  [ 1296/ 3200]\n",
            "loss: 1.193966  [ 1312/ 3200]\n",
            "loss: 1.104104  [ 1328/ 3200]\n",
            "loss: 1.038276  [ 1344/ 3200]\n",
            "loss: 1.122338  [ 1360/ 3200]\n",
            "loss: 1.176461  [ 1376/ 3200]\n",
            "loss: 1.269971  [ 1392/ 3200]\n",
            "loss: 1.147476  [ 1408/ 3200]\n",
            "loss: 1.194645  [ 1424/ 3200]\n",
            "loss: 1.047128  [ 1440/ 3200]\n",
            "loss: 1.196370  [ 1456/ 3200]\n",
            "loss: 1.196235  [ 1472/ 3200]\n",
            "loss: 0.971424  [ 1488/ 3200]\n",
            "loss: 1.167268  [ 1504/ 3200]\n",
            "loss: 1.038900  [ 1520/ 3200]\n",
            "loss: 1.016047  [ 1536/ 3200]\n",
            "loss: 1.086512  [ 1552/ 3200]\n",
            "loss: 1.207559  [ 1568/ 3200]\n",
            "loss: 1.127944  [ 1584/ 3200]\n",
            "loss: 1.108611  [ 1600/ 3200]\n",
            "loss: 1.074919  [ 1616/ 3200]\n",
            "loss: 1.180570  [ 1632/ 3200]\n",
            "loss: 1.164418  [ 1648/ 3200]\n",
            "loss: 1.046687  [ 1664/ 3200]\n",
            "loss: 0.871170  [ 1680/ 3200]\n",
            "loss: 1.227104  [ 1696/ 3200]\n",
            "loss: 1.063688  [ 1712/ 3200]\n",
            "loss: 1.030695  [ 1728/ 3200]\n",
            "loss: 1.027617  [ 1744/ 3200]\n",
            "loss: 0.929793  [ 1760/ 3200]\n",
            "loss: 1.130726  [ 1776/ 3200]\n",
            "loss: 1.071156  [ 1792/ 3200]\n",
            "loss: 1.153950  [ 1808/ 3200]\n",
            "loss: 1.245945  [ 1824/ 3200]\n",
            "loss: 1.167114  [ 1840/ 3200]\n",
            "loss: 1.102677  [ 1856/ 3200]\n",
            "loss: 1.148971  [ 1872/ 3200]\n",
            "loss: 1.258691  [ 1888/ 3200]\n",
            "loss: 1.055738  [ 1904/ 3200]\n",
            "loss: 1.217249  [ 1920/ 3200]\n",
            "loss: 1.125158  [ 1936/ 3200]\n",
            "loss: 1.034766  [ 1952/ 3200]\n",
            "loss: 1.029597  [ 1968/ 3200]\n",
            "loss: 1.104734  [ 1984/ 3200]\n",
            "loss: 1.161132  [ 2000/ 3200]\n",
            "loss: 1.288708  [ 2016/ 3200]\n",
            "loss: 0.902234  [ 2032/ 3200]\n",
            "loss: 1.144803  [ 2048/ 3200]\n",
            "loss: 1.261366  [ 2064/ 3200]\n",
            "loss: 1.003277  [ 2080/ 3200]\n",
            "loss: 1.018934  [ 2096/ 3200]\n",
            "loss: 1.202628  [ 2112/ 3200]\n",
            "loss: 1.043875  [ 2128/ 3200]\n",
            "loss: 1.004460  [ 2144/ 3200]\n",
            "loss: 1.075755  [ 2160/ 3200]\n",
            "loss: 1.225008  [ 2176/ 3200]\n",
            "loss: 1.044301  [ 2192/ 3200]\n",
            "loss: 0.965787  [ 2208/ 3200]\n",
            "loss: 1.238500  [ 2224/ 3200]\n",
            "loss: 0.997156  [ 2240/ 3200]\n",
            "loss: 1.030534  [ 2256/ 3200]\n",
            "loss: 1.123504  [ 2272/ 3200]\n",
            "loss: 1.091767  [ 2288/ 3200]\n",
            "loss: 1.060245  [ 2304/ 3200]\n",
            "loss: 1.147860  [ 2320/ 3200]\n",
            "loss: 1.100627  [ 2336/ 3200]\n",
            "loss: 1.102290  [ 2352/ 3200]\n",
            "loss: 1.217749  [ 2368/ 3200]\n",
            "loss: 1.173535  [ 2384/ 3200]\n",
            "loss: 1.174961  [ 2400/ 3200]\n",
            "loss: 1.057976  [ 2416/ 3200]\n",
            "loss: 1.174662  [ 2432/ 3200]\n",
            "loss: 1.210621  [ 2448/ 3200]\n",
            "loss: 1.042403  [ 2464/ 3200]\n",
            "loss: 1.114772  [ 2480/ 3200]\n",
            "loss: 1.192356  [ 2496/ 3200]\n",
            "loss: 1.118857  [ 2512/ 3200]\n",
            "loss: 0.970806  [ 2528/ 3200]\n",
            "loss: 1.102047  [ 2544/ 3200]\n",
            "loss: 1.100060  [ 2560/ 3200]\n",
            "loss: 0.917037  [ 2576/ 3200]\n",
            "loss: 1.257529  [ 2592/ 3200]\n",
            "loss: 1.054533  [ 2608/ 3200]\n",
            "loss: 1.241516  [ 2624/ 3200]\n",
            "loss: 1.090108  [ 2640/ 3200]\n",
            "loss: 1.190696  [ 2656/ 3200]\n",
            "loss: 1.189198  [ 2672/ 3200]\n",
            "loss: 1.193186  [ 2688/ 3200]\n",
            "loss: 1.186729  [ 2704/ 3200]\n",
            "loss: 1.201301  [ 2720/ 3200]\n",
            "loss: 1.089242  [ 2736/ 3200]\n",
            "loss: 1.088402  [ 2752/ 3200]\n",
            "loss: 0.930675  [ 2768/ 3200]\n",
            "loss: 1.180087  [ 2784/ 3200]\n",
            "loss: 0.985687  [ 2800/ 3200]\n",
            "loss: 1.350575  [ 2816/ 3200]\n",
            "loss: 1.095372  [ 2832/ 3200]\n",
            "loss: 1.145620  [ 2848/ 3200]\n",
            "loss: 1.095209  [ 2864/ 3200]\n",
            "loss: 1.147613  [ 2880/ 3200]\n",
            "loss: 1.064019  [ 2896/ 3200]\n",
            "loss: 1.018832  [ 2912/ 3200]\n",
            "loss: 1.018957  [ 2928/ 3200]\n",
            "loss: 1.051517  [ 2944/ 3200]\n",
            "loss: 0.892723  [ 2960/ 3200]\n",
            "loss: 1.024394  [ 2976/ 3200]\n",
            "loss: 1.130256  [ 2992/ 3200]\n",
            "loss: 1.032694  [ 3008/ 3200]\n",
            "loss: 1.104189  [ 3024/ 3200]\n",
            "loss: 1.299200  [ 3040/ 3200]\n",
            "loss: 0.945203  [ 3056/ 3200]\n",
            "loss: 1.237617  [ 3072/ 3200]\n",
            "loss: 1.144447  [ 3088/ 3200]\n",
            "loss: 0.841468  [ 3104/ 3200]\n",
            "loss: 1.187073  [ 3120/ 3200]\n",
            "loss: 1.069260  [ 3136/ 3200]\n",
            "loss: 1.025878  [ 3152/ 3200]\n",
            "loss: 1.050964  [ 3168/ 3200]\n",
            "loss: 1.118609  [ 3184/ 3200]\n",
            "current epoch: 13\n",
            "\n",
            "loss: 1.147345  [    0/ 3200]\n",
            "loss: 1.278937  [   16/ 3200]\n",
            "loss: 1.113863  [   32/ 3200]\n",
            "loss: 1.267663  [   48/ 3200]\n",
            "loss: 1.087244  [   64/ 3200]\n",
            "loss: 0.915819  [   80/ 3200]\n",
            "loss: 0.955871  [   96/ 3200]\n",
            "loss: 1.073400  [  112/ 3200]\n",
            "loss: 1.103420  [  128/ 3200]\n",
            "loss: 1.015982  [  144/ 3200]\n",
            "loss: 0.862535  [  160/ 3200]\n",
            "loss: 1.144509  [  176/ 3200]\n",
            "loss: 1.088474  [  192/ 3200]\n",
            "loss: 0.953663  [  208/ 3200]\n",
            "loss: 1.157945  [  224/ 3200]\n",
            "loss: 1.159639  [  240/ 3200]\n",
            "loss: 1.213743  [  256/ 3200]\n",
            "loss: 1.143371  [  272/ 3200]\n",
            "loss: 0.988676  [  288/ 3200]\n",
            "loss: 0.995293  [  304/ 3200]\n",
            "loss: 0.979047  [  320/ 3200]\n",
            "loss: 1.039902  [  336/ 3200]\n",
            "loss: 1.002386  [  352/ 3200]\n",
            "loss: 1.159990  [  368/ 3200]\n",
            "loss: 1.021851  [  384/ 3200]\n",
            "loss: 1.077255  [  400/ 3200]\n",
            "loss: 1.139969  [  416/ 3200]\n",
            "loss: 1.219459  [  432/ 3200]\n",
            "loss: 1.028658  [  448/ 3200]\n",
            "loss: 1.210284  [  464/ 3200]\n",
            "loss: 1.106059  [  480/ 3200]\n",
            "loss: 1.052361  [  496/ 3200]\n",
            "loss: 1.148195  [  512/ 3200]\n",
            "loss: 1.075507  [  528/ 3200]\n",
            "loss: 0.971635  [  544/ 3200]\n",
            "loss: 0.999755  [  560/ 3200]\n",
            "loss: 1.200228  [  576/ 3200]\n",
            "loss: 1.123997  [  592/ 3200]\n",
            "loss: 0.981162  [  608/ 3200]\n",
            "loss: 1.063389  [  624/ 3200]\n",
            "loss: 0.955366  [  640/ 3200]\n",
            "loss: 0.995873  [  656/ 3200]\n",
            "loss: 1.191580  [  672/ 3200]\n",
            "loss: 1.196276  [  688/ 3200]\n",
            "loss: 1.117491  [  704/ 3200]\n",
            "loss: 1.157228  [  720/ 3200]\n",
            "loss: 1.091926  [  736/ 3200]\n",
            "loss: 1.023632  [  752/ 3200]\n",
            "loss: 1.096456  [  768/ 3200]\n",
            "loss: 0.990070  [  784/ 3200]\n",
            "loss: 1.086119  [  800/ 3200]\n",
            "loss: 1.074575  [  816/ 3200]\n",
            "loss: 1.118880  [  832/ 3200]\n",
            "loss: 0.920774  [  848/ 3200]\n",
            "loss: 1.037686  [  864/ 3200]\n",
            "loss: 1.013743  [  880/ 3200]\n",
            "loss: 1.135429  [  896/ 3200]\n",
            "loss: 1.068429  [  912/ 3200]\n",
            "loss: 1.039365  [  928/ 3200]\n",
            "loss: 1.176980  [  944/ 3200]\n",
            "loss: 1.043031  [  960/ 3200]\n",
            "loss: 1.323243  [  976/ 3200]\n",
            "loss: 1.148702  [  992/ 3200]\n",
            "loss: 1.176026  [ 1008/ 3200]\n",
            "loss: 1.212493  [ 1024/ 3200]\n",
            "loss: 1.092718  [ 1040/ 3200]\n",
            "loss: 1.111070  [ 1056/ 3200]\n",
            "loss: 1.199966  [ 1072/ 3200]\n",
            "loss: 1.248152  [ 1088/ 3200]\n",
            "loss: 0.988905  [ 1104/ 3200]\n",
            "loss: 1.388190  [ 1120/ 3200]\n",
            "loss: 1.421109  [ 1136/ 3200]\n",
            "loss: 1.075795  [ 1152/ 3200]\n",
            "loss: 0.961337  [ 1168/ 3200]\n",
            "loss: 1.177882  [ 1184/ 3200]\n",
            "loss: 1.121197  [ 1200/ 3200]\n",
            "loss: 1.123336  [ 1216/ 3200]\n",
            "loss: 1.026279  [ 1232/ 3200]\n",
            "loss: 1.003583  [ 1248/ 3200]\n",
            "loss: 1.095821  [ 1264/ 3200]\n",
            "loss: 0.997483  [ 1280/ 3200]\n",
            "loss: 1.101645  [ 1296/ 3200]\n",
            "loss: 1.032452  [ 1312/ 3200]\n",
            "loss: 1.200394  [ 1328/ 3200]\n",
            "loss: 1.209772  [ 1344/ 3200]\n",
            "loss: 0.861948  [ 1360/ 3200]\n",
            "loss: 1.111333  [ 1376/ 3200]\n",
            "loss: 1.089547  [ 1392/ 3200]\n",
            "loss: 1.073684  [ 1408/ 3200]\n",
            "loss: 0.918152  [ 1424/ 3200]\n",
            "loss: 1.128105  [ 1440/ 3200]\n",
            "loss: 1.031405  [ 1456/ 3200]\n",
            "loss: 0.925074  [ 1472/ 3200]\n",
            "loss: 1.078846  [ 1488/ 3200]\n",
            "loss: 1.017983  [ 1504/ 3200]\n",
            "loss: 0.978085  [ 1520/ 3200]\n",
            "loss: 1.108443  [ 1536/ 3200]\n",
            "loss: 0.972474  [ 1552/ 3200]\n",
            "loss: 1.074288  [ 1568/ 3200]\n",
            "loss: 1.097286  [ 1584/ 3200]\n",
            "loss: 0.919027  [ 1600/ 3200]\n",
            "loss: 1.101388  [ 1616/ 3200]\n",
            "loss: 1.068437  [ 1632/ 3200]\n",
            "loss: 1.112848  [ 1648/ 3200]\n",
            "loss: 1.187428  [ 1664/ 3200]\n",
            "loss: 1.161373  [ 1680/ 3200]\n",
            "loss: 1.032851  [ 1696/ 3200]\n",
            "loss: 1.009815  [ 1712/ 3200]\n",
            "loss: 0.970381  [ 1728/ 3200]\n",
            "loss: 0.959713  [ 1744/ 3200]\n",
            "loss: 1.285118  [ 1760/ 3200]\n",
            "loss: 1.173379  [ 1776/ 3200]\n",
            "loss: 0.962153  [ 1792/ 3200]\n",
            "loss: 0.963358  [ 1808/ 3200]\n",
            "loss: 0.986760  [ 1824/ 3200]\n",
            "loss: 1.029018  [ 1840/ 3200]\n",
            "loss: 1.222043  [ 1856/ 3200]\n",
            "loss: 1.014646  [ 1872/ 3200]\n",
            "loss: 1.248057  [ 1888/ 3200]\n",
            "loss: 1.357548  [ 1904/ 3200]\n",
            "loss: 1.076502  [ 1920/ 3200]\n",
            "loss: 1.164975  [ 1936/ 3200]\n",
            "loss: 1.352664  [ 1952/ 3200]\n",
            "loss: 1.150557  [ 1968/ 3200]\n",
            "loss: 1.125885  [ 1984/ 3200]\n",
            "loss: 1.206410  [ 2000/ 3200]\n",
            "loss: 1.118319  [ 2016/ 3200]\n",
            "loss: 1.055140  [ 2032/ 3200]\n",
            "loss: 1.107685  [ 2048/ 3200]\n",
            "loss: 1.135802  [ 2064/ 3200]\n",
            "loss: 1.116007  [ 2080/ 3200]\n",
            "loss: 1.083371  [ 2096/ 3200]\n",
            "loss: 1.047624  [ 2112/ 3200]\n",
            "loss: 1.185592  [ 2128/ 3200]\n",
            "loss: 1.087569  [ 2144/ 3200]\n",
            "loss: 1.134319  [ 2160/ 3200]\n",
            "loss: 1.121230  [ 2176/ 3200]\n",
            "loss: 1.049556  [ 2192/ 3200]\n",
            "loss: 0.917317  [ 2208/ 3200]\n",
            "loss: 1.005252  [ 2224/ 3200]\n",
            "loss: 0.964218  [ 2240/ 3200]\n",
            "loss: 1.084619  [ 2256/ 3200]\n",
            "loss: 1.206768  [ 2272/ 3200]\n",
            "loss: 1.008519  [ 2288/ 3200]\n",
            "loss: 1.204919  [ 2304/ 3200]\n",
            "loss: 0.986863  [ 2320/ 3200]\n",
            "loss: 1.204643  [ 2336/ 3200]\n",
            "loss: 0.935760  [ 2352/ 3200]\n",
            "loss: 1.152781  [ 2368/ 3200]\n",
            "loss: 1.262884  [ 2384/ 3200]\n",
            "loss: 1.124722  [ 2400/ 3200]\n",
            "loss: 1.036213  [ 2416/ 3200]\n",
            "loss: 1.086509  [ 2432/ 3200]\n",
            "loss: 0.983380  [ 2448/ 3200]\n",
            "loss: 1.014971  [ 2464/ 3200]\n",
            "loss: 1.200238  [ 2480/ 3200]\n",
            "loss: 1.129642  [ 2496/ 3200]\n",
            "loss: 1.036813  [ 2512/ 3200]\n",
            "loss: 1.067712  [ 2528/ 3200]\n",
            "loss: 0.894199  [ 2544/ 3200]\n",
            "loss: 0.864870  [ 2560/ 3200]\n",
            "loss: 1.136831  [ 2576/ 3200]\n",
            "loss: 1.095507  [ 2592/ 3200]\n",
            "loss: 1.071702  [ 2608/ 3200]\n",
            "loss: 1.062675  [ 2624/ 3200]\n",
            "loss: 1.186416  [ 2640/ 3200]\n",
            "loss: 1.147901  [ 2656/ 3200]\n",
            "loss: 1.053678  [ 2672/ 3200]\n",
            "loss: 1.116773  [ 2688/ 3200]\n",
            "loss: 1.101011  [ 2704/ 3200]\n",
            "loss: 1.186783  [ 2720/ 3200]\n",
            "loss: 0.924656  [ 2736/ 3200]\n",
            "loss: 1.158496  [ 2752/ 3200]\n",
            "loss: 1.139892  [ 2768/ 3200]\n",
            "loss: 1.040230  [ 2784/ 3200]\n",
            "loss: 1.059333  [ 2800/ 3200]\n",
            "loss: 1.112767  [ 2816/ 3200]\n",
            "loss: 1.113937  [ 2832/ 3200]\n",
            "loss: 1.140639  [ 2848/ 3200]\n",
            "loss: 1.279239  [ 2864/ 3200]\n",
            "loss: 1.080122  [ 2880/ 3200]\n",
            "loss: 1.082449  [ 2896/ 3200]\n",
            "loss: 1.370208  [ 2912/ 3200]\n",
            "loss: 0.990107  [ 2928/ 3200]\n",
            "loss: 1.023214  [ 2944/ 3200]\n",
            "loss: 1.074432  [ 2960/ 3200]\n",
            "loss: 0.939591  [ 2976/ 3200]\n",
            "loss: 1.093175  [ 2992/ 3200]\n",
            "loss: 1.079398  [ 3008/ 3200]\n",
            "loss: 0.930300  [ 3024/ 3200]\n",
            "loss: 0.955335  [ 3040/ 3200]\n",
            "loss: 1.166293  [ 3056/ 3200]\n",
            "loss: 0.937049  [ 3072/ 3200]\n",
            "loss: 1.226698  [ 3088/ 3200]\n",
            "loss: 1.289527  [ 3104/ 3200]\n",
            "loss: 1.084358  [ 3120/ 3200]\n",
            "loss: 1.051171  [ 3136/ 3200]\n",
            "loss: 1.275127  [ 3152/ 3200]\n",
            "loss: 1.080968  [ 3168/ 3200]\n",
            "loss: 0.913437  [ 3184/ 3200]\n",
            "current epoch: 14\n",
            "\n",
            "loss: 1.101527  [    0/ 3200]\n",
            "loss: 1.342061  [   16/ 3200]\n",
            "loss: 1.262527  [   32/ 3200]\n",
            "loss: 0.971903  [   48/ 3200]\n",
            "loss: 1.146565  [   64/ 3200]\n",
            "loss: 1.097088  [   80/ 3200]\n",
            "loss: 0.986010  [   96/ 3200]\n",
            "loss: 0.975614  [  112/ 3200]\n",
            "loss: 0.891395  [  128/ 3200]\n",
            "loss: 1.255153  [  144/ 3200]\n",
            "loss: 0.768159  [  160/ 3200]\n",
            "loss: 1.117358  [  176/ 3200]\n",
            "loss: 1.135083  [  192/ 3200]\n",
            "loss: 1.087233  [  208/ 3200]\n",
            "loss: 1.057984  [  224/ 3200]\n",
            "loss: 1.163143  [  240/ 3200]\n",
            "loss: 1.227502  [  256/ 3200]\n",
            "loss: 1.137663  [  272/ 3200]\n",
            "loss: 1.038637  [  288/ 3200]\n",
            "loss: 1.222425  [  304/ 3200]\n",
            "loss: 0.970151  [  320/ 3200]\n",
            "loss: 1.033608  [  336/ 3200]\n",
            "loss: 1.042816  [  352/ 3200]\n",
            "loss: 0.998025  [  368/ 3200]\n",
            "loss: 1.077547  [  384/ 3200]\n",
            "loss: 1.201845  [  400/ 3200]\n",
            "loss: 0.970334  [  416/ 3200]\n",
            "loss: 1.068985  [  432/ 3200]\n",
            "loss: 1.102305  [  448/ 3200]\n",
            "loss: 0.940041  [  464/ 3200]\n",
            "loss: 1.033688  [  480/ 3200]\n",
            "loss: 1.071684  [  496/ 3200]\n",
            "loss: 1.088798  [  512/ 3200]\n",
            "loss: 0.954794  [  528/ 3200]\n",
            "loss: 1.190833  [  544/ 3200]\n",
            "loss: 0.911628  [  560/ 3200]\n",
            "loss: 1.150736  [  576/ 3200]\n",
            "loss: 1.304817  [  592/ 3200]\n",
            "loss: 1.029047  [  608/ 3200]\n",
            "loss: 1.121362  [  624/ 3200]\n",
            "loss: 1.016384  [  640/ 3200]\n",
            "loss: 1.146510  [  656/ 3200]\n",
            "loss: 1.041179  [  672/ 3200]\n",
            "loss: 1.089477  [  688/ 3200]\n",
            "loss: 0.954500  [  704/ 3200]\n",
            "loss: 1.085064  [  720/ 3200]\n",
            "loss: 1.196234  [  736/ 3200]\n",
            "loss: 1.049965  [  752/ 3200]\n",
            "loss: 1.222605  [  768/ 3200]\n",
            "loss: 1.219717  [  784/ 3200]\n",
            "loss: 1.082132  [  800/ 3200]\n",
            "loss: 1.121588  [  816/ 3200]\n",
            "loss: 1.225283  [  832/ 3200]\n",
            "loss: 1.210436  [  848/ 3200]\n",
            "loss: 1.035326  [  864/ 3200]\n",
            "loss: 1.160419  [  880/ 3200]\n",
            "loss: 1.170440  [  896/ 3200]\n",
            "loss: 1.259653  [  912/ 3200]\n",
            "loss: 1.125001  [  928/ 3200]\n",
            "loss: 0.900009  [  944/ 3200]\n",
            "loss: 1.160782  [  960/ 3200]\n",
            "loss: 1.096120  [  976/ 3200]\n",
            "loss: 0.984479  [  992/ 3200]\n",
            "loss: 1.068459  [ 1008/ 3200]\n",
            "loss: 1.070836  [ 1024/ 3200]\n",
            "loss: 0.935174  [ 1040/ 3200]\n",
            "loss: 1.033210  [ 1056/ 3200]\n",
            "loss: 0.997630  [ 1072/ 3200]\n",
            "loss: 1.061631  [ 1088/ 3200]\n",
            "loss: 1.086513  [ 1104/ 3200]\n",
            "loss: 1.117457  [ 1120/ 3200]\n",
            "loss: 1.083301  [ 1136/ 3200]\n",
            "loss: 1.326551  [ 1152/ 3200]\n",
            "loss: 1.053434  [ 1168/ 3200]\n",
            "loss: 1.060595  [ 1184/ 3200]\n",
            "loss: 1.052567  [ 1200/ 3200]\n",
            "loss: 1.134701  [ 1216/ 3200]\n",
            "loss: 1.213578  [ 1232/ 3200]\n",
            "loss: 1.039910  [ 1248/ 3200]\n",
            "loss: 0.887698  [ 1264/ 3200]\n",
            "loss: 1.109202  [ 1280/ 3200]\n",
            "loss: 1.147056  [ 1296/ 3200]\n",
            "loss: 0.996030  [ 1312/ 3200]\n",
            "loss: 1.173559  [ 1328/ 3200]\n",
            "loss: 1.141629  [ 1344/ 3200]\n",
            "loss: 1.126369  [ 1360/ 3200]\n",
            "loss: 1.114803  [ 1376/ 3200]\n",
            "loss: 0.988181  [ 1392/ 3200]\n",
            "loss: 0.962689  [ 1408/ 3200]\n",
            "loss: 1.048710  [ 1424/ 3200]\n",
            "loss: 0.821802  [ 1440/ 3200]\n",
            "loss: 1.238082  [ 1456/ 3200]\n",
            "loss: 1.126458  [ 1472/ 3200]\n",
            "loss: 1.038687  [ 1488/ 3200]\n",
            "loss: 0.917509  [ 1504/ 3200]\n",
            "loss: 1.130905  [ 1520/ 3200]\n",
            "loss: 1.058576  [ 1536/ 3200]\n",
            "loss: 0.959995  [ 1552/ 3200]\n",
            "loss: 1.006612  [ 1568/ 3200]\n",
            "loss: 1.006301  [ 1584/ 3200]\n",
            "loss: 0.882232  [ 1600/ 3200]\n",
            "loss: 1.021874  [ 1616/ 3200]\n",
            "loss: 1.048418  [ 1632/ 3200]\n",
            "loss: 1.031243  [ 1648/ 3200]\n",
            "loss: 1.154657  [ 1664/ 3200]\n",
            "loss: 1.051284  [ 1680/ 3200]\n",
            "loss: 1.215364  [ 1696/ 3200]\n",
            "loss: 1.216007  [ 1712/ 3200]\n",
            "loss: 1.066850  [ 1728/ 3200]\n",
            "loss: 1.193163  [ 1744/ 3200]\n",
            "loss: 0.815523  [ 1760/ 3200]\n",
            "loss: 0.837924  [ 1776/ 3200]\n",
            "loss: 0.991073  [ 1792/ 3200]\n",
            "loss: 1.066664  [ 1808/ 3200]\n",
            "loss: 0.903082  [ 1824/ 3200]\n",
            "loss: 1.141273  [ 1840/ 3200]\n",
            "loss: 1.113851  [ 1856/ 3200]\n",
            "loss: 1.299746  [ 1872/ 3200]\n",
            "loss: 0.991041  [ 1888/ 3200]\n",
            "loss: 0.919176  [ 1904/ 3200]\n",
            "loss: 0.959554  [ 1920/ 3200]\n",
            "loss: 0.940606  [ 1936/ 3200]\n",
            "loss: 0.912535  [ 1952/ 3200]\n",
            "loss: 1.229784  [ 1968/ 3200]\n",
            "loss: 1.021129  [ 1984/ 3200]\n",
            "loss: 1.154907  [ 2000/ 3200]\n",
            "loss: 1.002864  [ 2016/ 3200]\n",
            "loss: 0.973647  [ 2032/ 3200]\n",
            "loss: 1.116340  [ 2048/ 3200]\n",
            "loss: 1.178246  [ 2064/ 3200]\n",
            "loss: 0.984215  [ 2080/ 3200]\n",
            "loss: 0.762651  [ 2096/ 3200]\n",
            "loss: 1.147788  [ 2112/ 3200]\n",
            "loss: 1.050525  [ 2128/ 3200]\n",
            "loss: 1.121735  [ 2144/ 3200]\n",
            "loss: 1.037816  [ 2160/ 3200]\n",
            "loss: 1.055238  [ 2176/ 3200]\n",
            "loss: 0.914134  [ 2192/ 3200]\n",
            "loss: 1.020411  [ 2208/ 3200]\n",
            "loss: 0.961967  [ 2224/ 3200]\n",
            "loss: 0.981445  [ 2240/ 3200]\n",
            "loss: 1.117408  [ 2256/ 3200]\n",
            "loss: 0.961414  [ 2272/ 3200]\n",
            "loss: 1.082474  [ 2288/ 3200]\n",
            "loss: 0.889722  [ 2304/ 3200]\n",
            "loss: 1.091282  [ 2320/ 3200]\n",
            "loss: 0.890602  [ 2336/ 3200]\n",
            "loss: 1.137542  [ 2352/ 3200]\n",
            "loss: 1.050051  [ 2368/ 3200]\n",
            "loss: 1.003575  [ 2384/ 3200]\n",
            "loss: 1.062554  [ 2400/ 3200]\n",
            "loss: 0.931671  [ 2416/ 3200]\n",
            "loss: 1.082407  [ 2432/ 3200]\n",
            "loss: 1.120288  [ 2448/ 3200]\n",
            "loss: 1.077605  [ 2464/ 3200]\n",
            "loss: 1.113022  [ 2480/ 3200]\n",
            "loss: 0.881621  [ 2496/ 3200]\n",
            "loss: 1.335329  [ 2512/ 3200]\n",
            "loss: 1.139259  [ 2528/ 3200]\n",
            "loss: 1.148551  [ 2544/ 3200]\n",
            "loss: 1.128621  [ 2560/ 3200]\n",
            "loss: 1.136096  [ 2576/ 3200]\n",
            "loss: 1.024608  [ 2592/ 3200]\n",
            "loss: 1.016847  [ 2608/ 3200]\n",
            "loss: 1.204605  [ 2624/ 3200]\n",
            "loss: 1.181833  [ 2640/ 3200]\n",
            "loss: 0.815208  [ 2656/ 3200]\n",
            "loss: 1.207195  [ 2672/ 3200]\n",
            "loss: 1.065067  [ 2688/ 3200]\n",
            "loss: 1.071921  [ 2704/ 3200]\n",
            "loss: 1.062168  [ 2720/ 3200]\n",
            "loss: 1.125164  [ 2736/ 3200]\n",
            "loss: 1.124687  [ 2752/ 3200]\n",
            "loss: 1.063234  [ 2768/ 3200]\n",
            "loss: 1.351047  [ 2784/ 3200]\n",
            "loss: 0.934033  [ 2800/ 3200]\n",
            "loss: 1.218669  [ 2816/ 3200]\n",
            "loss: 1.096051  [ 2832/ 3200]\n",
            "loss: 1.125534  [ 2848/ 3200]\n",
            "loss: 1.049209  [ 2864/ 3200]\n",
            "loss: 1.160993  [ 2880/ 3200]\n",
            "loss: 1.203759  [ 2896/ 3200]\n",
            "loss: 1.052230  [ 2912/ 3200]\n",
            "loss: 1.057714  [ 2928/ 3200]\n",
            "loss: 1.029886  [ 2944/ 3200]\n",
            "loss: 0.860271  [ 2960/ 3200]\n",
            "loss: 1.181978  [ 2976/ 3200]\n",
            "loss: 1.147268  [ 2992/ 3200]\n",
            "loss: 0.941691  [ 3008/ 3200]\n",
            "loss: 1.052529  [ 3024/ 3200]\n",
            "loss: 1.078607  [ 3040/ 3200]\n",
            "loss: 0.996768  [ 3056/ 3200]\n",
            "loss: 1.059641  [ 3072/ 3200]\n",
            "loss: 1.355476  [ 3088/ 3200]\n",
            "loss: 1.037679  [ 3104/ 3200]\n",
            "loss: 1.172081  [ 3120/ 3200]\n",
            "loss: 1.151755  [ 3136/ 3200]\n",
            "loss: 1.133355  [ 3152/ 3200]\n",
            "loss: 1.055155  [ 3168/ 3200]\n",
            "loss: 1.204854  [ 3184/ 3200]\n",
            "current epoch: 15\n",
            "\n",
            "loss: 0.891845  [    0/ 3200]\n",
            "loss: 1.124553  [   16/ 3200]\n",
            "loss: 1.128312  [   32/ 3200]\n",
            "loss: 1.017966  [   48/ 3200]\n",
            "loss: 0.979368  [   64/ 3200]\n",
            "loss: 0.945536  [   80/ 3200]\n",
            "loss: 0.950725  [   96/ 3200]\n",
            "loss: 1.086292  [  112/ 3200]\n",
            "loss: 1.056705  [  128/ 3200]\n",
            "loss: 1.336926  [  144/ 3200]\n",
            "loss: 1.026497  [  160/ 3200]\n",
            "loss: 1.069190  [  176/ 3200]\n",
            "loss: 1.200119  [  192/ 3200]\n",
            "loss: 1.016155  [  208/ 3200]\n",
            "loss: 0.977384  [  224/ 3200]\n",
            "loss: 0.961130  [  240/ 3200]\n",
            "loss: 0.830381  [  256/ 3200]\n",
            "loss: 1.119360  [  272/ 3200]\n",
            "loss: 0.972590  [  288/ 3200]\n",
            "loss: 0.722251  [  304/ 3200]\n",
            "loss: 1.281878  [  320/ 3200]\n",
            "loss: 1.201567  [  336/ 3200]\n",
            "loss: 1.094232  [  352/ 3200]\n",
            "loss: 1.163798  [  368/ 3200]\n",
            "loss: 0.921823  [  384/ 3200]\n",
            "loss: 1.179163  [  400/ 3200]\n",
            "loss: 1.065138  [  416/ 3200]\n",
            "loss: 0.933815  [  432/ 3200]\n",
            "loss: 1.125823  [  448/ 3200]\n",
            "loss: 1.027929  [  464/ 3200]\n",
            "loss: 0.967171  [  480/ 3200]\n",
            "loss: 1.129124  [  496/ 3200]\n",
            "loss: 1.176518  [  512/ 3200]\n",
            "loss: 0.931213  [  528/ 3200]\n",
            "loss: 1.379837  [  544/ 3200]\n",
            "loss: 1.116823  [  560/ 3200]\n",
            "loss: 1.053079  [  576/ 3200]\n",
            "loss: 1.059505  [  592/ 3200]\n",
            "loss: 1.327822  [  608/ 3200]\n",
            "loss: 1.076794  [  624/ 3200]\n",
            "loss: 0.970318  [  640/ 3200]\n",
            "loss: 1.043130  [  656/ 3200]\n",
            "loss: 0.980378  [  672/ 3200]\n",
            "loss: 1.097212  [  688/ 3200]\n",
            "loss: 0.955351  [  704/ 3200]\n",
            "loss: 1.246663  [  720/ 3200]\n",
            "loss: 1.293150  [  736/ 3200]\n",
            "loss: 1.224858  [  752/ 3200]\n",
            "loss: 1.057499  [  768/ 3200]\n",
            "loss: 1.088452  [  784/ 3200]\n",
            "loss: 1.296322  [  800/ 3200]\n",
            "loss: 1.170987  [  816/ 3200]\n",
            "loss: 0.907191  [  832/ 3200]\n",
            "loss: 0.929324  [  848/ 3200]\n",
            "loss: 1.005296  [  864/ 3200]\n",
            "loss: 1.114336  [  880/ 3200]\n",
            "loss: 1.082059  [  896/ 3200]\n",
            "loss: 1.159579  [  912/ 3200]\n",
            "loss: 1.050803  [  928/ 3200]\n",
            "loss: 1.025960  [  944/ 3200]\n",
            "loss: 1.179505  [  960/ 3200]\n",
            "loss: 0.877738  [  976/ 3200]\n",
            "loss: 1.068175  [  992/ 3200]\n",
            "loss: 0.886650  [ 1008/ 3200]\n",
            "loss: 1.113828  [ 1024/ 3200]\n",
            "loss: 0.966201  [ 1040/ 3200]\n",
            "loss: 0.992854  [ 1056/ 3200]\n",
            "loss: 0.889724  [ 1072/ 3200]\n",
            "loss: 1.074992  [ 1088/ 3200]\n",
            "loss: 0.981232  [ 1104/ 3200]\n",
            "loss: 0.971567  [ 1120/ 3200]\n",
            "loss: 1.004123  [ 1136/ 3200]\n",
            "loss: 1.350634  [ 1152/ 3200]\n",
            "loss: 1.010879  [ 1168/ 3200]\n",
            "loss: 1.272577  [ 1184/ 3200]\n",
            "loss: 0.984234  [ 1200/ 3200]\n",
            "loss: 1.062160  [ 1216/ 3200]\n",
            "loss: 0.975240  [ 1232/ 3200]\n",
            "loss: 0.981767  [ 1248/ 3200]\n",
            "loss: 1.181061  [ 1264/ 3200]\n",
            "loss: 1.046345  [ 1280/ 3200]\n",
            "loss: 1.315795  [ 1296/ 3200]\n",
            "loss: 0.909238  [ 1312/ 3200]\n",
            "loss: 1.153342  [ 1328/ 3200]\n",
            "loss: 0.999357  [ 1344/ 3200]\n",
            "loss: 1.386106  [ 1360/ 3200]\n",
            "loss: 0.869179  [ 1376/ 3200]\n",
            "loss: 1.003262  [ 1392/ 3200]\n",
            "loss: 0.952244  [ 1408/ 3200]\n",
            "loss: 0.983155  [ 1424/ 3200]\n",
            "loss: 1.107848  [ 1440/ 3200]\n",
            "loss: 1.081704  [ 1456/ 3200]\n",
            "loss: 1.146304  [ 1472/ 3200]\n",
            "loss: 1.176271  [ 1488/ 3200]\n",
            "loss: 0.887636  [ 1504/ 3200]\n",
            "loss: 1.102835  [ 1520/ 3200]\n",
            "loss: 1.281232  [ 1536/ 3200]\n",
            "loss: 1.193436  [ 1552/ 3200]\n",
            "loss: 0.983592  [ 1568/ 3200]\n",
            "loss: 1.055988  [ 1584/ 3200]\n",
            "loss: 1.055927  [ 1600/ 3200]\n",
            "loss: 1.000091  [ 1616/ 3200]\n",
            "loss: 1.028624  [ 1632/ 3200]\n",
            "loss: 1.136037  [ 1648/ 3200]\n",
            "loss: 1.213906  [ 1664/ 3200]\n",
            "loss: 0.963678  [ 1680/ 3200]\n",
            "loss: 1.021844  [ 1696/ 3200]\n",
            "loss: 1.114981  [ 1712/ 3200]\n",
            "loss: 1.327890  [ 1728/ 3200]\n",
            "loss: 1.024344  [ 1744/ 3200]\n",
            "loss: 1.178091  [ 1760/ 3200]\n",
            "loss: 1.059237  [ 1776/ 3200]\n",
            "loss: 1.081102  [ 1792/ 3200]\n",
            "loss: 1.006575  [ 1808/ 3200]\n",
            "loss: 1.191034  [ 1824/ 3200]\n",
            "loss: 0.854456  [ 1840/ 3200]\n",
            "loss: 1.041221  [ 1856/ 3200]\n",
            "loss: 1.026798  [ 1872/ 3200]\n",
            "loss: 1.078767  [ 1888/ 3200]\n",
            "loss: 0.957219  [ 1904/ 3200]\n",
            "loss: 1.033451  [ 1920/ 3200]\n",
            "loss: 1.047376  [ 1936/ 3200]\n",
            "loss: 1.058987  [ 1952/ 3200]\n",
            "loss: 0.925569  [ 1968/ 3200]\n",
            "loss: 0.847281  [ 1984/ 3200]\n",
            "loss: 0.962286  [ 2000/ 3200]\n",
            "loss: 0.944978  [ 2016/ 3200]\n",
            "loss: 0.894105  [ 2032/ 3200]\n",
            "loss: 1.157823  [ 2048/ 3200]\n",
            "loss: 1.120664  [ 2064/ 3200]\n",
            "loss: 0.969450  [ 2080/ 3200]\n",
            "loss: 1.136975  [ 2096/ 3200]\n",
            "loss: 1.031299  [ 2112/ 3200]\n",
            "loss: 1.082736  [ 2128/ 3200]\n",
            "loss: 1.085354  [ 2144/ 3200]\n",
            "loss: 1.343076  [ 2160/ 3200]\n",
            "loss: 0.912102  [ 2176/ 3200]\n",
            "loss: 1.190777  [ 2192/ 3200]\n",
            "loss: 0.965438  [ 2208/ 3200]\n",
            "loss: 1.050029  [ 2224/ 3200]\n",
            "loss: 1.100352  [ 2240/ 3200]\n",
            "loss: 0.929421  [ 2256/ 3200]\n",
            "loss: 0.917060  [ 2272/ 3200]\n",
            "loss: 0.735769  [ 2288/ 3200]\n",
            "loss: 1.101358  [ 2304/ 3200]\n",
            "loss: 1.062794  [ 2320/ 3200]\n",
            "loss: 1.095647  [ 2336/ 3200]\n",
            "loss: 0.933720  [ 2352/ 3200]\n",
            "loss: 1.083806  [ 2368/ 3200]\n",
            "loss: 1.249023  [ 2384/ 3200]\n",
            "loss: 1.056342  [ 2400/ 3200]\n",
            "loss: 0.884276  [ 2416/ 3200]\n",
            "loss: 1.021487  [ 2432/ 3200]\n",
            "loss: 1.152886  [ 2448/ 3200]\n",
            "loss: 1.176621  [ 2464/ 3200]\n",
            "loss: 0.951011  [ 2480/ 3200]\n",
            "loss: 0.915697  [ 2496/ 3200]\n",
            "loss: 1.143105  [ 2512/ 3200]\n",
            "loss: 1.032527  [ 2528/ 3200]\n",
            "loss: 1.167496  [ 2544/ 3200]\n",
            "loss: 1.202089  [ 2560/ 3200]\n",
            "loss: 1.062543  [ 2576/ 3200]\n",
            "loss: 1.065437  [ 2592/ 3200]\n",
            "loss: 1.080122  [ 2608/ 3200]\n",
            "loss: 1.030792  [ 2624/ 3200]\n",
            "loss: 0.922159  [ 2640/ 3200]\n",
            "loss: 0.912866  [ 2656/ 3200]\n",
            "loss: 1.133734  [ 2672/ 3200]\n",
            "loss: 0.844592  [ 2688/ 3200]\n",
            "loss: 0.965092  [ 2704/ 3200]\n",
            "loss: 0.978131  [ 2720/ 3200]\n",
            "loss: 1.306599  [ 2736/ 3200]\n",
            "loss: 0.864070  [ 2752/ 3200]\n",
            "loss: 1.041105  [ 2768/ 3200]\n",
            "loss: 0.847876  [ 2784/ 3200]\n",
            "loss: 1.036711  [ 2800/ 3200]\n",
            "loss: 1.115835  [ 2816/ 3200]\n",
            "loss: 1.040329  [ 2832/ 3200]\n",
            "loss: 0.979277  [ 2848/ 3200]\n",
            "loss: 1.019275  [ 2864/ 3200]\n",
            "loss: 0.889968  [ 2880/ 3200]\n",
            "loss: 0.731760  [ 2896/ 3200]\n",
            "loss: 1.067224  [ 2912/ 3200]\n",
            "loss: 0.953741  [ 2928/ 3200]\n",
            "loss: 1.090254  [ 2944/ 3200]\n",
            "loss: 1.034463  [ 2960/ 3200]\n",
            "loss: 1.251055  [ 2976/ 3200]\n",
            "loss: 1.225126  [ 2992/ 3200]\n",
            "loss: 0.935306  [ 3008/ 3200]\n",
            "loss: 1.269317  [ 3024/ 3200]\n",
            "loss: 1.072910  [ 3040/ 3200]\n",
            "loss: 1.044407  [ 3056/ 3200]\n",
            "loss: 1.061937  [ 3072/ 3200]\n",
            "loss: 1.449836  [ 3088/ 3200]\n",
            "loss: 1.054871  [ 3104/ 3200]\n",
            "loss: 1.006450  [ 3120/ 3200]\n",
            "loss: 1.035032  [ 3136/ 3200]\n",
            "loss: 0.985646  [ 3152/ 3200]\n",
            "loss: 1.080453  [ 3168/ 3200]\n",
            "loss: 1.034833  [ 3184/ 3200]\n",
            "current epoch: 16\n",
            "\n",
            "loss: 1.019301  [    0/ 3200]\n",
            "loss: 1.266544  [   16/ 3200]\n",
            "loss: 1.177319  [   32/ 3200]\n",
            "loss: 0.851701  [   48/ 3200]\n",
            "loss: 0.900593  [   64/ 3200]\n",
            "loss: 1.289797  [   80/ 3200]\n",
            "loss: 1.092285  [   96/ 3200]\n",
            "loss: 1.188099  [  112/ 3200]\n",
            "loss: 1.098076  [  128/ 3200]\n",
            "loss: 1.035692  [  144/ 3200]\n",
            "loss: 1.097483  [  160/ 3200]\n",
            "loss: 1.186420  [  176/ 3200]\n",
            "loss: 1.130049  [  192/ 3200]\n",
            "loss: 1.162030  [  208/ 3200]\n",
            "loss: 0.889818  [  224/ 3200]\n",
            "loss: 0.755476  [  240/ 3200]\n",
            "loss: 1.060392  [  256/ 3200]\n",
            "loss: 1.041935  [  272/ 3200]\n",
            "loss: 0.785405  [  288/ 3200]\n",
            "loss: 1.220567  [  304/ 3200]\n",
            "loss: 0.733927  [  320/ 3200]\n",
            "loss: 1.164519  [  336/ 3200]\n",
            "loss: 1.136627  [  352/ 3200]\n",
            "loss: 1.046926  [  368/ 3200]\n",
            "loss: 1.246561  [  384/ 3200]\n",
            "loss: 1.116513  [  400/ 3200]\n",
            "loss: 1.025204  [  416/ 3200]\n",
            "loss: 1.060562  [  432/ 3200]\n",
            "loss: 0.779033  [  448/ 3200]\n",
            "loss: 1.410567  [  464/ 3200]\n",
            "loss: 0.824974  [  480/ 3200]\n",
            "loss: 0.931395  [  496/ 3200]\n",
            "loss: 1.105399  [  512/ 3200]\n",
            "loss: 1.064445  [  528/ 3200]\n",
            "loss: 1.175503  [  544/ 3200]\n",
            "loss: 1.159071  [  560/ 3200]\n",
            "loss: 1.029868  [  576/ 3200]\n",
            "loss: 0.987404  [  592/ 3200]\n",
            "loss: 1.016696  [  608/ 3200]\n",
            "loss: 0.992501  [  624/ 3200]\n",
            "loss: 0.998511  [  640/ 3200]\n",
            "loss: 1.089120  [  656/ 3200]\n",
            "loss: 0.960922  [  672/ 3200]\n",
            "loss: 1.031676  [  688/ 3200]\n",
            "loss: 1.001572  [  704/ 3200]\n",
            "loss: 0.949691  [  720/ 3200]\n",
            "loss: 1.311200  [  736/ 3200]\n",
            "loss: 1.015299  [  752/ 3200]\n",
            "loss: 1.343284  [  768/ 3200]\n",
            "loss: 1.101971  [  784/ 3200]\n",
            "loss: 0.731617  [  800/ 3200]\n",
            "loss: 0.929871  [  816/ 3200]\n",
            "loss: 0.992428  [  832/ 3200]\n",
            "loss: 0.884971  [  848/ 3200]\n",
            "loss: 1.063281  [  864/ 3200]\n",
            "loss: 1.156249  [  880/ 3200]\n",
            "loss: 1.238153  [  896/ 3200]\n",
            "loss: 0.958914  [  912/ 3200]\n",
            "loss: 1.255333  [  928/ 3200]\n",
            "loss: 0.877418  [  944/ 3200]\n",
            "loss: 1.050879  [  960/ 3200]\n",
            "loss: 1.026541  [  976/ 3200]\n",
            "loss: 1.216480  [  992/ 3200]\n",
            "loss: 1.027676  [ 1008/ 3200]\n",
            "loss: 0.908671  [ 1024/ 3200]\n",
            "loss: 1.091401  [ 1040/ 3200]\n",
            "loss: 1.059463  [ 1056/ 3200]\n",
            "loss: 0.943779  [ 1072/ 3200]\n",
            "loss: 0.985035  [ 1088/ 3200]\n",
            "loss: 1.241786  [ 1104/ 3200]\n",
            "loss: 0.984752  [ 1120/ 3200]\n",
            "loss: 1.057462  [ 1136/ 3200]\n",
            "loss: 1.425424  [ 1152/ 3200]\n",
            "loss: 1.187529  [ 1168/ 3200]\n",
            "loss: 0.920166  [ 1184/ 3200]\n",
            "loss: 1.185652  [ 1200/ 3200]\n",
            "loss: 1.115792  [ 1216/ 3200]\n",
            "loss: 0.862411  [ 1232/ 3200]\n",
            "loss: 1.165006  [ 1248/ 3200]\n",
            "loss: 1.052276  [ 1264/ 3200]\n",
            "loss: 1.022244  [ 1280/ 3200]\n",
            "loss: 1.037932  [ 1296/ 3200]\n",
            "loss: 1.182473  [ 1312/ 3200]\n",
            "loss: 1.066721  [ 1328/ 3200]\n",
            "loss: 0.968031  [ 1344/ 3200]\n",
            "loss: 1.111095  [ 1360/ 3200]\n",
            "loss: 0.908677  [ 1376/ 3200]\n",
            "loss: 1.058239  [ 1392/ 3200]\n",
            "loss: 1.125747  [ 1408/ 3200]\n",
            "loss: 0.970264  [ 1424/ 3200]\n",
            "loss: 1.246265  [ 1440/ 3200]\n",
            "loss: 0.912423  [ 1456/ 3200]\n",
            "loss: 1.008082  [ 1472/ 3200]\n",
            "loss: 1.028692  [ 1488/ 3200]\n",
            "loss: 1.168994  [ 1504/ 3200]\n",
            "loss: 0.977458  [ 1520/ 3200]\n",
            "loss: 1.205338  [ 1536/ 3200]\n",
            "loss: 1.049830  [ 1552/ 3200]\n",
            "loss: 0.888306  [ 1568/ 3200]\n",
            "loss: 1.074784  [ 1584/ 3200]\n",
            "loss: 0.963436  [ 1600/ 3200]\n",
            "loss: 1.237039  [ 1616/ 3200]\n",
            "loss: 1.049074  [ 1632/ 3200]\n",
            "loss: 0.842709  [ 1648/ 3200]\n",
            "loss: 1.094434  [ 1664/ 3200]\n",
            "loss: 1.038737  [ 1680/ 3200]\n",
            "loss: 1.113045  [ 1696/ 3200]\n",
            "loss: 0.979150  [ 1712/ 3200]\n",
            "loss: 1.100485  [ 1728/ 3200]\n",
            "loss: 1.253464  [ 1744/ 3200]\n",
            "loss: 1.116442  [ 1760/ 3200]\n",
            "loss: 1.091693  [ 1776/ 3200]\n",
            "loss: 1.079829  [ 1792/ 3200]\n",
            "loss: 1.017969  [ 1808/ 3200]\n",
            "loss: 0.867176  [ 1824/ 3200]\n",
            "loss: 0.916995  [ 1840/ 3200]\n",
            "loss: 1.141434  [ 1856/ 3200]\n",
            "loss: 1.114620  [ 1872/ 3200]\n",
            "loss: 1.073799  [ 1888/ 3200]\n",
            "loss: 1.035771  [ 1904/ 3200]\n",
            "loss: 1.050055  [ 1920/ 3200]\n",
            "loss: 1.000092  [ 1936/ 3200]\n",
            "loss: 0.926228  [ 1952/ 3200]\n",
            "loss: 1.040703  [ 1968/ 3200]\n",
            "loss: 0.979684  [ 1984/ 3200]\n",
            "loss: 0.824112  [ 2000/ 3200]\n",
            "loss: 1.053035  [ 2016/ 3200]\n",
            "loss: 1.275104  [ 2032/ 3200]\n",
            "loss: 1.107267  [ 2048/ 3200]\n",
            "loss: 1.041881  [ 2064/ 3200]\n",
            "loss: 1.042793  [ 2080/ 3200]\n",
            "loss: 0.979876  [ 2096/ 3200]\n",
            "loss: 1.042542  [ 2112/ 3200]\n",
            "loss: 1.066050  [ 2128/ 3200]\n",
            "loss: 0.979942  [ 2144/ 3200]\n",
            "loss: 0.988573  [ 2160/ 3200]\n",
            "loss: 1.070828  [ 2176/ 3200]\n",
            "loss: 0.957444  [ 2192/ 3200]\n",
            "loss: 0.848100  [ 2208/ 3200]\n",
            "loss: 1.111701  [ 2224/ 3200]\n",
            "loss: 1.136316  [ 2240/ 3200]\n",
            "loss: 0.999429  [ 2256/ 3200]\n",
            "loss: 0.856350  [ 2272/ 3200]\n",
            "loss: 1.059828  [ 2288/ 3200]\n",
            "loss: 0.884316  [ 2304/ 3200]\n",
            "loss: 1.156987  [ 2320/ 3200]\n",
            "loss: 1.391661  [ 2336/ 3200]\n",
            "loss: 1.059438  [ 2352/ 3200]\n",
            "loss: 0.995376  [ 2368/ 3200]\n",
            "loss: 1.243582  [ 2384/ 3200]\n",
            "loss: 0.858502  [ 2400/ 3200]\n",
            "loss: 1.300668  [ 2416/ 3200]\n",
            "loss: 1.244976  [ 2432/ 3200]\n",
            "loss: 1.146878  [ 2448/ 3200]\n",
            "loss: 0.876915  [ 2464/ 3200]\n",
            "loss: 0.906394  [ 2480/ 3200]\n",
            "loss: 1.045097  [ 2496/ 3200]\n",
            "loss: 1.194228  [ 2512/ 3200]\n",
            "loss: 1.291054  [ 2528/ 3200]\n",
            "loss: 1.048387  [ 2544/ 3200]\n",
            "loss: 0.877946  [ 2560/ 3200]\n",
            "loss: 1.122749  [ 2576/ 3200]\n",
            "loss: 1.002846  [ 2592/ 3200]\n",
            "loss: 0.795581  [ 2608/ 3200]\n",
            "loss: 1.093653  [ 2624/ 3200]\n",
            "loss: 1.020824  [ 2640/ 3200]\n",
            "loss: 0.941886  [ 2656/ 3200]\n",
            "loss: 1.135012  [ 2672/ 3200]\n",
            "loss: 0.894468  [ 2688/ 3200]\n",
            "loss: 0.915166  [ 2704/ 3200]\n",
            "loss: 1.133485  [ 2720/ 3200]\n",
            "loss: 0.947168  [ 2736/ 3200]\n",
            "loss: 1.031544  [ 2752/ 3200]\n",
            "loss: 0.992785  [ 2768/ 3200]\n",
            "loss: 1.086602  [ 2784/ 3200]\n",
            "loss: 0.977585  [ 2800/ 3200]\n",
            "loss: 1.008737  [ 2816/ 3200]\n",
            "loss: 0.825003  [ 2832/ 3200]\n",
            "loss: 0.808188  [ 2848/ 3200]\n",
            "loss: 1.235785  [ 2864/ 3200]\n",
            "loss: 0.984002  [ 2880/ 3200]\n",
            "loss: 1.071080  [ 2896/ 3200]\n",
            "loss: 1.017791  [ 2912/ 3200]\n",
            "loss: 1.005166  [ 2928/ 3200]\n",
            "loss: 0.916276  [ 2944/ 3200]\n",
            "loss: 0.899004  [ 2960/ 3200]\n",
            "loss: 0.957385  [ 2976/ 3200]\n",
            "loss: 1.106007  [ 2992/ 3200]\n",
            "loss: 0.890512  [ 3008/ 3200]\n",
            "loss: 0.955794  [ 3024/ 3200]\n",
            "loss: 1.071416  [ 3040/ 3200]\n",
            "loss: 1.015553  [ 3056/ 3200]\n",
            "loss: 0.943736  [ 3072/ 3200]\n",
            "loss: 1.077990  [ 3088/ 3200]\n",
            "loss: 1.124738  [ 3104/ 3200]\n",
            "loss: 1.184656  [ 3120/ 3200]\n",
            "loss: 1.137140  [ 3136/ 3200]\n",
            "loss: 0.988578  [ 3152/ 3200]\n",
            "loss: 1.199689  [ 3168/ 3200]\n",
            "loss: 1.135349  [ 3184/ 3200]\n",
            "current epoch: 17\n",
            "\n",
            "loss: 0.724256  [    0/ 3200]\n",
            "loss: 1.141078  [   16/ 3200]\n",
            "loss: 1.150077  [   32/ 3200]\n",
            "loss: 1.302183  [   48/ 3200]\n",
            "loss: 1.110006  [   64/ 3200]\n",
            "loss: 0.995660  [   80/ 3200]\n",
            "loss: 1.155810  [   96/ 3200]\n",
            "loss: 1.347216  [  112/ 3200]\n",
            "loss: 1.205207  [  128/ 3200]\n",
            "loss: 0.942851  [  144/ 3200]\n",
            "loss: 1.296025  [  160/ 3200]\n",
            "loss: 1.104728  [  176/ 3200]\n",
            "loss: 0.916109  [  192/ 3200]\n",
            "loss: 1.044186  [  208/ 3200]\n",
            "loss: 1.152062  [  224/ 3200]\n",
            "loss: 0.989257  [  240/ 3200]\n",
            "loss: 1.082540  [  256/ 3200]\n",
            "loss: 1.063171  [  272/ 3200]\n",
            "loss: 0.809983  [  288/ 3200]\n",
            "loss: 1.151910  [  304/ 3200]\n",
            "loss: 0.985128  [  320/ 3200]\n",
            "loss: 0.877504  [  336/ 3200]\n",
            "loss: 1.150824  [  352/ 3200]\n",
            "loss: 1.008242  [  368/ 3200]\n",
            "loss: 1.087793  [  384/ 3200]\n",
            "loss: 0.980138  [  400/ 3200]\n",
            "loss: 0.937760  [  416/ 3200]\n",
            "loss: 1.147050  [  432/ 3200]\n",
            "loss: 1.026815  [  448/ 3200]\n",
            "loss: 0.953660  [  464/ 3200]\n",
            "loss: 1.065179  [  480/ 3200]\n",
            "loss: 1.044457  [  496/ 3200]\n",
            "loss: 0.840787  [  512/ 3200]\n",
            "loss: 1.219495  [  528/ 3200]\n",
            "loss: 0.965839  [  544/ 3200]\n",
            "loss: 0.910041  [  560/ 3200]\n",
            "loss: 1.087794  [  576/ 3200]\n",
            "loss: 0.825244  [  592/ 3200]\n",
            "loss: 0.783865  [  608/ 3200]\n",
            "loss: 1.150650  [  624/ 3200]\n",
            "loss: 1.034211  [  640/ 3200]\n",
            "loss: 0.820130  [  656/ 3200]\n",
            "loss: 1.203142  [  672/ 3200]\n",
            "loss: 1.239325  [  688/ 3200]\n",
            "loss: 0.899690  [  704/ 3200]\n",
            "loss: 1.192854  [  720/ 3200]\n",
            "loss: 1.063234  [  736/ 3200]\n",
            "loss: 1.177223  [  752/ 3200]\n",
            "loss: 1.194649  [  768/ 3200]\n",
            "loss: 0.918821  [  784/ 3200]\n",
            "loss: 0.890248  [  800/ 3200]\n",
            "loss: 0.990001  [  816/ 3200]\n",
            "loss: 0.984309  [  832/ 3200]\n",
            "loss: 1.154678  [  848/ 3200]\n",
            "loss: 1.054341  [  864/ 3200]\n",
            "loss: 1.099448  [  880/ 3200]\n",
            "loss: 1.055444  [  896/ 3200]\n",
            "loss: 1.030415  [  912/ 3200]\n",
            "loss: 1.134786  [  928/ 3200]\n",
            "loss: 1.130847  [  944/ 3200]\n",
            "loss: 0.969090  [  960/ 3200]\n",
            "loss: 1.106033  [  976/ 3200]\n",
            "loss: 0.840973  [  992/ 3200]\n",
            "loss: 1.227273  [ 1008/ 3200]\n",
            "loss: 1.166385  [ 1024/ 3200]\n",
            "loss: 0.946008  [ 1040/ 3200]\n",
            "loss: 1.045547  [ 1056/ 3200]\n",
            "loss: 1.044781  [ 1072/ 3200]\n",
            "loss: 0.932063  [ 1088/ 3200]\n",
            "loss: 1.093192  [ 1104/ 3200]\n",
            "loss: 0.838240  [ 1120/ 3200]\n",
            "loss: 1.043113  [ 1136/ 3200]\n",
            "loss: 0.922722  [ 1152/ 3200]\n",
            "loss: 0.899022  [ 1168/ 3200]\n",
            "loss: 1.033099  [ 1184/ 3200]\n",
            "loss: 0.944341  [ 1200/ 3200]\n",
            "loss: 1.038427  [ 1216/ 3200]\n",
            "loss: 1.014086  [ 1232/ 3200]\n",
            "loss: 0.976332  [ 1248/ 3200]\n",
            "loss: 1.063760  [ 1264/ 3200]\n",
            "loss: 1.161140  [ 1280/ 3200]\n",
            "loss: 1.004525  [ 1296/ 3200]\n",
            "loss: 1.219227  [ 1312/ 3200]\n",
            "loss: 1.392400  [ 1328/ 3200]\n",
            "loss: 0.946680  [ 1344/ 3200]\n",
            "loss: 0.944960  [ 1360/ 3200]\n",
            "loss: 0.966599  [ 1376/ 3200]\n",
            "loss: 1.036828  [ 1392/ 3200]\n",
            "loss: 0.882661  [ 1408/ 3200]\n",
            "loss: 1.262878  [ 1424/ 3200]\n",
            "loss: 0.910748  [ 1440/ 3200]\n",
            "loss: 1.022896  [ 1456/ 3200]\n",
            "loss: 1.134732  [ 1472/ 3200]\n",
            "loss: 1.078316  [ 1488/ 3200]\n",
            "loss: 1.130808  [ 1504/ 3200]\n",
            "loss: 0.973027  [ 1520/ 3200]\n",
            "loss: 1.016158  [ 1536/ 3200]\n",
            "loss: 0.853944  [ 1552/ 3200]\n",
            "loss: 1.156319  [ 1568/ 3200]\n",
            "loss: 1.051799  [ 1584/ 3200]\n",
            "loss: 1.126545  [ 1600/ 3200]\n",
            "loss: 1.202223  [ 1616/ 3200]\n",
            "loss: 0.956327  [ 1632/ 3200]\n",
            "loss: 0.944098  [ 1648/ 3200]\n",
            "loss: 1.054849  [ 1664/ 3200]\n",
            "loss: 0.955146  [ 1680/ 3200]\n",
            "loss: 1.000761  [ 1696/ 3200]\n",
            "loss: 1.080918  [ 1712/ 3200]\n",
            "loss: 0.984651  [ 1728/ 3200]\n",
            "loss: 1.016365  [ 1744/ 3200]\n",
            "loss: 0.988737  [ 1760/ 3200]\n",
            "loss: 0.842983  [ 1776/ 3200]\n",
            "loss: 0.927506  [ 1792/ 3200]\n",
            "loss: 1.032735  [ 1808/ 3200]\n",
            "loss: 1.084466  [ 1824/ 3200]\n",
            "loss: 1.137851  [ 1840/ 3200]\n",
            "loss: 0.917801  [ 1856/ 3200]\n",
            "loss: 1.022479  [ 1872/ 3200]\n",
            "loss: 0.934521  [ 1888/ 3200]\n",
            "loss: 1.131269  [ 1904/ 3200]\n",
            "loss: 1.298091  [ 1920/ 3200]\n",
            "loss: 0.965380  [ 1936/ 3200]\n",
            "loss: 0.998011  [ 1952/ 3200]\n",
            "loss: 1.042701  [ 1968/ 3200]\n",
            "loss: 0.989782  [ 1984/ 3200]\n",
            "loss: 1.130227  [ 2000/ 3200]\n",
            "loss: 1.150912  [ 2016/ 3200]\n",
            "loss: 1.180272  [ 2032/ 3200]\n",
            "loss: 0.966353  [ 2048/ 3200]\n",
            "loss: 0.954754  [ 2064/ 3200]\n",
            "loss: 1.242131  [ 2080/ 3200]\n",
            "loss: 1.522065  [ 2096/ 3200]\n",
            "loss: 0.961285  [ 2112/ 3200]\n",
            "loss: 1.278136  [ 2128/ 3200]\n",
            "loss: 0.817900  [ 2144/ 3200]\n",
            "loss: 0.987244  [ 2160/ 3200]\n",
            "loss: 1.126997  [ 2176/ 3200]\n",
            "loss: 0.756505  [ 2192/ 3200]\n",
            "loss: 1.025893  [ 2208/ 3200]\n",
            "loss: 1.184570  [ 2224/ 3200]\n",
            "loss: 0.905524  [ 2240/ 3200]\n",
            "loss: 0.923636  [ 2256/ 3200]\n",
            "loss: 0.946827  [ 2272/ 3200]\n",
            "loss: 0.935650  [ 2288/ 3200]\n",
            "loss: 0.765989  [ 2304/ 3200]\n",
            "loss: 0.989371  [ 2320/ 3200]\n",
            "loss: 0.715209  [ 2336/ 3200]\n",
            "loss: 1.172873  [ 2352/ 3200]\n",
            "loss: 1.037197  [ 2368/ 3200]\n",
            "loss: 1.050944  [ 2384/ 3200]\n",
            "loss: 1.160409  [ 2400/ 3200]\n",
            "loss: 0.981946  [ 2416/ 3200]\n",
            "loss: 1.094669  [ 2432/ 3200]\n",
            "loss: 1.126220  [ 2448/ 3200]\n",
            "loss: 0.993174  [ 2464/ 3200]\n",
            "loss: 0.646733  [ 2480/ 3200]\n",
            "loss: 1.154072  [ 2496/ 3200]\n",
            "loss: 0.959951  [ 2512/ 3200]\n",
            "loss: 0.961435  [ 2528/ 3200]\n",
            "loss: 0.990318  [ 2544/ 3200]\n",
            "loss: 0.946255  [ 2560/ 3200]\n",
            "loss: 0.843471  [ 2576/ 3200]\n",
            "loss: 1.267613  [ 2592/ 3200]\n",
            "loss: 0.993263  [ 2608/ 3200]\n",
            "loss: 1.038782  [ 2624/ 3200]\n",
            "loss: 0.986548  [ 2640/ 3200]\n",
            "loss: 0.823269  [ 2656/ 3200]\n",
            "loss: 0.759131  [ 2672/ 3200]\n",
            "loss: 1.229018  [ 2688/ 3200]\n",
            "loss: 0.950752  [ 2704/ 3200]\n",
            "loss: 0.937880  [ 2720/ 3200]\n",
            "loss: 0.851085  [ 2736/ 3200]\n",
            "loss: 0.992787  [ 2752/ 3200]\n",
            "loss: 1.010357  [ 2768/ 3200]\n",
            "loss: 1.027282  [ 2784/ 3200]\n",
            "loss: 1.330021  [ 2800/ 3200]\n",
            "loss: 1.080422  [ 2816/ 3200]\n",
            "loss: 1.001550  [ 2832/ 3200]\n",
            "loss: 1.012901  [ 2848/ 3200]\n",
            "loss: 1.121581  [ 2864/ 3200]\n",
            "loss: 1.240251  [ 2880/ 3200]\n",
            "loss: 0.899907  [ 2896/ 3200]\n",
            "loss: 1.064339  [ 2912/ 3200]\n",
            "loss: 1.043524  [ 2928/ 3200]\n",
            "loss: 0.886245  [ 2944/ 3200]\n",
            "loss: 1.011396  [ 2960/ 3200]\n",
            "loss: 1.254574  [ 2976/ 3200]\n",
            "loss: 0.808554  [ 2992/ 3200]\n",
            "loss: 0.940929  [ 3008/ 3200]\n",
            "loss: 1.104161  [ 3024/ 3200]\n",
            "loss: 1.025891  [ 3040/ 3200]\n",
            "loss: 0.799039  [ 3056/ 3200]\n",
            "loss: 1.199215  [ 3072/ 3200]\n",
            "loss: 1.014966  [ 3088/ 3200]\n",
            "loss: 0.880049  [ 3104/ 3200]\n",
            "loss: 1.026948  [ 3120/ 3200]\n",
            "loss: 0.882610  [ 3136/ 3200]\n",
            "loss: 0.861554  [ 3152/ 3200]\n",
            "loss: 0.929952  [ 3168/ 3200]\n",
            "loss: 0.823723  [ 3184/ 3200]\n",
            "current epoch: 18\n",
            "\n",
            "loss: 0.907821  [    0/ 3200]\n",
            "loss: 1.007006  [   16/ 3200]\n",
            "loss: 0.791068  [   32/ 3200]\n",
            "loss: 0.983474  [   48/ 3200]\n",
            "loss: 1.045207  [   64/ 3200]\n",
            "loss: 1.167886  [   80/ 3200]\n",
            "loss: 1.143177  [   96/ 3200]\n",
            "loss: 1.250276  [  112/ 3200]\n",
            "loss: 0.943443  [  128/ 3200]\n",
            "loss: 1.033695  [  144/ 3200]\n",
            "loss: 0.841444  [  160/ 3200]\n",
            "loss: 1.059050  [  176/ 3200]\n",
            "loss: 0.953165  [  192/ 3200]\n",
            "loss: 1.025556  [  208/ 3200]\n",
            "loss: 1.071825  [  224/ 3200]\n",
            "loss: 1.082847  [  240/ 3200]\n",
            "loss: 0.893242  [  256/ 3200]\n",
            "loss: 0.944974  [  272/ 3200]\n",
            "loss: 1.199263  [  288/ 3200]\n",
            "loss: 0.939603  [  304/ 3200]\n",
            "loss: 1.028803  [  320/ 3200]\n",
            "loss: 1.262786  [  336/ 3200]\n",
            "loss: 1.090582  [  352/ 3200]\n",
            "loss: 1.053939  [  368/ 3200]\n",
            "loss: 0.900603  [  384/ 3200]\n",
            "loss: 1.076761  [  400/ 3200]\n",
            "loss: 1.109948  [  416/ 3200]\n",
            "loss: 1.124063  [  432/ 3200]\n",
            "loss: 1.127663  [  448/ 3200]\n",
            "loss: 1.022285  [  464/ 3200]\n",
            "loss: 0.946643  [  480/ 3200]\n",
            "loss: 0.839087  [  496/ 3200]\n",
            "loss: 0.777912  [  512/ 3200]\n",
            "loss: 1.049806  [  528/ 3200]\n",
            "loss: 1.225573  [  544/ 3200]\n",
            "loss: 0.849327  [  560/ 3200]\n",
            "loss: 1.069880  [  576/ 3200]\n",
            "loss: 1.113064  [  592/ 3200]\n",
            "loss: 1.160304  [  608/ 3200]\n",
            "loss: 1.338209  [  624/ 3200]\n",
            "loss: 0.777410  [  640/ 3200]\n",
            "loss: 1.021539  [  656/ 3200]\n",
            "loss: 0.907674  [  672/ 3200]\n",
            "loss: 0.864469  [  688/ 3200]\n",
            "loss: 1.189677  [  704/ 3200]\n",
            "loss: 0.926167  [  720/ 3200]\n",
            "loss: 0.882937  [  736/ 3200]\n",
            "loss: 0.814498  [  752/ 3200]\n",
            "loss: 1.042503  [  768/ 3200]\n",
            "loss: 1.123464  [  784/ 3200]\n",
            "loss: 1.028718  [  800/ 3200]\n",
            "loss: 0.923588  [  816/ 3200]\n",
            "loss: 1.082153  [  832/ 3200]\n",
            "loss: 0.971520  [  848/ 3200]\n",
            "loss: 1.369201  [  864/ 3200]\n",
            "loss: 0.754466  [  880/ 3200]\n",
            "loss: 0.911177  [  896/ 3200]\n",
            "loss: 0.755998  [  912/ 3200]\n",
            "loss: 1.255185  [  928/ 3200]\n",
            "loss: 0.864035  [  944/ 3200]\n",
            "loss: 1.108689  [  960/ 3200]\n",
            "loss: 0.857019  [  976/ 3200]\n",
            "loss: 0.969888  [  992/ 3200]\n",
            "loss: 0.926806  [ 1008/ 3200]\n",
            "loss: 1.044608  [ 1024/ 3200]\n",
            "loss: 1.190280  [ 1040/ 3200]\n",
            "loss: 1.216787  [ 1056/ 3200]\n",
            "loss: 0.974073  [ 1072/ 3200]\n",
            "loss: 0.843872  [ 1088/ 3200]\n",
            "loss: 0.986934  [ 1104/ 3200]\n",
            "loss: 1.257594  [ 1120/ 3200]\n",
            "loss: 0.769807  [ 1136/ 3200]\n",
            "loss: 0.831839  [ 1152/ 3200]\n",
            "loss: 0.802268  [ 1168/ 3200]\n",
            "loss: 1.066903  [ 1184/ 3200]\n",
            "loss: 0.988535  [ 1200/ 3200]\n",
            "loss: 0.892709  [ 1216/ 3200]\n",
            "loss: 0.953482  [ 1232/ 3200]\n",
            "loss: 1.085905  [ 1248/ 3200]\n",
            "loss: 1.010667  [ 1264/ 3200]\n",
            "loss: 0.956359  [ 1280/ 3200]\n",
            "loss: 0.948906  [ 1296/ 3200]\n",
            "loss: 1.143922  [ 1312/ 3200]\n",
            "loss: 0.898336  [ 1328/ 3200]\n",
            "loss: 1.343993  [ 1344/ 3200]\n",
            "loss: 1.003662  [ 1360/ 3200]\n",
            "loss: 0.996651  [ 1376/ 3200]\n",
            "loss: 0.775053  [ 1392/ 3200]\n",
            "loss: 1.065530  [ 1408/ 3200]\n",
            "loss: 1.017556  [ 1424/ 3200]\n",
            "loss: 1.237208  [ 1440/ 3200]\n",
            "loss: 1.049815  [ 1456/ 3200]\n",
            "loss: 1.060809  [ 1472/ 3200]\n",
            "loss: 0.982341  [ 1488/ 3200]\n",
            "loss: 0.754743  [ 1504/ 3200]\n",
            "loss: 1.096965  [ 1520/ 3200]\n",
            "loss: 1.076881  [ 1536/ 3200]\n",
            "loss: 1.168416  [ 1552/ 3200]\n",
            "loss: 0.750550  [ 1568/ 3200]\n",
            "loss: 1.153378  [ 1584/ 3200]\n",
            "loss: 1.020409  [ 1600/ 3200]\n",
            "loss: 0.809772  [ 1616/ 3200]\n",
            "loss: 0.782197  [ 1632/ 3200]\n",
            "loss: 1.016814  [ 1648/ 3200]\n",
            "loss: 1.293479  [ 1664/ 3200]\n",
            "loss: 1.053190  [ 1680/ 3200]\n",
            "loss: 1.246833  [ 1696/ 3200]\n",
            "loss: 1.023926  [ 1712/ 3200]\n",
            "loss: 0.886334  [ 1728/ 3200]\n",
            "loss: 1.023779  [ 1744/ 3200]\n",
            "loss: 0.994125  [ 1760/ 3200]\n",
            "loss: 0.953251  [ 1776/ 3200]\n",
            "loss: 1.124232  [ 1792/ 3200]\n",
            "loss: 0.915003  [ 1808/ 3200]\n",
            "loss: 1.048735  [ 1824/ 3200]\n",
            "loss: 0.926913  [ 1840/ 3200]\n",
            "loss: 0.970369  [ 1856/ 3200]\n",
            "loss: 1.018170  [ 1872/ 3200]\n",
            "loss: 1.361666  [ 1888/ 3200]\n",
            "loss: 0.973604  [ 1904/ 3200]\n",
            "loss: 0.980027  [ 1920/ 3200]\n",
            "loss: 1.017008  [ 1936/ 3200]\n",
            "loss: 1.061244  [ 1952/ 3200]\n",
            "loss: 1.011926  [ 1968/ 3200]\n",
            "loss: 0.925399  [ 1984/ 3200]\n",
            "loss: 1.026822  [ 2000/ 3200]\n",
            "loss: 1.476928  [ 2016/ 3200]\n",
            "loss: 0.994962  [ 2032/ 3200]\n",
            "loss: 0.977262  [ 2048/ 3200]\n",
            "loss: 0.964024  [ 2064/ 3200]\n",
            "loss: 0.892468  [ 2080/ 3200]\n",
            "loss: 1.038142  [ 2096/ 3200]\n",
            "loss: 1.011802  [ 2112/ 3200]\n",
            "loss: 0.936760  [ 2128/ 3200]\n",
            "loss: 0.747328  [ 2144/ 3200]\n",
            "loss: 1.205071  [ 2160/ 3200]\n",
            "loss: 1.318115  [ 2176/ 3200]\n",
            "loss: 1.033317  [ 2192/ 3200]\n",
            "loss: 1.048953  [ 2208/ 3200]\n",
            "loss: 0.927739  [ 2224/ 3200]\n",
            "loss: 0.750319  [ 2240/ 3200]\n",
            "loss: 0.967901  [ 2256/ 3200]\n",
            "loss: 0.989536  [ 2272/ 3200]\n",
            "loss: 1.157899  [ 2288/ 3200]\n",
            "loss: 1.152928  [ 2304/ 3200]\n",
            "loss: 1.034427  [ 2320/ 3200]\n",
            "loss: 0.870880  [ 2336/ 3200]\n",
            "loss: 0.998227  [ 2352/ 3200]\n",
            "loss: 1.156905  [ 2368/ 3200]\n",
            "loss: 1.289145  [ 2384/ 3200]\n",
            "loss: 1.242548  [ 2400/ 3200]\n",
            "loss: 0.796554  [ 2416/ 3200]\n",
            "loss: 1.109460  [ 2432/ 3200]\n",
            "loss: 0.878089  [ 2448/ 3200]\n",
            "loss: 0.945645  [ 2464/ 3200]\n",
            "loss: 1.044509  [ 2480/ 3200]\n",
            "loss: 0.940629  [ 2496/ 3200]\n",
            "loss: 1.153528  [ 2512/ 3200]\n",
            "loss: 0.793468  [ 2528/ 3200]\n",
            "loss: 0.975291  [ 2544/ 3200]\n",
            "loss: 0.986652  [ 2560/ 3200]\n",
            "loss: 1.019595  [ 2576/ 3200]\n",
            "loss: 1.013989  [ 2592/ 3200]\n",
            "loss: 1.008003  [ 2608/ 3200]\n",
            "loss: 1.179071  [ 2624/ 3200]\n",
            "loss: 0.822452  [ 2640/ 3200]\n",
            "loss: 0.858688  [ 2656/ 3200]\n",
            "loss: 0.939919  [ 2672/ 3200]\n",
            "loss: 1.115999  [ 2688/ 3200]\n",
            "loss: 1.188064  [ 2704/ 3200]\n",
            "loss: 0.887373  [ 2720/ 3200]\n",
            "loss: 1.059718  [ 2736/ 3200]\n",
            "loss: 0.946512  [ 2752/ 3200]\n",
            "loss: 0.884610  [ 2768/ 3200]\n",
            "loss: 1.072279  [ 2784/ 3200]\n",
            "loss: 1.058189  [ 2800/ 3200]\n",
            "loss: 0.876865  [ 2816/ 3200]\n",
            "loss: 0.868080  [ 2832/ 3200]\n",
            "loss: 1.007569  [ 2848/ 3200]\n",
            "loss: 0.824026  [ 2864/ 3200]\n",
            "loss: 0.878914  [ 2880/ 3200]\n",
            "loss: 1.089792  [ 2896/ 3200]\n",
            "loss: 1.353590  [ 2912/ 3200]\n",
            "loss: 1.058683  [ 2928/ 3200]\n",
            "loss: 1.001700  [ 2944/ 3200]\n",
            "loss: 1.004840  [ 2960/ 3200]\n",
            "loss: 1.221892  [ 2976/ 3200]\n",
            "loss: 1.189155  [ 2992/ 3200]\n",
            "loss: 0.945536  [ 3008/ 3200]\n",
            "loss: 0.878219  [ 3024/ 3200]\n",
            "loss: 1.116879  [ 3040/ 3200]\n",
            "loss: 1.180378  [ 3056/ 3200]\n",
            "loss: 1.053535  [ 3072/ 3200]\n",
            "loss: 1.001669  [ 3088/ 3200]\n",
            "loss: 1.031719  [ 3104/ 3200]\n",
            "loss: 0.948403  [ 3120/ 3200]\n",
            "loss: 0.982397  [ 3136/ 3200]\n",
            "loss: 0.923110  [ 3152/ 3200]\n",
            "loss: 0.925575  [ 3168/ 3200]\n",
            "loss: 1.097857  [ 3184/ 3200]\n",
            "current epoch: 19\n",
            "\n",
            "loss: 1.052689  [    0/ 3200]\n",
            "loss: 1.004838  [   16/ 3200]\n",
            "loss: 0.870389  [   32/ 3200]\n",
            "loss: 1.112968  [   48/ 3200]\n",
            "loss: 0.924120  [   64/ 3200]\n",
            "loss: 1.052649  [   80/ 3200]\n",
            "loss: 0.938918  [   96/ 3200]\n",
            "loss: 1.091944  [  112/ 3200]\n",
            "loss: 0.849829  [  128/ 3200]\n",
            "loss: 0.806491  [  144/ 3200]\n",
            "loss: 1.072382  [  160/ 3200]\n",
            "loss: 0.870606  [  176/ 3200]\n",
            "loss: 0.944552  [  192/ 3200]\n",
            "loss: 1.041140  [  208/ 3200]\n",
            "loss: 1.074251  [  224/ 3200]\n",
            "loss: 0.808383  [  240/ 3200]\n",
            "loss: 1.211569  [  256/ 3200]\n",
            "loss: 0.957980  [  272/ 3200]\n",
            "loss: 0.976279  [  288/ 3200]\n",
            "loss: 1.329383  [  304/ 3200]\n",
            "loss: 0.940228  [  320/ 3200]\n",
            "loss: 1.078604  [  336/ 3200]\n",
            "loss: 0.919267  [  352/ 3200]\n",
            "loss: 0.927963  [  368/ 3200]\n",
            "loss: 0.678605  [  384/ 3200]\n",
            "loss: 1.247532  [  400/ 3200]\n",
            "loss: 0.951342  [  416/ 3200]\n",
            "loss: 1.161156  [  432/ 3200]\n",
            "loss: 1.287053  [  448/ 3200]\n",
            "loss: 1.285453  [  464/ 3200]\n",
            "loss: 0.883169  [  480/ 3200]\n",
            "loss: 0.838721  [  496/ 3200]\n",
            "loss: 1.115675  [  512/ 3200]\n",
            "loss: 1.145405  [  528/ 3200]\n",
            "loss: 1.246337  [  544/ 3200]\n",
            "loss: 1.185021  [  560/ 3200]\n",
            "loss: 1.018485  [  576/ 3200]\n",
            "loss: 1.271971  [  592/ 3200]\n",
            "loss: 1.063979  [  608/ 3200]\n",
            "loss: 0.856584  [  624/ 3200]\n",
            "loss: 0.915854  [  640/ 3200]\n",
            "loss: 0.928607  [  656/ 3200]\n",
            "loss: 1.307447  [  672/ 3200]\n",
            "loss: 0.987917  [  688/ 3200]\n",
            "loss: 0.868292  [  704/ 3200]\n",
            "loss: 0.923944  [  720/ 3200]\n",
            "loss: 0.911808  [  736/ 3200]\n",
            "loss: 1.116767  [  752/ 3200]\n",
            "loss: 0.935690  [  768/ 3200]\n",
            "loss: 1.087556  [  784/ 3200]\n",
            "loss: 1.090888  [  800/ 3200]\n",
            "loss: 0.906881  [  816/ 3200]\n",
            "loss: 0.784762  [  832/ 3200]\n",
            "loss: 0.833975  [  848/ 3200]\n",
            "loss: 0.635234  [  864/ 3200]\n",
            "loss: 0.922771  [  880/ 3200]\n",
            "loss: 0.762592  [  896/ 3200]\n",
            "loss: 0.954867  [  912/ 3200]\n",
            "loss: 1.127383  [  928/ 3200]\n",
            "loss: 0.949343  [  944/ 3200]\n",
            "loss: 0.991231  [  960/ 3200]\n",
            "loss: 0.893745  [  976/ 3200]\n",
            "loss: 0.896834  [  992/ 3200]\n",
            "loss: 0.969118  [ 1008/ 3200]\n",
            "loss: 0.991957  [ 1024/ 3200]\n",
            "loss: 1.205567  [ 1040/ 3200]\n",
            "loss: 0.970897  [ 1056/ 3200]\n",
            "loss: 0.921933  [ 1072/ 3200]\n",
            "loss: 0.809053  [ 1088/ 3200]\n",
            "loss: 1.105292  [ 1104/ 3200]\n",
            "loss: 1.039540  [ 1120/ 3200]\n",
            "loss: 0.954939  [ 1136/ 3200]\n",
            "loss: 0.967166  [ 1152/ 3200]\n",
            "loss: 0.831413  [ 1168/ 3200]\n",
            "loss: 0.796039  [ 1184/ 3200]\n",
            "loss: 0.732089  [ 1200/ 3200]\n",
            "loss: 1.017111  [ 1216/ 3200]\n",
            "loss: 0.894996  [ 1232/ 3200]\n",
            "loss: 1.093778  [ 1248/ 3200]\n",
            "loss: 1.177837  [ 1264/ 3200]\n",
            "loss: 0.958106  [ 1280/ 3200]\n",
            "loss: 1.072179  [ 1296/ 3200]\n",
            "loss: 0.843670  [ 1312/ 3200]\n",
            "loss: 1.088312  [ 1328/ 3200]\n",
            "loss: 1.052015  [ 1344/ 3200]\n",
            "loss: 0.884803  [ 1360/ 3200]\n",
            "loss: 0.784602  [ 1376/ 3200]\n",
            "loss: 1.098106  [ 1392/ 3200]\n",
            "loss: 0.974203  [ 1408/ 3200]\n",
            "loss: 0.918253  [ 1424/ 3200]\n",
            "loss: 1.122308  [ 1440/ 3200]\n",
            "loss: 1.010812  [ 1456/ 3200]\n",
            "loss: 1.036988  [ 1472/ 3200]\n",
            "loss: 0.814551  [ 1488/ 3200]\n",
            "loss: 1.364849  [ 1504/ 3200]\n",
            "loss: 0.893601  [ 1520/ 3200]\n",
            "loss: 0.872886  [ 1536/ 3200]\n",
            "loss: 1.183306  [ 1552/ 3200]\n",
            "loss: 0.830346  [ 1568/ 3200]\n",
            "loss: 1.120617  [ 1584/ 3200]\n",
            "loss: 0.994268  [ 1600/ 3200]\n",
            "loss: 0.855942  [ 1616/ 3200]\n",
            "loss: 1.051916  [ 1632/ 3200]\n",
            "loss: 0.892696  [ 1648/ 3200]\n",
            "loss: 0.912623  [ 1664/ 3200]\n",
            "loss: 0.917787  [ 1680/ 3200]\n",
            "loss: 0.894052  [ 1696/ 3200]\n",
            "loss: 0.924315  [ 1712/ 3200]\n",
            "loss: 1.228318  [ 1728/ 3200]\n",
            "loss: 1.053156  [ 1744/ 3200]\n",
            "loss: 0.881064  [ 1760/ 3200]\n",
            "loss: 1.095029  [ 1776/ 3200]\n",
            "loss: 1.108396  [ 1792/ 3200]\n",
            "loss: 1.034148  [ 1808/ 3200]\n",
            "loss: 0.983661  [ 1824/ 3200]\n",
            "loss: 1.301215  [ 1840/ 3200]\n",
            "loss: 0.947604  [ 1856/ 3200]\n",
            "loss: 1.094140  [ 1872/ 3200]\n",
            "loss: 0.849851  [ 1888/ 3200]\n",
            "loss: 0.929087  [ 1904/ 3200]\n",
            "loss: 1.006045  [ 1920/ 3200]\n",
            "loss: 0.975116  [ 1936/ 3200]\n",
            "loss: 1.084064  [ 1952/ 3200]\n",
            "loss: 1.086175  [ 1968/ 3200]\n",
            "loss: 1.172027  [ 1984/ 3200]\n",
            "loss: 1.000807  [ 2000/ 3200]\n",
            "loss: 1.132864  [ 2016/ 3200]\n",
            "loss: 1.055329  [ 2032/ 3200]\n",
            "loss: 1.296396  [ 2048/ 3200]\n",
            "loss: 1.159349  [ 2064/ 3200]\n",
            "loss: 0.844749  [ 2080/ 3200]\n",
            "loss: 0.960927  [ 2096/ 3200]\n",
            "loss: 1.044290  [ 2112/ 3200]\n",
            "loss: 1.101558  [ 2128/ 3200]\n",
            "loss: 1.253653  [ 2144/ 3200]\n",
            "loss: 0.934471  [ 2160/ 3200]\n",
            "loss: 0.852694  [ 2176/ 3200]\n",
            "loss: 0.910028  [ 2192/ 3200]\n",
            "loss: 0.917798  [ 2208/ 3200]\n",
            "loss: 0.804427  [ 2224/ 3200]\n",
            "loss: 1.053239  [ 2240/ 3200]\n",
            "loss: 1.062449  [ 2256/ 3200]\n",
            "loss: 0.996853  [ 2272/ 3200]\n",
            "loss: 0.820833  [ 2288/ 3200]\n",
            "loss: 0.995870  [ 2304/ 3200]\n",
            "loss: 0.998197  [ 2320/ 3200]\n",
            "loss: 1.079100  [ 2336/ 3200]\n",
            "loss: 0.956429  [ 2352/ 3200]\n",
            "loss: 0.951872  [ 2368/ 3200]\n",
            "loss: 1.163887  [ 2384/ 3200]\n",
            "loss: 0.910933  [ 2400/ 3200]\n",
            "loss: 1.098601  [ 2416/ 3200]\n",
            "loss: 0.630494  [ 2432/ 3200]\n",
            "loss: 1.040811  [ 2448/ 3200]\n",
            "loss: 1.182402  [ 2464/ 3200]\n",
            "loss: 0.989191  [ 2480/ 3200]\n",
            "loss: 0.772403  [ 2496/ 3200]\n",
            "loss: 0.977864  [ 2512/ 3200]\n",
            "loss: 1.045216  [ 2528/ 3200]\n",
            "loss: 1.041360  [ 2544/ 3200]\n",
            "loss: 0.994569  [ 2560/ 3200]\n",
            "loss: 1.199572  [ 2576/ 3200]\n",
            "loss: 0.998408  [ 2592/ 3200]\n",
            "loss: 1.207686  [ 2608/ 3200]\n",
            "loss: 1.247109  [ 2624/ 3200]\n",
            "loss: 1.059871  [ 2640/ 3200]\n",
            "loss: 1.048021  [ 2656/ 3200]\n",
            "loss: 1.081951  [ 2672/ 3200]\n",
            "loss: 0.902367  [ 2688/ 3200]\n",
            "loss: 1.235189  [ 2704/ 3200]\n",
            "loss: 0.934784  [ 2720/ 3200]\n",
            "loss: 1.145799  [ 2736/ 3200]\n",
            "loss: 1.090724  [ 2752/ 3200]\n",
            "loss: 1.032163  [ 2768/ 3200]\n",
            "loss: 1.175942  [ 2784/ 3200]\n",
            "loss: 0.845028  [ 2800/ 3200]\n",
            "loss: 1.050460  [ 2816/ 3200]\n",
            "loss: 1.065290  [ 2832/ 3200]\n",
            "loss: 0.791855  [ 2848/ 3200]\n",
            "loss: 0.958873  [ 2864/ 3200]\n",
            "loss: 1.208673  [ 2880/ 3200]\n",
            "loss: 1.165344  [ 2896/ 3200]\n",
            "loss: 0.750145  [ 2912/ 3200]\n",
            "loss: 0.901911  [ 2928/ 3200]\n",
            "loss: 1.079248  [ 2944/ 3200]\n",
            "loss: 1.011381  [ 2960/ 3200]\n",
            "loss: 0.846000  [ 2976/ 3200]\n",
            "loss: 0.894236  [ 2992/ 3200]\n",
            "loss: 1.008315  [ 3008/ 3200]\n",
            "loss: 0.906894  [ 3024/ 3200]\n",
            "loss: 1.069236  [ 3040/ 3200]\n",
            "loss: 0.967788  [ 3056/ 3200]\n",
            "loss: 1.050183  [ 3072/ 3200]\n",
            "loss: 1.077433  [ 3088/ 3200]\n",
            "loss: 1.104282  [ 3104/ 3200]\n",
            "loss: 0.604658  [ 3120/ 3200]\n",
            "loss: 0.974230  [ 3136/ 3200]\n",
            "loss: 1.370764  [ 3152/ 3200]\n",
            "loss: 1.018655  [ 3168/ 3200]\n",
            "loss: 1.039363  [ 3184/ 3200]\n",
            "current epoch: 20\n",
            "\n",
            "loss: 0.947735  [    0/ 3200]\n",
            "loss: 0.973541  [   16/ 3200]\n",
            "loss: 0.889603  [   32/ 3200]\n",
            "loss: 1.091077  [   48/ 3200]\n",
            "loss: 0.836210  [   64/ 3200]\n",
            "loss: 1.428548  [   80/ 3200]\n",
            "loss: 0.959188  [   96/ 3200]\n",
            "loss: 1.013216  [  112/ 3200]\n",
            "loss: 0.865998  [  128/ 3200]\n",
            "loss: 0.886628  [  144/ 3200]\n",
            "loss: 0.880940  [  160/ 3200]\n",
            "loss: 0.973077  [  176/ 3200]\n",
            "loss: 1.068700  [  192/ 3200]\n",
            "loss: 0.992046  [  208/ 3200]\n",
            "loss: 0.956253  [  224/ 3200]\n",
            "loss: 0.891175  [  240/ 3200]\n",
            "loss: 1.095161  [  256/ 3200]\n",
            "loss: 0.983363  [  272/ 3200]\n",
            "loss: 1.074336  [  288/ 3200]\n",
            "loss: 0.740936  [  304/ 3200]\n",
            "loss: 1.268564  [  320/ 3200]\n",
            "loss: 0.891733  [  336/ 3200]\n",
            "loss: 1.660898  [  352/ 3200]\n",
            "loss: 0.771816  [  368/ 3200]\n",
            "loss: 1.193151  [  384/ 3200]\n",
            "loss: 1.263668  [  400/ 3200]\n",
            "loss: 1.173038  [  416/ 3200]\n",
            "loss: 1.132503  [  432/ 3200]\n",
            "loss: 0.945182  [  448/ 3200]\n",
            "loss: 0.698702  [  464/ 3200]\n",
            "loss: 0.965651  [  480/ 3200]\n",
            "loss: 0.829048  [  496/ 3200]\n",
            "loss: 0.804975  [  512/ 3200]\n",
            "loss: 0.842276  [  528/ 3200]\n",
            "loss: 0.958056  [  544/ 3200]\n",
            "loss: 0.902214  [  560/ 3200]\n",
            "loss: 0.708722  [  576/ 3200]\n",
            "loss: 1.073038  [  592/ 3200]\n",
            "loss: 0.989760  [  608/ 3200]\n",
            "loss: 1.218863  [  624/ 3200]\n",
            "loss: 0.841801  [  640/ 3200]\n",
            "loss: 1.130620  [  656/ 3200]\n",
            "loss: 1.451140  [  672/ 3200]\n",
            "loss: 1.156757  [  688/ 3200]\n",
            "loss: 1.296981  [  704/ 3200]\n",
            "loss: 1.008573  [  720/ 3200]\n",
            "loss: 1.052072  [  736/ 3200]\n",
            "loss: 0.898038  [  752/ 3200]\n",
            "loss: 1.223558  [  768/ 3200]\n",
            "loss: 1.190148  [  784/ 3200]\n",
            "loss: 0.885763  [  800/ 3200]\n",
            "loss: 1.162173  [  816/ 3200]\n",
            "loss: 1.019802  [  832/ 3200]\n",
            "loss: 0.839109  [  848/ 3200]\n",
            "loss: 0.970921  [  864/ 3200]\n",
            "loss: 0.972843  [  880/ 3200]\n",
            "loss: 0.835840  [  896/ 3200]\n",
            "loss: 1.037764  [  912/ 3200]\n",
            "loss: 0.755634  [  928/ 3200]\n",
            "loss: 0.735939  [  944/ 3200]\n",
            "loss: 1.008291  [  960/ 3200]\n",
            "loss: 0.809445  [  976/ 3200]\n",
            "loss: 1.124917  [  992/ 3200]\n",
            "loss: 0.951175  [ 1008/ 3200]\n",
            "loss: 1.167371  [ 1024/ 3200]\n",
            "loss: 1.047454  [ 1040/ 3200]\n",
            "loss: 0.854795  [ 1056/ 3200]\n",
            "loss: 1.149575  [ 1072/ 3200]\n",
            "loss: 0.959665  [ 1088/ 3200]\n",
            "loss: 1.109619  [ 1104/ 3200]\n",
            "loss: 1.008736  [ 1120/ 3200]\n",
            "loss: 1.155620  [ 1136/ 3200]\n",
            "loss: 0.861011  [ 1152/ 3200]\n",
            "loss: 0.937897  [ 1168/ 3200]\n",
            "loss: 0.863787  [ 1184/ 3200]\n",
            "loss: 0.683460  [ 1200/ 3200]\n",
            "loss: 0.890988  [ 1216/ 3200]\n",
            "loss: 1.103312  [ 1232/ 3200]\n",
            "loss: 1.137548  [ 1248/ 3200]\n",
            "loss: 1.228697  [ 1264/ 3200]\n",
            "loss: 0.928269  [ 1280/ 3200]\n",
            "loss: 1.160113  [ 1296/ 3200]\n",
            "loss: 1.044322  [ 1312/ 3200]\n",
            "loss: 0.816860  [ 1328/ 3200]\n",
            "loss: 0.783299  [ 1344/ 3200]\n",
            "loss: 0.999858  [ 1360/ 3200]\n",
            "loss: 1.163347  [ 1376/ 3200]\n",
            "loss: 0.810281  [ 1392/ 3200]\n",
            "loss: 0.925733  [ 1408/ 3200]\n",
            "loss: 0.920293  [ 1424/ 3200]\n",
            "loss: 0.951629  [ 1440/ 3200]\n",
            "loss: 1.151010  [ 1456/ 3200]\n",
            "loss: 0.867648  [ 1472/ 3200]\n",
            "loss: 0.927247  [ 1488/ 3200]\n",
            "loss: 1.003272  [ 1504/ 3200]\n",
            "loss: 0.890637  [ 1520/ 3200]\n",
            "loss: 1.163734  [ 1536/ 3200]\n",
            "loss: 1.031857  [ 1552/ 3200]\n",
            "loss: 1.062742  [ 1568/ 3200]\n",
            "loss: 1.036863  [ 1584/ 3200]\n",
            "loss: 1.011197  [ 1600/ 3200]\n",
            "loss: 0.857412  [ 1616/ 3200]\n",
            "loss: 1.106023  [ 1632/ 3200]\n",
            "loss: 0.836300  [ 1648/ 3200]\n",
            "loss: 1.291175  [ 1664/ 3200]\n",
            "loss: 1.227586  [ 1680/ 3200]\n",
            "loss: 0.952467  [ 1696/ 3200]\n",
            "loss: 0.982501  [ 1712/ 3200]\n",
            "loss: 0.934743  [ 1728/ 3200]\n",
            "loss: 1.056481  [ 1744/ 3200]\n",
            "loss: 0.855326  [ 1760/ 3200]\n",
            "loss: 0.905016  [ 1776/ 3200]\n",
            "loss: 0.851115  [ 1792/ 3200]\n",
            "loss: 0.857240  [ 1808/ 3200]\n",
            "loss: 0.941814  [ 1824/ 3200]\n",
            "loss: 0.916291  [ 1840/ 3200]\n",
            "loss: 1.090006  [ 1856/ 3200]\n",
            "loss: 0.789875  [ 1872/ 3200]\n",
            "loss: 1.063117  [ 1888/ 3200]\n",
            "loss: 1.019450  [ 1904/ 3200]\n",
            "loss: 0.756958  [ 1920/ 3200]\n",
            "loss: 0.779905  [ 1936/ 3200]\n",
            "loss: 1.079826  [ 1952/ 3200]\n",
            "loss: 1.062322  [ 1968/ 3200]\n",
            "loss: 1.071250  [ 1984/ 3200]\n",
            "loss: 0.874539  [ 2000/ 3200]\n",
            "loss: 1.232118  [ 2016/ 3200]\n",
            "loss: 1.247081  [ 2032/ 3200]\n",
            "loss: 0.950296  [ 2048/ 3200]\n",
            "loss: 1.066573  [ 2064/ 3200]\n",
            "loss: 0.972510  [ 2080/ 3200]\n",
            "loss: 0.855946  [ 2096/ 3200]\n",
            "loss: 1.154258  [ 2112/ 3200]\n",
            "loss: 0.997584  [ 2128/ 3200]\n",
            "loss: 1.063081  [ 2144/ 3200]\n",
            "loss: 0.992707  [ 2160/ 3200]\n",
            "loss: 0.961205  [ 2176/ 3200]\n",
            "loss: 1.089755  [ 2192/ 3200]\n",
            "loss: 1.128398  [ 2208/ 3200]\n",
            "loss: 0.734331  [ 2224/ 3200]\n",
            "loss: 0.941479  [ 2240/ 3200]\n",
            "loss: 0.908901  [ 2256/ 3200]\n",
            "loss: 0.728780  [ 2272/ 3200]\n",
            "loss: 1.348600  [ 2288/ 3200]\n",
            "loss: 1.173388  [ 2304/ 3200]\n",
            "loss: 1.093235  [ 2320/ 3200]\n",
            "loss: 1.018159  [ 2336/ 3200]\n",
            "loss: 1.173067  [ 2352/ 3200]\n",
            "loss: 0.791257  [ 2368/ 3200]\n",
            "loss: 0.895600  [ 2384/ 3200]\n",
            "loss: 0.943402  [ 2400/ 3200]\n",
            "loss: 1.120868  [ 2416/ 3200]\n",
            "loss: 0.860146  [ 2432/ 3200]\n",
            "loss: 0.888170  [ 2448/ 3200]\n",
            "loss: 1.443601  [ 2464/ 3200]\n",
            "loss: 1.078736  [ 2480/ 3200]\n",
            "loss: 0.877136  [ 2496/ 3200]\n",
            "loss: 0.773770  [ 2512/ 3200]\n",
            "loss: 0.886221  [ 2528/ 3200]\n",
            "loss: 1.111419  [ 2544/ 3200]\n",
            "loss: 1.093959  [ 2560/ 3200]\n",
            "loss: 0.791994  [ 2576/ 3200]\n",
            "loss: 0.712799  [ 2592/ 3200]\n",
            "loss: 1.309400  [ 2608/ 3200]\n",
            "loss: 1.106496  [ 2624/ 3200]\n",
            "loss: 1.138284  [ 2640/ 3200]\n",
            "loss: 1.047645  [ 2656/ 3200]\n",
            "loss: 0.997388  [ 2672/ 3200]\n",
            "loss: 1.135003  [ 2688/ 3200]\n",
            "loss: 1.104504  [ 2704/ 3200]\n",
            "loss: 1.005942  [ 2720/ 3200]\n",
            "loss: 0.767243  [ 2736/ 3200]\n",
            "loss: 0.848011  [ 2752/ 3200]\n",
            "loss: 1.028747  [ 2768/ 3200]\n",
            "loss: 1.008617  [ 2784/ 3200]\n",
            "loss: 0.929248  [ 2800/ 3200]\n",
            "loss: 1.074272  [ 2816/ 3200]\n",
            "loss: 0.941061  [ 2832/ 3200]\n",
            "loss: 1.030823  [ 2848/ 3200]\n",
            "loss: 0.890296  [ 2864/ 3200]\n",
            "loss: 1.439236  [ 2880/ 3200]\n",
            "loss: 1.056200  [ 2896/ 3200]\n",
            "loss: 1.013739  [ 2912/ 3200]\n",
            "loss: 0.856373  [ 2928/ 3200]\n",
            "loss: 1.092924  [ 2944/ 3200]\n",
            "loss: 0.942659  [ 2960/ 3200]\n",
            "loss: 1.284986  [ 2976/ 3200]\n",
            "loss: 0.988287  [ 2992/ 3200]\n",
            "loss: 0.853304  [ 3008/ 3200]\n",
            "loss: 0.773693  [ 3024/ 3200]\n",
            "loss: 0.980642  [ 3040/ 3200]\n",
            "loss: 0.906984  [ 3056/ 3200]\n",
            "loss: 1.119009  [ 3072/ 3200]\n",
            "loss: 1.012567  [ 3088/ 3200]\n",
            "loss: 1.067844  [ 3104/ 3200]\n",
            "loss: 0.939312  [ 3120/ 3200]\n",
            "loss: 0.892453  [ 3136/ 3200]\n",
            "loss: 1.076919  [ 3152/ 3200]\n",
            "loss: 0.901471  [ 3168/ 3200]\n",
            "loss: 1.015252  [ 3184/ 3200]\n",
            "current epoch: 21\n",
            "\n",
            "loss: 1.069325  [    0/ 3200]\n",
            "loss: 1.267533  [   16/ 3200]\n",
            "loss: 0.984429  [   32/ 3200]\n",
            "loss: 1.198440  [   48/ 3200]\n",
            "loss: 1.087863  [   64/ 3200]\n",
            "loss: 0.951495  [   80/ 3200]\n",
            "loss: 0.916423  [   96/ 3200]\n",
            "loss: 1.311726  [  112/ 3200]\n",
            "loss: 1.225704  [  128/ 3200]\n",
            "loss: 0.831399  [  144/ 3200]\n",
            "loss: 0.766215  [  160/ 3200]\n",
            "loss: 0.928585  [  176/ 3200]\n",
            "loss: 1.110411  [  192/ 3200]\n",
            "loss: 0.979757  [  208/ 3200]\n",
            "loss: 0.755778  [  224/ 3200]\n",
            "loss: 0.893039  [  240/ 3200]\n",
            "loss: 0.927590  [  256/ 3200]\n",
            "loss: 0.893571  [  272/ 3200]\n",
            "loss: 0.937097  [  288/ 3200]\n",
            "loss: 0.890725  [  304/ 3200]\n",
            "loss: 1.145052  [  320/ 3200]\n",
            "loss: 0.985768  [  336/ 3200]\n",
            "loss: 0.894917  [  352/ 3200]\n",
            "loss: 1.185529  [  368/ 3200]\n",
            "loss: 0.920729  [  384/ 3200]\n",
            "loss: 1.142192  [  400/ 3200]\n",
            "loss: 1.283584  [  416/ 3200]\n",
            "loss: 0.755600  [  432/ 3200]\n",
            "loss: 0.980372  [  448/ 3200]\n",
            "loss: 1.155415  [  464/ 3200]\n",
            "loss: 0.932707  [  480/ 3200]\n",
            "loss: 0.828434  [  496/ 3200]\n",
            "loss: 1.093302  [  512/ 3200]\n",
            "loss: 1.108834  [  528/ 3200]\n",
            "loss: 0.859710  [  544/ 3200]\n",
            "loss: 0.842269  [  560/ 3200]\n",
            "loss: 0.942848  [  576/ 3200]\n",
            "loss: 1.109064  [  592/ 3200]\n",
            "loss: 0.958502  [  608/ 3200]\n",
            "loss: 1.045835  [  624/ 3200]\n",
            "loss: 0.821400  [  640/ 3200]\n",
            "loss: 0.953782  [  656/ 3200]\n",
            "loss: 0.704869  [  672/ 3200]\n",
            "loss: 1.188592  [  688/ 3200]\n",
            "loss: 1.085453  [  704/ 3200]\n",
            "loss: 1.290356  [  720/ 3200]\n",
            "loss: 1.372751  [  736/ 3200]\n",
            "loss: 1.031844  [  752/ 3200]\n",
            "loss: 0.928697  [  768/ 3200]\n",
            "loss: 1.117775  [  784/ 3200]\n",
            "loss: 1.031834  [  800/ 3200]\n",
            "loss: 1.000722  [  816/ 3200]\n",
            "loss: 0.937193  [  832/ 3200]\n",
            "loss: 1.144690  [  848/ 3200]\n",
            "loss: 1.152792  [  864/ 3200]\n",
            "loss: 0.856010  [  880/ 3200]\n",
            "loss: 0.920101  [  896/ 3200]\n",
            "loss: 0.912011  [  912/ 3200]\n",
            "loss: 0.850295  [  928/ 3200]\n",
            "loss: 0.898598  [  944/ 3200]\n",
            "loss: 0.844090  [  960/ 3200]\n",
            "loss: 0.798977  [  976/ 3200]\n",
            "loss: 0.992109  [  992/ 3200]\n",
            "loss: 0.892660  [ 1008/ 3200]\n",
            "loss: 0.904098  [ 1024/ 3200]\n",
            "loss: 0.733498  [ 1040/ 3200]\n",
            "loss: 1.257517  [ 1056/ 3200]\n",
            "loss: 0.870636  [ 1072/ 3200]\n",
            "loss: 1.007686  [ 1088/ 3200]\n",
            "loss: 1.133290  [ 1104/ 3200]\n",
            "loss: 0.853195  [ 1120/ 3200]\n",
            "loss: 0.747808  [ 1136/ 3200]\n",
            "loss: 0.899521  [ 1152/ 3200]\n",
            "loss: 0.872565  [ 1168/ 3200]\n",
            "loss: 0.828028  [ 1184/ 3200]\n",
            "loss: 1.006832  [ 1200/ 3200]\n",
            "loss: 0.910829  [ 1216/ 3200]\n",
            "loss: 1.094485  [ 1232/ 3200]\n",
            "loss: 0.954168  [ 1248/ 3200]\n",
            "loss: 1.058632  [ 1264/ 3200]\n",
            "loss: 1.119674  [ 1280/ 3200]\n",
            "loss: 1.304334  [ 1296/ 3200]\n",
            "loss: 1.026988  [ 1312/ 3200]\n",
            "loss: 0.925304  [ 1328/ 3200]\n",
            "loss: 1.106570  [ 1344/ 3200]\n",
            "loss: 1.143285  [ 1360/ 3200]\n",
            "loss: 1.079541  [ 1376/ 3200]\n",
            "loss: 1.034136  [ 1392/ 3200]\n",
            "loss: 1.553296  [ 1408/ 3200]\n",
            "loss: 0.842706  [ 1424/ 3200]\n",
            "loss: 0.786653  [ 1440/ 3200]\n",
            "loss: 1.363204  [ 1456/ 3200]\n",
            "loss: 0.753699  [ 1472/ 3200]\n",
            "loss: 0.974594  [ 1488/ 3200]\n",
            "loss: 1.037130  [ 1504/ 3200]\n",
            "loss: 1.017159  [ 1520/ 3200]\n",
            "loss: 1.100993  [ 1536/ 3200]\n",
            "loss: 1.263121  [ 1552/ 3200]\n",
            "loss: 0.891158  [ 1568/ 3200]\n",
            "loss: 0.906952  [ 1584/ 3200]\n",
            "loss: 1.085767  [ 1600/ 3200]\n",
            "loss: 0.818208  [ 1616/ 3200]\n",
            "loss: 0.847548  [ 1632/ 3200]\n",
            "loss: 0.715099  [ 1648/ 3200]\n",
            "loss: 0.941696  [ 1664/ 3200]\n",
            "loss: 1.033466  [ 1680/ 3200]\n",
            "loss: 0.865607  [ 1696/ 3200]\n",
            "loss: 1.011253  [ 1712/ 3200]\n",
            "loss: 0.885601  [ 1728/ 3200]\n",
            "loss: 0.844006  [ 1744/ 3200]\n",
            "loss: 1.004686  [ 1760/ 3200]\n",
            "loss: 1.021338  [ 1776/ 3200]\n",
            "loss: 0.783747  [ 1792/ 3200]\n",
            "loss: 0.859649  [ 1808/ 3200]\n",
            "loss: 1.079412  [ 1824/ 3200]\n",
            "loss: 0.998510  [ 1840/ 3200]\n",
            "loss: 0.976931  [ 1856/ 3200]\n",
            "loss: 0.996455  [ 1872/ 3200]\n",
            "loss: 0.869942  [ 1888/ 3200]\n",
            "loss: 1.082002  [ 1904/ 3200]\n",
            "loss: 0.979785  [ 1920/ 3200]\n",
            "loss: 1.082158  [ 1936/ 3200]\n",
            "loss: 1.095949  [ 1952/ 3200]\n",
            "loss: 1.177203  [ 1968/ 3200]\n",
            "loss: 0.904544  [ 1984/ 3200]\n",
            "loss: 0.923334  [ 2000/ 3200]\n",
            "loss: 0.991152  [ 2016/ 3200]\n",
            "loss: 1.059334  [ 2032/ 3200]\n",
            "loss: 1.078237  [ 2048/ 3200]\n",
            "loss: 1.021033  [ 2064/ 3200]\n",
            "loss: 0.834340  [ 2080/ 3200]\n",
            "loss: 0.956614  [ 2096/ 3200]\n",
            "loss: 1.034332  [ 2112/ 3200]\n",
            "loss: 1.307382  [ 2128/ 3200]\n",
            "loss: 1.069661  [ 2144/ 3200]\n",
            "loss: 0.872882  [ 2160/ 3200]\n",
            "loss: 1.012137  [ 2176/ 3200]\n",
            "loss: 1.178559  [ 2192/ 3200]\n",
            "loss: 1.228389  [ 2208/ 3200]\n",
            "loss: 1.162223  [ 2224/ 3200]\n",
            "loss: 0.881407  [ 2240/ 3200]\n",
            "loss: 0.906729  [ 2256/ 3200]\n",
            "loss: 0.839956  [ 2272/ 3200]\n",
            "loss: 1.016593  [ 2288/ 3200]\n",
            "loss: 1.028186  [ 2304/ 3200]\n",
            "loss: 1.103043  [ 2320/ 3200]\n",
            "loss: 1.250173  [ 2336/ 3200]\n",
            "loss: 0.866074  [ 2352/ 3200]\n",
            "loss: 0.881260  [ 2368/ 3200]\n",
            "loss: 1.029319  [ 2384/ 3200]\n",
            "loss: 1.001334  [ 2400/ 3200]\n",
            "loss: 1.005056  [ 2416/ 3200]\n",
            "loss: 1.437967  [ 2432/ 3200]\n",
            "loss: 0.975385  [ 2448/ 3200]\n",
            "loss: 0.737763  [ 2464/ 3200]\n",
            "loss: 1.303176  [ 2480/ 3200]\n",
            "loss: 0.927879  [ 2496/ 3200]\n",
            "loss: 1.183371  [ 2512/ 3200]\n",
            "loss: 0.908677  [ 2528/ 3200]\n",
            "loss: 1.088832  [ 2544/ 3200]\n",
            "loss: 0.686152  [ 2560/ 3200]\n",
            "loss: 1.220245  [ 2576/ 3200]\n",
            "loss: 0.910700  [ 2592/ 3200]\n",
            "loss: 0.793536  [ 2608/ 3200]\n",
            "loss: 1.044859  [ 2624/ 3200]\n",
            "loss: 0.910564  [ 2640/ 3200]\n",
            "loss: 0.905023  [ 2656/ 3200]\n",
            "loss: 0.909106  [ 2672/ 3200]\n",
            "loss: 0.989376  [ 2688/ 3200]\n",
            "loss: 1.122417  [ 2704/ 3200]\n",
            "loss: 1.178849  [ 2720/ 3200]\n",
            "loss: 0.946334  [ 2736/ 3200]\n",
            "loss: 0.777142  [ 2752/ 3200]\n",
            "loss: 0.981806  [ 2768/ 3200]\n",
            "loss: 1.077142  [ 2784/ 3200]\n",
            "loss: 1.081918  [ 2800/ 3200]\n",
            "loss: 1.174157  [ 2816/ 3200]\n",
            "loss: 0.689257  [ 2832/ 3200]\n",
            "loss: 0.923766  [ 2848/ 3200]\n",
            "loss: 0.745317  [ 2864/ 3200]\n",
            "loss: 0.981353  [ 2880/ 3200]\n",
            "loss: 0.832959  [ 2896/ 3200]\n",
            "loss: 1.035103  [ 2912/ 3200]\n",
            "loss: 0.907034  [ 2928/ 3200]\n",
            "loss: 0.913645  [ 2944/ 3200]\n",
            "loss: 1.018280  [ 2960/ 3200]\n",
            "loss: 0.927522  [ 2976/ 3200]\n",
            "loss: 1.124074  [ 2992/ 3200]\n",
            "loss: 1.005759  [ 3008/ 3200]\n",
            "loss: 0.754269  [ 3024/ 3200]\n",
            "loss: 0.898928  [ 3040/ 3200]\n",
            "loss: 0.915786  [ 3056/ 3200]\n",
            "loss: 0.877222  [ 3072/ 3200]\n",
            "loss: 0.838161  [ 3088/ 3200]\n",
            "loss: 1.062409  [ 3104/ 3200]\n",
            "loss: 0.918768  [ 3120/ 3200]\n",
            "loss: 0.829711  [ 3136/ 3200]\n",
            "loss: 1.129563  [ 3152/ 3200]\n",
            "loss: 0.730967  [ 3168/ 3200]\n",
            "loss: 1.022214  [ 3184/ 3200]\n",
            "current epoch: 22\n",
            "\n",
            "loss: 0.867572  [    0/ 3200]\n",
            "loss: 1.154948  [   16/ 3200]\n",
            "loss: 1.365688  [   32/ 3200]\n",
            "loss: 1.214751  [   48/ 3200]\n",
            "loss: 0.836054  [   64/ 3200]\n",
            "loss: 0.956506  [   80/ 3200]\n",
            "loss: 1.473149  [   96/ 3200]\n",
            "loss: 0.921116  [  112/ 3200]\n",
            "loss: 1.197525  [  128/ 3200]\n",
            "loss: 1.121911  [  144/ 3200]\n",
            "loss: 0.878620  [  160/ 3200]\n",
            "loss: 1.011894  [  176/ 3200]\n",
            "loss: 0.768552  [  192/ 3200]\n",
            "loss: 0.863669  [  208/ 3200]\n",
            "loss: 0.926688  [  224/ 3200]\n",
            "loss: 0.942652  [  240/ 3200]\n",
            "loss: 1.489383  [  256/ 3200]\n",
            "loss: 0.990034  [  272/ 3200]\n",
            "loss: 0.876879  [  288/ 3200]\n",
            "loss: 0.865058  [  304/ 3200]\n",
            "loss: 0.985817  [  320/ 3200]\n",
            "loss: 0.954276  [  336/ 3200]\n",
            "loss: 1.167147  [  352/ 3200]\n",
            "loss: 0.869656  [  368/ 3200]\n",
            "loss: 0.970179  [  384/ 3200]\n",
            "loss: 1.208665  [  400/ 3200]\n",
            "loss: 0.812493  [  416/ 3200]\n",
            "loss: 1.023411  [  432/ 3200]\n",
            "loss: 0.899681  [  448/ 3200]\n",
            "loss: 1.139481  [  464/ 3200]\n",
            "loss: 0.774087  [  480/ 3200]\n",
            "loss: 0.988268  [  496/ 3200]\n",
            "loss: 1.000731  [  512/ 3200]\n",
            "loss: 1.115442  [  528/ 3200]\n",
            "loss: 1.033941  [  544/ 3200]\n",
            "loss: 1.047248  [  560/ 3200]\n",
            "loss: 1.146920  [  576/ 3200]\n",
            "loss: 1.001199  [  592/ 3200]\n",
            "loss: 0.857518  [  608/ 3200]\n",
            "loss: 0.790268  [  624/ 3200]\n",
            "loss: 0.854150  [  640/ 3200]\n",
            "loss: 1.002910  [  656/ 3200]\n",
            "loss: 0.793679  [  672/ 3200]\n",
            "loss: 1.408287  [  688/ 3200]\n",
            "loss: 0.925186  [  704/ 3200]\n",
            "loss: 0.735755  [  720/ 3200]\n",
            "loss: 1.022526  [  736/ 3200]\n",
            "loss: 0.968435  [  752/ 3200]\n",
            "loss: 1.233963  [  768/ 3200]\n",
            "loss: 0.972293  [  784/ 3200]\n",
            "loss: 1.082422  [  800/ 3200]\n",
            "loss: 0.802625  [  816/ 3200]\n",
            "loss: 0.652305  [  832/ 3200]\n",
            "loss: 0.656540  [  848/ 3200]\n",
            "loss: 0.963213  [  864/ 3200]\n",
            "loss: 0.920189  [  880/ 3200]\n",
            "loss: 1.278584  [  896/ 3200]\n",
            "loss: 1.118166  [  912/ 3200]\n",
            "loss: 0.934543  [  928/ 3200]\n",
            "loss: 0.933443  [  944/ 3200]\n",
            "loss: 0.865912  [  960/ 3200]\n",
            "loss: 0.790322  [  976/ 3200]\n",
            "loss: 0.906167  [  992/ 3200]\n",
            "loss: 1.250462  [ 1008/ 3200]\n",
            "loss: 1.035199  [ 1024/ 3200]\n",
            "loss: 1.144978  [ 1040/ 3200]\n",
            "loss: 1.045297  [ 1056/ 3200]\n",
            "loss: 1.061674  [ 1072/ 3200]\n",
            "loss: 1.000844  [ 1088/ 3200]\n",
            "loss: 0.828215  [ 1104/ 3200]\n",
            "loss: 0.946547  [ 1120/ 3200]\n",
            "loss: 0.693786  [ 1136/ 3200]\n",
            "loss: 0.797300  [ 1152/ 3200]\n",
            "loss: 1.244220  [ 1168/ 3200]\n",
            "loss: 0.987862  [ 1184/ 3200]\n",
            "loss: 0.834155  [ 1200/ 3200]\n",
            "loss: 0.889380  [ 1216/ 3200]\n",
            "loss: 1.108874  [ 1232/ 3200]\n",
            "loss: 0.901685  [ 1248/ 3200]\n",
            "loss: 0.961147  [ 1264/ 3200]\n",
            "loss: 1.203770  [ 1280/ 3200]\n",
            "loss: 0.786968  [ 1296/ 3200]\n",
            "loss: 0.934463  [ 1312/ 3200]\n",
            "loss: 0.913769  [ 1328/ 3200]\n",
            "loss: 1.131233  [ 1344/ 3200]\n",
            "loss: 1.129241  [ 1360/ 3200]\n",
            "loss: 0.972497  [ 1376/ 3200]\n",
            "loss: 0.699219  [ 1392/ 3200]\n",
            "loss: 1.239179  [ 1408/ 3200]\n",
            "loss: 0.815188  [ 1424/ 3200]\n",
            "loss: 0.789821  [ 1440/ 3200]\n",
            "loss: 0.914743  [ 1456/ 3200]\n",
            "loss: 0.935299  [ 1472/ 3200]\n",
            "loss: 0.965312  [ 1488/ 3200]\n",
            "loss: 1.187443  [ 1504/ 3200]\n",
            "loss: 1.239191  [ 1520/ 3200]\n",
            "loss: 0.953770  [ 1536/ 3200]\n",
            "loss: 0.909031  [ 1552/ 3200]\n",
            "loss: 0.816189  [ 1568/ 3200]\n",
            "loss: 0.905263  [ 1584/ 3200]\n",
            "loss: 0.973329  [ 1600/ 3200]\n",
            "loss: 1.114077  [ 1616/ 3200]\n",
            "loss: 0.794452  [ 1632/ 3200]\n",
            "loss: 1.127250  [ 1648/ 3200]\n",
            "loss: 1.007027  [ 1664/ 3200]\n",
            "loss: 0.683644  [ 1680/ 3200]\n",
            "loss: 0.798301  [ 1696/ 3200]\n",
            "loss: 1.178316  [ 1712/ 3200]\n",
            "loss: 0.642492  [ 1728/ 3200]\n",
            "loss: 1.189196  [ 1744/ 3200]\n",
            "loss: 0.740526  [ 1760/ 3200]\n",
            "loss: 0.739799  [ 1776/ 3200]\n",
            "loss: 1.043181  [ 1792/ 3200]\n",
            "loss: 0.833698  [ 1808/ 3200]\n",
            "loss: 1.185219  [ 1824/ 3200]\n",
            "loss: 1.138449  [ 1840/ 3200]\n",
            "loss: 1.023694  [ 1856/ 3200]\n",
            "loss: 0.969833  [ 1872/ 3200]\n",
            "loss: 1.044648  [ 1888/ 3200]\n",
            "loss: 1.054677  [ 1904/ 3200]\n",
            "loss: 0.829705  [ 1920/ 3200]\n",
            "loss: 0.903320  [ 1936/ 3200]\n",
            "loss: 1.106773  [ 1952/ 3200]\n",
            "loss: 0.908199  [ 1968/ 3200]\n",
            "loss: 0.859209  [ 1984/ 3200]\n",
            "loss: 1.032612  [ 2000/ 3200]\n",
            "loss: 0.625912  [ 2016/ 3200]\n",
            "loss: 1.166768  [ 2032/ 3200]\n",
            "loss: 1.073383  [ 2048/ 3200]\n",
            "loss: 1.088631  [ 2064/ 3200]\n",
            "loss: 0.985841  [ 2080/ 3200]\n",
            "loss: 1.146281  [ 2096/ 3200]\n",
            "loss: 1.065377  [ 2112/ 3200]\n",
            "loss: 1.023757  [ 2128/ 3200]\n",
            "loss: 0.929178  [ 2144/ 3200]\n",
            "loss: 1.031224  [ 2160/ 3200]\n",
            "loss: 1.010722  [ 2176/ 3200]\n",
            "loss: 0.805717  [ 2192/ 3200]\n",
            "loss: 1.195344  [ 2208/ 3200]\n",
            "loss: 0.954015  [ 2224/ 3200]\n",
            "loss: 1.017946  [ 2240/ 3200]\n",
            "loss: 1.160892  [ 2256/ 3200]\n",
            "loss: 1.248558  [ 2272/ 3200]\n",
            "loss: 0.855707  [ 2288/ 3200]\n",
            "loss: 0.922195  [ 2304/ 3200]\n",
            "loss: 1.050385  [ 2320/ 3200]\n",
            "loss: 1.013147  [ 2336/ 3200]\n",
            "loss: 1.080997  [ 2352/ 3200]\n",
            "loss: 1.094137  [ 2368/ 3200]\n",
            "loss: 1.053717  [ 2384/ 3200]\n",
            "loss: 1.072146  [ 2400/ 3200]\n",
            "loss: 0.832946  [ 2416/ 3200]\n",
            "loss: 1.088828  [ 2432/ 3200]\n",
            "loss: 1.128160  [ 2448/ 3200]\n",
            "loss: 0.810296  [ 2464/ 3200]\n",
            "loss: 1.054138  [ 2480/ 3200]\n",
            "loss: 0.999535  [ 2496/ 3200]\n",
            "loss: 0.711508  [ 2512/ 3200]\n",
            "loss: 1.083036  [ 2528/ 3200]\n",
            "loss: 0.917764  [ 2544/ 3200]\n",
            "loss: 0.941397  [ 2560/ 3200]\n",
            "loss: 1.012970  [ 2576/ 3200]\n",
            "loss: 0.863075  [ 2592/ 3200]\n",
            "loss: 0.903547  [ 2608/ 3200]\n",
            "loss: 0.919166  [ 2624/ 3200]\n",
            "loss: 0.930687  [ 2640/ 3200]\n",
            "loss: 0.800862  [ 2656/ 3200]\n",
            "loss: 1.020211  [ 2672/ 3200]\n",
            "loss: 1.034855  [ 2688/ 3200]\n",
            "loss: 1.279409  [ 2704/ 3200]\n",
            "loss: 1.211108  [ 2720/ 3200]\n",
            "loss: 0.903571  [ 2736/ 3200]\n",
            "loss: 0.918791  [ 2752/ 3200]\n",
            "loss: 0.798087  [ 2768/ 3200]\n",
            "loss: 1.081461  [ 2784/ 3200]\n",
            "loss: 1.098278  [ 2800/ 3200]\n",
            "loss: 0.998184  [ 2816/ 3200]\n",
            "loss: 0.924766  [ 2832/ 3200]\n",
            "loss: 1.136258  [ 2848/ 3200]\n",
            "loss: 0.888426  [ 2864/ 3200]\n",
            "loss: 1.095263  [ 2880/ 3200]\n",
            "loss: 0.743385  [ 2896/ 3200]\n",
            "loss: 0.991957  [ 2912/ 3200]\n",
            "loss: 1.032722  [ 2928/ 3200]\n",
            "loss: 0.828082  [ 2944/ 3200]\n",
            "loss: 0.951687  [ 2960/ 3200]\n",
            "loss: 0.932670  [ 2976/ 3200]\n",
            "loss: 1.395985  [ 2992/ 3200]\n",
            "loss: 0.968304  [ 3008/ 3200]\n",
            "loss: 0.933579  [ 3024/ 3200]\n",
            "loss: 0.823665  [ 3040/ 3200]\n",
            "loss: 1.187311  [ 3056/ 3200]\n",
            "loss: 1.089344  [ 3072/ 3200]\n",
            "loss: 0.941990  [ 3088/ 3200]\n",
            "loss: 0.826934  [ 3104/ 3200]\n",
            "loss: 1.014157  [ 3120/ 3200]\n",
            "loss: 0.948178  [ 3136/ 3200]\n",
            "loss: 0.711150  [ 3152/ 3200]\n",
            "loss: 0.712585  [ 3168/ 3200]\n",
            "loss: 0.843652  [ 3184/ 3200]\n",
            "current epoch: 23\n",
            "\n",
            "loss: 1.331368  [    0/ 3200]\n",
            "loss: 1.132952  [   16/ 3200]\n",
            "loss: 1.120303  [   32/ 3200]\n",
            "loss: 0.977291  [   48/ 3200]\n",
            "loss: 1.060818  [   64/ 3200]\n",
            "loss: 0.768352  [   80/ 3200]\n",
            "loss: 1.141508  [   96/ 3200]\n",
            "loss: 0.930037  [  112/ 3200]\n",
            "loss: 1.073194  [  128/ 3200]\n",
            "loss: 0.972553  [  144/ 3200]\n",
            "loss: 0.583451  [  160/ 3200]\n",
            "loss: 0.880318  [  176/ 3200]\n",
            "loss: 1.091072  [  192/ 3200]\n",
            "loss: 0.718596  [  208/ 3200]\n",
            "loss: 1.126586  [  224/ 3200]\n",
            "loss: 1.211625  [  240/ 3200]\n",
            "loss: 0.678423  [  256/ 3200]\n",
            "loss: 1.095491  [  272/ 3200]\n",
            "loss: 1.027295  [  288/ 3200]\n",
            "loss: 1.138227  [  304/ 3200]\n",
            "loss: 0.796456  [  320/ 3200]\n",
            "loss: 1.107666  [  336/ 3200]\n",
            "loss: 1.050277  [  352/ 3200]\n",
            "loss: 1.258318  [  368/ 3200]\n",
            "loss: 0.843482  [  384/ 3200]\n",
            "loss: 1.098488  [  400/ 3200]\n",
            "loss: 1.194842  [  416/ 3200]\n",
            "loss: 1.234556  [  432/ 3200]\n",
            "loss: 1.009671  [  448/ 3200]\n",
            "loss: 0.929503  [  464/ 3200]\n",
            "loss: 1.007695  [  480/ 3200]\n",
            "loss: 1.041086  [  496/ 3200]\n",
            "loss: 1.049137  [  512/ 3200]\n",
            "loss: 0.889616  [  528/ 3200]\n",
            "loss: 0.707271  [  544/ 3200]\n",
            "loss: 1.332011  [  560/ 3200]\n",
            "loss: 1.072672  [  576/ 3200]\n",
            "loss: 1.158649  [  592/ 3200]\n",
            "loss: 0.920394  [  608/ 3200]\n",
            "loss: 1.140676  [  624/ 3200]\n",
            "loss: 0.914218  [  640/ 3200]\n",
            "loss: 0.977853  [  656/ 3200]\n",
            "loss: 0.522924  [  672/ 3200]\n",
            "loss: 1.181687  [  688/ 3200]\n",
            "loss: 0.749073  [  704/ 3200]\n",
            "loss: 0.829888  [  720/ 3200]\n",
            "loss: 0.962328  [  736/ 3200]\n",
            "loss: 1.029535  [  752/ 3200]\n",
            "loss: 0.910180  [  768/ 3200]\n",
            "loss: 0.916910  [  784/ 3200]\n",
            "loss: 1.174247  [  800/ 3200]\n",
            "loss: 0.884736  [  816/ 3200]\n",
            "loss: 0.932504  [  832/ 3200]\n",
            "loss: 1.452858  [  848/ 3200]\n",
            "loss: 1.140056  [  864/ 3200]\n",
            "loss: 0.890899  [  880/ 3200]\n",
            "loss: 1.019160  [  896/ 3200]\n",
            "loss: 1.026120  [  912/ 3200]\n",
            "loss: 1.116800  [  928/ 3200]\n",
            "loss: 0.619900  [  944/ 3200]\n",
            "loss: 0.963342  [  960/ 3200]\n",
            "loss: 1.523086  [  976/ 3200]\n",
            "loss: 1.053487  [  992/ 3200]\n",
            "loss: 0.894150  [ 1008/ 3200]\n",
            "loss: 0.721332  [ 1024/ 3200]\n",
            "loss: 1.209918  [ 1040/ 3200]\n",
            "loss: 1.034194  [ 1056/ 3200]\n",
            "loss: 1.191654  [ 1072/ 3200]\n",
            "loss: 0.772309  [ 1088/ 3200]\n",
            "loss: 0.986360  [ 1104/ 3200]\n",
            "loss: 1.010809  [ 1120/ 3200]\n",
            "loss: 1.160669  [ 1136/ 3200]\n",
            "loss: 0.885084  [ 1152/ 3200]\n",
            "loss: 0.830914  [ 1168/ 3200]\n",
            "loss: 0.751996  [ 1184/ 3200]\n",
            "loss: 0.793399  [ 1200/ 3200]\n",
            "loss: 0.939207  [ 1216/ 3200]\n",
            "loss: 0.644578  [ 1232/ 3200]\n",
            "loss: 0.627019  [ 1248/ 3200]\n",
            "loss: 1.045508  [ 1264/ 3200]\n",
            "loss: 1.058600  [ 1280/ 3200]\n",
            "loss: 0.696585  [ 1296/ 3200]\n",
            "loss: 0.880021  [ 1312/ 3200]\n",
            "loss: 0.949189  [ 1328/ 3200]\n",
            "loss: 1.156719  [ 1344/ 3200]\n",
            "loss: 0.767024  [ 1360/ 3200]\n",
            "loss: 0.953562  [ 1376/ 3200]\n",
            "loss: 0.925004  [ 1392/ 3200]\n",
            "loss: 1.403558  [ 1408/ 3200]\n",
            "loss: 1.193333  [ 1424/ 3200]\n",
            "loss: 1.184325  [ 1440/ 3200]\n",
            "loss: 0.852624  [ 1456/ 3200]\n",
            "loss: 0.954926  [ 1472/ 3200]\n",
            "loss: 1.120548  [ 1488/ 3200]\n",
            "loss: 1.259910  [ 1504/ 3200]\n",
            "loss: 0.797786  [ 1520/ 3200]\n",
            "loss: 0.931129  [ 1536/ 3200]\n",
            "loss: 0.953374  [ 1552/ 3200]\n",
            "loss: 0.956518  [ 1568/ 3200]\n",
            "loss: 0.990361  [ 1584/ 3200]\n",
            "loss: 0.798978  [ 1600/ 3200]\n",
            "loss: 0.828723  [ 1616/ 3200]\n",
            "loss: 0.986350  [ 1632/ 3200]\n",
            "loss: 1.019888  [ 1648/ 3200]\n",
            "loss: 0.945082  [ 1664/ 3200]\n",
            "loss: 1.062158  [ 1680/ 3200]\n",
            "loss: 0.691069  [ 1696/ 3200]\n",
            "loss: 0.745234  [ 1712/ 3200]\n",
            "loss: 0.928865  [ 1728/ 3200]\n",
            "loss: 1.005346  [ 1744/ 3200]\n",
            "loss: 0.882064  [ 1760/ 3200]\n",
            "loss: 0.754763  [ 1776/ 3200]\n",
            "loss: 0.921081  [ 1792/ 3200]\n",
            "loss: 0.879083  [ 1808/ 3200]\n",
            "loss: 0.953948  [ 1824/ 3200]\n",
            "loss: 1.050802  [ 1840/ 3200]\n",
            "loss: 0.799340  [ 1856/ 3200]\n",
            "loss: 1.262234  [ 1872/ 3200]\n",
            "loss: 0.932076  [ 1888/ 3200]\n",
            "loss: 1.068562  [ 1904/ 3200]\n",
            "loss: 0.834447  [ 1920/ 3200]\n",
            "loss: 1.354448  [ 1936/ 3200]\n",
            "loss: 1.341882  [ 1952/ 3200]\n",
            "loss: 1.134941  [ 1968/ 3200]\n",
            "loss: 0.970046  [ 1984/ 3200]\n",
            "loss: 0.755574  [ 2000/ 3200]\n",
            "loss: 0.980097  [ 2016/ 3200]\n",
            "loss: 0.851390  [ 2032/ 3200]\n",
            "loss: 1.120509  [ 2048/ 3200]\n",
            "loss: 1.209885  [ 2064/ 3200]\n",
            "loss: 0.952815  [ 2080/ 3200]\n",
            "loss: 1.210478  [ 2096/ 3200]\n",
            "loss: 1.179674  [ 2112/ 3200]\n",
            "loss: 1.099795  [ 2128/ 3200]\n",
            "loss: 0.948599  [ 2144/ 3200]\n",
            "loss: 1.085699  [ 2160/ 3200]\n",
            "loss: 0.875237  [ 2176/ 3200]\n",
            "loss: 0.913520  [ 2192/ 3200]\n",
            "loss: 1.283302  [ 2208/ 3200]\n",
            "loss: 0.991625  [ 2224/ 3200]\n",
            "loss: 1.191819  [ 2240/ 3200]\n",
            "loss: 0.992873  [ 2256/ 3200]\n",
            "loss: 0.728280  [ 2272/ 3200]\n",
            "loss: 1.005617  [ 2288/ 3200]\n",
            "loss: 0.901186  [ 2304/ 3200]\n",
            "loss: 0.988840  [ 2320/ 3200]\n",
            "loss: 1.027162  [ 2336/ 3200]\n",
            "loss: 1.141946  [ 2352/ 3200]\n",
            "loss: 1.048132  [ 2368/ 3200]\n",
            "loss: 0.892926  [ 2384/ 3200]\n",
            "loss: 0.765211  [ 2400/ 3200]\n",
            "loss: 0.910789  [ 2416/ 3200]\n",
            "loss: 0.881286  [ 2432/ 3200]\n",
            "loss: 0.824980  [ 2448/ 3200]\n",
            "loss: 0.827022  [ 2464/ 3200]\n",
            "loss: 1.115134  [ 2480/ 3200]\n",
            "loss: 0.617996  [ 2496/ 3200]\n",
            "loss: 0.906864  [ 2512/ 3200]\n",
            "loss: 1.048015  [ 2528/ 3200]\n",
            "loss: 0.952600  [ 2544/ 3200]\n",
            "loss: 1.177835  [ 2560/ 3200]\n",
            "loss: 0.991547  [ 2576/ 3200]\n",
            "loss: 0.810763  [ 2592/ 3200]\n",
            "loss: 0.864365  [ 2608/ 3200]\n",
            "loss: 1.108737  [ 2624/ 3200]\n",
            "loss: 0.750381  [ 2640/ 3200]\n",
            "loss: 0.879299  [ 2656/ 3200]\n",
            "loss: 1.091534  [ 2672/ 3200]\n",
            "loss: 0.804454  [ 2688/ 3200]\n",
            "loss: 0.980006  [ 2704/ 3200]\n",
            "loss: 1.156170  [ 2720/ 3200]\n",
            "loss: 0.887813  [ 2736/ 3200]\n",
            "loss: 1.111508  [ 2752/ 3200]\n",
            "loss: 1.039223  [ 2768/ 3200]\n",
            "loss: 0.803112  [ 2784/ 3200]\n",
            "loss: 0.957451  [ 2800/ 3200]\n",
            "loss: 1.046216  [ 2816/ 3200]\n",
            "loss: 0.983608  [ 2832/ 3200]\n",
            "loss: 1.126546  [ 2848/ 3200]\n",
            "loss: 0.820304  [ 2864/ 3200]\n",
            "loss: 0.831142  [ 2880/ 3200]\n",
            "loss: 0.855416  [ 2896/ 3200]\n",
            "loss: 0.944373  [ 2912/ 3200]\n",
            "loss: 1.086330  [ 2928/ 3200]\n",
            "loss: 0.840051  [ 2944/ 3200]\n",
            "loss: 0.913981  [ 2960/ 3200]\n",
            "loss: 0.822996  [ 2976/ 3200]\n",
            "loss: 0.986389  [ 2992/ 3200]\n",
            "loss: 1.055880  [ 3008/ 3200]\n",
            "loss: 1.163760  [ 3024/ 3200]\n",
            "loss: 1.306725  [ 3040/ 3200]\n",
            "loss: 0.965264  [ 3056/ 3200]\n",
            "loss: 0.890704  [ 3072/ 3200]\n",
            "loss: 0.997841  [ 3088/ 3200]\n",
            "loss: 1.002697  [ 3104/ 3200]\n",
            "loss: 0.762867  [ 3120/ 3200]\n",
            "loss: 1.064159  [ 3136/ 3200]\n",
            "loss: 0.957815  [ 3152/ 3200]\n",
            "loss: 0.775650  [ 3168/ 3200]\n",
            "loss: 0.910256  [ 3184/ 3200]\n",
            "current epoch: 24\n",
            "\n",
            "loss: 1.014958  [    0/ 3200]\n",
            "loss: 0.890904  [   16/ 3200]\n",
            "loss: 0.928665  [   32/ 3200]\n",
            "loss: 1.064865  [   48/ 3200]\n",
            "loss: 1.193045  [   64/ 3200]\n",
            "loss: 0.848791  [   80/ 3200]\n",
            "loss: 0.990338  [   96/ 3200]\n",
            "loss: 0.926389  [  112/ 3200]\n",
            "loss: 0.874516  [  128/ 3200]\n",
            "loss: 1.035062  [  144/ 3200]\n",
            "loss: 1.138288  [  160/ 3200]\n",
            "loss: 0.648095  [  176/ 3200]\n",
            "loss: 1.177725  [  192/ 3200]\n",
            "loss: 0.894935  [  208/ 3200]\n",
            "loss: 0.762745  [  224/ 3200]\n",
            "loss: 0.837390  [  240/ 3200]\n",
            "loss: 1.062011  [  256/ 3200]\n",
            "loss: 0.861240  [  272/ 3200]\n",
            "loss: 0.901189  [  288/ 3200]\n",
            "loss: 0.790429  [  304/ 3200]\n",
            "loss: 1.223333  [  320/ 3200]\n",
            "loss: 0.893126  [  336/ 3200]\n",
            "loss: 0.937312  [  352/ 3200]\n",
            "loss: 0.921531  [  368/ 3200]\n",
            "loss: 1.231225  [  384/ 3200]\n",
            "loss: 0.844102  [  400/ 3200]\n",
            "loss: 0.883680  [  416/ 3200]\n",
            "loss: 0.883836  [  432/ 3200]\n",
            "loss: 0.846817  [  448/ 3200]\n",
            "loss: 1.021469  [  464/ 3200]\n",
            "loss: 0.991437  [  480/ 3200]\n",
            "loss: 0.864124  [  496/ 3200]\n",
            "loss: 1.127026  [  512/ 3200]\n",
            "loss: 0.930241  [  528/ 3200]\n",
            "loss: 0.946914  [  544/ 3200]\n",
            "loss: 1.154674  [  560/ 3200]\n",
            "loss: 0.977869  [  576/ 3200]\n",
            "loss: 0.859963  [  592/ 3200]\n",
            "loss: 0.923034  [  608/ 3200]\n",
            "loss: 0.786707  [  624/ 3200]\n",
            "loss: 0.967892  [  640/ 3200]\n",
            "loss: 0.820040  [  656/ 3200]\n",
            "loss: 0.605701  [  672/ 3200]\n",
            "loss: 0.859101  [  688/ 3200]\n",
            "loss: 0.970625  [  704/ 3200]\n",
            "loss: 1.297888  [  720/ 3200]\n",
            "loss: 0.888172  [  736/ 3200]\n",
            "loss: 0.899445  [  752/ 3200]\n",
            "loss: 0.896519  [  768/ 3200]\n",
            "loss: 0.955932  [  784/ 3200]\n",
            "loss: 1.057649  [  800/ 3200]\n",
            "loss: 1.132532  [  816/ 3200]\n",
            "loss: 0.785420  [  832/ 3200]\n",
            "loss: 1.121982  [  848/ 3200]\n",
            "loss: 0.730136  [  864/ 3200]\n",
            "loss: 0.660263  [  880/ 3200]\n",
            "loss: 0.886085  [  896/ 3200]\n",
            "loss: 1.024878  [  912/ 3200]\n",
            "loss: 1.196074  [  928/ 3200]\n",
            "loss: 0.975478  [  944/ 3200]\n",
            "loss: 0.829472  [  960/ 3200]\n",
            "loss: 1.060041  [  976/ 3200]\n",
            "loss: 1.174111  [  992/ 3200]\n",
            "loss: 0.688222  [ 1008/ 3200]\n",
            "loss: 1.418242  [ 1024/ 3200]\n",
            "loss: 1.033310  [ 1040/ 3200]\n",
            "loss: 1.087640  [ 1056/ 3200]\n",
            "loss: 0.808951  [ 1072/ 3200]\n",
            "loss: 0.996261  [ 1088/ 3200]\n",
            "loss: 0.897190  [ 1104/ 3200]\n",
            "loss: 0.957845  [ 1120/ 3200]\n",
            "loss: 0.988698  [ 1136/ 3200]\n",
            "loss: 1.048299  [ 1152/ 3200]\n",
            "loss: 1.129058  [ 1168/ 3200]\n",
            "loss: 1.177302  [ 1184/ 3200]\n",
            "loss: 0.929801  [ 1200/ 3200]\n",
            "loss: 1.201149  [ 1216/ 3200]\n",
            "loss: 0.952796  [ 1232/ 3200]\n",
            "loss: 0.710278  [ 1248/ 3200]\n",
            "loss: 1.182045  [ 1264/ 3200]\n",
            "loss: 0.891822  [ 1280/ 3200]\n",
            "loss: 1.164082  [ 1296/ 3200]\n",
            "loss: 0.816903  [ 1312/ 3200]\n",
            "loss: 1.456611  [ 1328/ 3200]\n",
            "loss: 0.749559  [ 1344/ 3200]\n",
            "loss: 1.020383  [ 1360/ 3200]\n",
            "loss: 0.750056  [ 1376/ 3200]\n",
            "loss: 0.884840  [ 1392/ 3200]\n",
            "loss: 0.836516  [ 1408/ 3200]\n",
            "loss: 0.876513  [ 1424/ 3200]\n",
            "loss: 0.842395  [ 1440/ 3200]\n",
            "loss: 0.863441  [ 1456/ 3200]\n",
            "loss: 0.948152  [ 1472/ 3200]\n",
            "loss: 0.944575  [ 1488/ 3200]\n",
            "loss: 0.819203  [ 1504/ 3200]\n",
            "loss: 1.024899  [ 1520/ 3200]\n",
            "loss: 0.957159  [ 1536/ 3200]\n",
            "loss: 0.727158  [ 1552/ 3200]\n",
            "loss: 0.905505  [ 1568/ 3200]\n",
            "loss: 1.058374  [ 1584/ 3200]\n",
            "loss: 0.978119  [ 1600/ 3200]\n",
            "loss: 0.930990  [ 1616/ 3200]\n",
            "loss: 0.747867  [ 1632/ 3200]\n",
            "loss: 0.755531  [ 1648/ 3200]\n",
            "loss: 0.840605  [ 1664/ 3200]\n",
            "loss: 1.033675  [ 1680/ 3200]\n",
            "loss: 0.915822  [ 1696/ 3200]\n",
            "loss: 0.956781  [ 1712/ 3200]\n",
            "loss: 1.390599  [ 1728/ 3200]\n",
            "loss: 1.331476  [ 1744/ 3200]\n",
            "loss: 1.078484  [ 1760/ 3200]\n",
            "loss: 1.061218  [ 1776/ 3200]\n",
            "loss: 0.855879  [ 1792/ 3200]\n",
            "loss: 0.963880  [ 1808/ 3200]\n",
            "loss: 0.833339  [ 1824/ 3200]\n",
            "loss: 1.006675  [ 1840/ 3200]\n",
            "loss: 1.283548  [ 1856/ 3200]\n",
            "loss: 0.880541  [ 1872/ 3200]\n",
            "loss: 0.964897  [ 1888/ 3200]\n",
            "loss: 0.946097  [ 1904/ 3200]\n",
            "loss: 0.691418  [ 1920/ 3200]\n",
            "loss: 1.041118  [ 1936/ 3200]\n",
            "loss: 0.984857  [ 1952/ 3200]\n",
            "loss: 1.079263  [ 1968/ 3200]\n",
            "loss: 0.891370  [ 1984/ 3200]\n",
            "loss: 0.869168  [ 2000/ 3200]\n",
            "loss: 1.006724  [ 2016/ 3200]\n",
            "loss: 1.074665  [ 2032/ 3200]\n",
            "loss: 0.926983  [ 2048/ 3200]\n",
            "loss: 1.098799  [ 2064/ 3200]\n",
            "loss: 0.703197  [ 2080/ 3200]\n",
            "loss: 0.983660  [ 2096/ 3200]\n",
            "loss: 1.327853  [ 2112/ 3200]\n",
            "loss: 1.091218  [ 2128/ 3200]\n",
            "loss: 1.150795  [ 2144/ 3200]\n",
            "loss: 0.955800  [ 2160/ 3200]\n",
            "loss: 1.224514  [ 2176/ 3200]\n",
            "loss: 0.976444  [ 2192/ 3200]\n",
            "loss: 0.874357  [ 2208/ 3200]\n",
            "loss: 1.041595  [ 2224/ 3200]\n",
            "loss: 0.705670  [ 2240/ 3200]\n",
            "loss: 1.553281  [ 2256/ 3200]\n",
            "loss: 0.859987  [ 2272/ 3200]\n",
            "loss: 0.977284  [ 2288/ 3200]\n",
            "loss: 1.209025  [ 2304/ 3200]\n",
            "loss: 0.922385  [ 2320/ 3200]\n",
            "loss: 0.791939  [ 2336/ 3200]\n",
            "loss: 1.028519  [ 2352/ 3200]\n",
            "loss: 1.018979  [ 2368/ 3200]\n",
            "loss: 0.994420  [ 2384/ 3200]\n",
            "loss: 0.783588  [ 2400/ 3200]\n",
            "loss: 0.920779  [ 2416/ 3200]\n",
            "loss: 0.727136  [ 2432/ 3200]\n",
            "loss: 0.801491  [ 2448/ 3200]\n",
            "loss: 1.225133  [ 2464/ 3200]\n",
            "loss: 0.919340  [ 2480/ 3200]\n",
            "loss: 0.812382  [ 2496/ 3200]\n",
            "loss: 0.877563  [ 2512/ 3200]\n",
            "loss: 1.098302  [ 2528/ 3200]\n",
            "loss: 0.847723  [ 2544/ 3200]\n",
            "loss: 0.800602  [ 2560/ 3200]\n",
            "loss: 1.121674  [ 2576/ 3200]\n",
            "loss: 0.989886  [ 2592/ 3200]\n",
            "loss: 0.902226  [ 2608/ 3200]\n",
            "loss: 0.983192  [ 2624/ 3200]\n",
            "loss: 0.761655  [ 2640/ 3200]\n",
            "loss: 0.672556  [ 2656/ 3200]\n",
            "loss: 0.807877  [ 2672/ 3200]\n",
            "loss: 0.930921  [ 2688/ 3200]\n",
            "loss: 1.159278  [ 2704/ 3200]\n",
            "loss: 0.903799  [ 2720/ 3200]\n",
            "loss: 1.053760  [ 2736/ 3200]\n",
            "loss: 0.852915  [ 2752/ 3200]\n",
            "loss: 1.043177  [ 2768/ 3200]\n",
            "loss: 0.928747  [ 2784/ 3200]\n",
            "loss: 1.138050  [ 2800/ 3200]\n",
            "loss: 1.317295  [ 2816/ 3200]\n",
            "loss: 0.935117  [ 2832/ 3200]\n",
            "loss: 0.915638  [ 2848/ 3200]\n",
            "loss: 0.933764  [ 2864/ 3200]\n",
            "loss: 0.829044  [ 2880/ 3200]\n",
            "loss: 0.874941  [ 2896/ 3200]\n",
            "loss: 1.141979  [ 2912/ 3200]\n",
            "loss: 1.149221  [ 2928/ 3200]\n",
            "loss: 1.032377  [ 2944/ 3200]\n",
            "loss: 0.938339  [ 2960/ 3200]\n",
            "loss: 0.793063  [ 2976/ 3200]\n",
            "loss: 0.888359  [ 2992/ 3200]\n",
            "loss: 0.761552  [ 3008/ 3200]\n",
            "loss: 0.991568  [ 3024/ 3200]\n",
            "loss: 0.998477  [ 3040/ 3200]\n",
            "loss: 1.196320  [ 3056/ 3200]\n",
            "loss: 0.914033  [ 3072/ 3200]\n",
            "loss: 0.894362  [ 3088/ 3200]\n",
            "loss: 1.058986  [ 3104/ 3200]\n",
            "loss: 1.219431  [ 3120/ 3200]\n",
            "loss: 0.973896  [ 3136/ 3200]\n",
            "loss: 1.119002  [ 3152/ 3200]\n",
            "loss: 0.667279  [ 3168/ 3200]\n",
            "loss: 0.954833  [ 3184/ 3200]\n",
            "current epoch: 25\n",
            "\n",
            "loss: 1.200072  [    0/ 3200]\n",
            "loss: 1.430479  [   16/ 3200]\n",
            "loss: 0.921692  [   32/ 3200]\n",
            "loss: 1.282186  [   48/ 3200]\n",
            "loss: 0.923759  [   64/ 3200]\n",
            "loss: 1.010352  [   80/ 3200]\n",
            "loss: 0.779907  [   96/ 3200]\n",
            "loss: 0.775864  [  112/ 3200]\n",
            "loss: 0.901943  [  128/ 3200]\n",
            "loss: 0.829129  [  144/ 3200]\n",
            "loss: 0.916308  [  160/ 3200]\n",
            "loss: 0.951559  [  176/ 3200]\n",
            "loss: 1.097984  [  192/ 3200]\n",
            "loss: 0.756833  [  208/ 3200]\n",
            "loss: 1.113178  [  224/ 3200]\n",
            "loss: 0.920721  [  240/ 3200]\n",
            "loss: 1.042999  [  256/ 3200]\n",
            "loss: 0.787319  [  272/ 3200]\n",
            "loss: 0.797130  [  288/ 3200]\n",
            "loss: 1.030174  [  304/ 3200]\n",
            "loss: 0.999571  [  320/ 3200]\n",
            "loss: 0.931578  [  336/ 3200]\n",
            "loss: 0.803514  [  352/ 3200]\n",
            "loss: 0.802468  [  368/ 3200]\n",
            "loss: 0.895796  [  384/ 3200]\n",
            "loss: 1.068298  [  400/ 3200]\n",
            "loss: 1.019063  [  416/ 3200]\n",
            "loss: 0.694548  [  432/ 3200]\n",
            "loss: 1.078434  [  448/ 3200]\n",
            "loss: 0.936746  [  464/ 3200]\n",
            "loss: 0.743915  [  480/ 3200]\n",
            "loss: 0.953765  [  496/ 3200]\n",
            "loss: 1.049511  [  512/ 3200]\n",
            "loss: 0.911302  [  528/ 3200]\n",
            "loss: 0.911093  [  544/ 3200]\n",
            "loss: 0.702359  [  560/ 3200]\n",
            "loss: 1.053274  [  576/ 3200]\n",
            "loss: 0.892956  [  592/ 3200]\n",
            "loss: 0.908746  [  608/ 3200]\n",
            "loss: 1.048290  [  624/ 3200]\n",
            "loss: 1.042847  [  640/ 3200]\n",
            "loss: 1.016164  [  656/ 3200]\n",
            "loss: 0.951289  [  672/ 3200]\n",
            "loss: 0.750441  [  688/ 3200]\n",
            "loss: 0.974658  [  704/ 3200]\n",
            "loss: 0.952109  [  720/ 3200]\n",
            "loss: 0.722128  [  736/ 3200]\n",
            "loss: 1.149476  [  752/ 3200]\n",
            "loss: 0.936905  [  768/ 3200]\n",
            "loss: 0.661326  [  784/ 3200]\n",
            "loss: 1.050297  [  800/ 3200]\n",
            "loss: 1.019503  [  816/ 3200]\n",
            "loss: 0.896523  [  832/ 3200]\n",
            "loss: 0.763146  [  848/ 3200]\n",
            "loss: 0.923930  [  864/ 3200]\n",
            "loss: 1.128163  [  880/ 3200]\n",
            "loss: 0.868903  [  896/ 3200]\n",
            "loss: 0.715690  [  912/ 3200]\n",
            "loss: 0.769491  [  928/ 3200]\n",
            "loss: 1.048586  [  944/ 3200]\n",
            "loss: 1.049925  [  960/ 3200]\n",
            "loss: 1.103662  [  976/ 3200]\n",
            "loss: 0.754906  [  992/ 3200]\n",
            "loss: 1.026284  [ 1008/ 3200]\n",
            "loss: 0.877963  [ 1024/ 3200]\n",
            "loss: 0.986671  [ 1040/ 3200]\n",
            "loss: 1.157575  [ 1056/ 3200]\n",
            "loss: 0.939355  [ 1072/ 3200]\n",
            "loss: 0.932636  [ 1088/ 3200]\n",
            "loss: 0.945657  [ 1104/ 3200]\n",
            "loss: 0.801712  [ 1120/ 3200]\n",
            "loss: 0.939903  [ 1136/ 3200]\n",
            "loss: 0.984077  [ 1152/ 3200]\n",
            "loss: 0.937006  [ 1168/ 3200]\n",
            "loss: 0.871985  [ 1184/ 3200]\n",
            "loss: 0.900733  [ 1200/ 3200]\n",
            "loss: 1.151004  [ 1216/ 3200]\n",
            "loss: 0.898719  [ 1232/ 3200]\n",
            "loss: 0.761459  [ 1248/ 3200]\n",
            "loss: 0.904076  [ 1264/ 3200]\n",
            "loss: 1.068783  [ 1280/ 3200]\n",
            "loss: 1.514567  [ 1296/ 3200]\n",
            "loss: 1.188256  [ 1312/ 3200]\n",
            "loss: 0.737384  [ 1328/ 3200]\n",
            "loss: 0.791961  [ 1344/ 3200]\n",
            "loss: 0.975754  [ 1360/ 3200]\n",
            "loss: 1.028629  [ 1376/ 3200]\n",
            "loss: 0.876295  [ 1392/ 3200]\n",
            "loss: 0.694821  [ 1408/ 3200]\n",
            "loss: 1.081855  [ 1424/ 3200]\n",
            "loss: 0.656685  [ 1440/ 3200]\n",
            "loss: 1.218460  [ 1456/ 3200]\n",
            "loss: 1.238065  [ 1472/ 3200]\n",
            "loss: 1.008604  [ 1488/ 3200]\n",
            "loss: 0.880482  [ 1504/ 3200]\n",
            "loss: 0.883883  [ 1520/ 3200]\n",
            "loss: 0.920158  [ 1536/ 3200]\n",
            "loss: 0.913977  [ 1552/ 3200]\n",
            "loss: 1.214305  [ 1568/ 3200]\n",
            "loss: 0.900060  [ 1584/ 3200]\n",
            "loss: 1.125873  [ 1600/ 3200]\n",
            "loss: 0.909660  [ 1616/ 3200]\n",
            "loss: 0.918034  [ 1632/ 3200]\n",
            "loss: 1.000303  [ 1648/ 3200]\n",
            "loss: 0.764171  [ 1664/ 3200]\n",
            "loss: 0.878332  [ 1680/ 3200]\n",
            "loss: 1.161099  [ 1696/ 3200]\n",
            "loss: 0.920193  [ 1712/ 3200]\n",
            "loss: 1.133234  [ 1728/ 3200]\n",
            "loss: 1.189758  [ 1744/ 3200]\n",
            "loss: 0.956047  [ 1760/ 3200]\n",
            "loss: 0.908060  [ 1776/ 3200]\n",
            "loss: 0.927712  [ 1792/ 3200]\n",
            "loss: 1.150400  [ 1808/ 3200]\n",
            "loss: 0.999127  [ 1824/ 3200]\n",
            "loss: 1.201799  [ 1840/ 3200]\n",
            "loss: 0.776980  [ 1856/ 3200]\n",
            "loss: 1.006457  [ 1872/ 3200]\n",
            "loss: 0.966901  [ 1888/ 3200]\n",
            "loss: 1.191531  [ 1904/ 3200]\n",
            "loss: 0.960710  [ 1920/ 3200]\n",
            "loss: 1.109020  [ 1936/ 3200]\n",
            "loss: 1.139431  [ 1952/ 3200]\n",
            "loss: 1.024428  [ 1968/ 3200]\n",
            "loss: 1.202456  [ 1984/ 3200]\n",
            "loss: 1.430423  [ 2000/ 3200]\n",
            "loss: 1.004040  [ 2016/ 3200]\n",
            "loss: 0.770572  [ 2032/ 3200]\n",
            "loss: 0.991113  [ 2048/ 3200]\n",
            "loss: 1.111227  [ 2064/ 3200]\n",
            "loss: 1.065597  [ 2080/ 3200]\n",
            "loss: 0.845835  [ 2096/ 3200]\n",
            "loss: 1.150161  [ 2112/ 3200]\n",
            "loss: 0.876289  [ 2128/ 3200]\n",
            "loss: 0.831535  [ 2144/ 3200]\n",
            "loss: 1.275062  [ 2160/ 3200]\n",
            "loss: 1.055321  [ 2176/ 3200]\n",
            "loss: 0.694061  [ 2192/ 3200]\n",
            "loss: 1.049226  [ 2208/ 3200]\n",
            "loss: 0.886868  [ 2224/ 3200]\n",
            "loss: 1.109393  [ 2240/ 3200]\n",
            "loss: 1.036134  [ 2256/ 3200]\n",
            "loss: 0.770737  [ 2272/ 3200]\n",
            "loss: 1.253181  [ 2288/ 3200]\n",
            "loss: 1.007808  [ 2304/ 3200]\n",
            "loss: 0.899246  [ 2320/ 3200]\n",
            "loss: 1.110419  [ 2336/ 3200]\n",
            "loss: 0.878190  [ 2352/ 3200]\n",
            "loss: 0.696276  [ 2368/ 3200]\n",
            "loss: 0.957942  [ 2384/ 3200]\n",
            "loss: 0.826065  [ 2400/ 3200]\n",
            "loss: 0.747782  [ 2416/ 3200]\n",
            "loss: 0.896602  [ 2432/ 3200]\n",
            "loss: 0.871670  [ 2448/ 3200]\n",
            "loss: 0.792760  [ 2464/ 3200]\n",
            "loss: 1.069246  [ 2480/ 3200]\n",
            "loss: 0.893065  [ 2496/ 3200]\n",
            "loss: 0.804235  [ 2512/ 3200]\n",
            "loss: 0.847134  [ 2528/ 3200]\n",
            "loss: 0.890527  [ 2544/ 3200]\n",
            "loss: 0.916710  [ 2560/ 3200]\n",
            "loss: 0.881988  [ 2576/ 3200]\n",
            "loss: 0.834463  [ 2592/ 3200]\n",
            "loss: 1.104209  [ 2608/ 3200]\n",
            "loss: 1.035006  [ 2624/ 3200]\n",
            "loss: 0.721402  [ 2640/ 3200]\n",
            "loss: 1.128459  [ 2656/ 3200]\n",
            "loss: 1.159656  [ 2672/ 3200]\n",
            "loss: 0.909759  [ 2688/ 3200]\n",
            "loss: 1.130353  [ 2704/ 3200]\n",
            "loss: 0.812248  [ 2720/ 3200]\n",
            "loss: 0.589839  [ 2736/ 3200]\n",
            "loss: 0.840038  [ 2752/ 3200]\n",
            "loss: 1.005401  [ 2768/ 3200]\n",
            "loss: 0.891588  [ 2784/ 3200]\n",
            "loss: 0.906712  [ 2800/ 3200]\n",
            "loss: 1.027561  [ 2816/ 3200]\n",
            "loss: 0.787327  [ 2832/ 3200]\n",
            "loss: 1.045742  [ 2848/ 3200]\n",
            "loss: 1.130225  [ 2864/ 3200]\n",
            "loss: 0.965931  [ 2880/ 3200]\n",
            "loss: 1.139700  [ 2896/ 3200]\n",
            "loss: 0.691006  [ 2912/ 3200]\n",
            "loss: 0.937433  [ 2928/ 3200]\n",
            "loss: 0.968675  [ 2944/ 3200]\n",
            "loss: 0.657354  [ 2960/ 3200]\n",
            "loss: 1.021676  [ 2976/ 3200]\n",
            "loss: 1.113901  [ 2992/ 3200]\n",
            "loss: 0.792446  [ 3008/ 3200]\n",
            "loss: 0.866079  [ 3024/ 3200]\n",
            "loss: 0.852266  [ 3040/ 3200]\n",
            "loss: 0.817335  [ 3056/ 3200]\n",
            "loss: 1.123705  [ 3072/ 3200]\n",
            "loss: 0.947287  [ 3088/ 3200]\n",
            "loss: 1.258301  [ 3104/ 3200]\n",
            "loss: 0.941630  [ 3120/ 3200]\n",
            "loss: 0.975452  [ 3136/ 3200]\n",
            "loss: 0.727259  [ 3152/ 3200]\n",
            "loss: 0.738280  [ 3168/ 3200]\n",
            "loss: 1.116338  [ 3184/ 3200]\n",
            "current epoch: 26\n",
            "\n",
            "loss: 0.767963  [    0/ 3200]\n",
            "loss: 1.084451  [   16/ 3200]\n",
            "loss: 1.273145  [   32/ 3200]\n",
            "loss: 0.792446  [   48/ 3200]\n",
            "loss: 1.106635  [   64/ 3200]\n",
            "loss: 0.927711  [   80/ 3200]\n",
            "loss: 0.895151  [   96/ 3200]\n",
            "loss: 0.873093  [  112/ 3200]\n",
            "loss: 0.762659  [  128/ 3200]\n",
            "loss: 0.755578  [  144/ 3200]\n",
            "loss: 0.857230  [  160/ 3200]\n",
            "loss: 0.936654  [  176/ 3200]\n",
            "loss: 1.120103  [  192/ 3200]\n",
            "loss: 0.707258  [  208/ 3200]\n",
            "loss: 0.740466  [  224/ 3200]\n",
            "loss: 0.772397  [  240/ 3200]\n",
            "loss: 1.187975  [  256/ 3200]\n",
            "loss: 1.162546  [  272/ 3200]\n",
            "loss: 0.993851  [  288/ 3200]\n",
            "loss: 0.829338  [  304/ 3200]\n",
            "loss: 1.023804  [  320/ 3200]\n",
            "loss: 1.160149  [  336/ 3200]\n",
            "loss: 1.120281  [  352/ 3200]\n",
            "loss: 1.249296  [  368/ 3200]\n",
            "loss: 0.881632  [  384/ 3200]\n",
            "loss: 0.821657  [  400/ 3200]\n",
            "loss: 1.011192  [  416/ 3200]\n",
            "loss: 1.066584  [  432/ 3200]\n",
            "loss: 0.970782  [  448/ 3200]\n",
            "loss: 0.677758  [  464/ 3200]\n",
            "loss: 1.175282  [  480/ 3200]\n",
            "loss: 0.925400  [  496/ 3200]\n",
            "loss: 0.951927  [  512/ 3200]\n",
            "loss: 0.807266  [  528/ 3200]\n",
            "loss: 0.988961  [  544/ 3200]\n",
            "loss: 1.263936  [  560/ 3200]\n",
            "loss: 1.109423  [  576/ 3200]\n",
            "loss: 0.990893  [  592/ 3200]\n",
            "loss: 1.346382  [  608/ 3200]\n",
            "loss: 1.222523  [  624/ 3200]\n",
            "loss: 0.977045  [  640/ 3200]\n",
            "loss: 0.689493  [  656/ 3200]\n",
            "loss: 1.076397  [  672/ 3200]\n",
            "loss: 0.708863  [  688/ 3200]\n",
            "loss: 0.931720  [  704/ 3200]\n",
            "loss: 1.224276  [  720/ 3200]\n",
            "loss: 0.929063  [  736/ 3200]\n",
            "loss: 1.326910  [  752/ 3200]\n",
            "loss: 1.002317  [  768/ 3200]\n",
            "loss: 1.421789  [  784/ 3200]\n",
            "loss: 1.086840  [  800/ 3200]\n",
            "loss: 1.000779  [  816/ 3200]\n",
            "loss: 0.940030  [  832/ 3200]\n",
            "loss: 0.971727  [  848/ 3200]\n",
            "loss: 0.875758  [  864/ 3200]\n",
            "loss: 1.046106  [  880/ 3200]\n",
            "loss: 0.738461  [  896/ 3200]\n",
            "loss: 0.782520  [  912/ 3200]\n",
            "loss: 0.693415  [  928/ 3200]\n",
            "loss: 0.795606  [  944/ 3200]\n",
            "loss: 0.972449  [  960/ 3200]\n",
            "loss: 0.950138  [  976/ 3200]\n",
            "loss: 0.930266  [  992/ 3200]\n",
            "loss: 1.116625  [ 1008/ 3200]\n",
            "loss: 0.850056  [ 1024/ 3200]\n",
            "loss: 1.050700  [ 1040/ 3200]\n",
            "loss: 0.898100  [ 1056/ 3200]\n",
            "loss: 0.959374  [ 1072/ 3200]\n",
            "loss: 1.061087  [ 1088/ 3200]\n",
            "loss: 0.727645  [ 1104/ 3200]\n",
            "loss: 0.685462  [ 1120/ 3200]\n",
            "loss: 1.021365  [ 1136/ 3200]\n",
            "loss: 1.371538  [ 1152/ 3200]\n",
            "loss: 0.865490  [ 1168/ 3200]\n",
            "loss: 1.259468  [ 1184/ 3200]\n",
            "loss: 0.781045  [ 1200/ 3200]\n",
            "loss: 0.889337  [ 1216/ 3200]\n",
            "loss: 0.820523  [ 1232/ 3200]\n",
            "loss: 0.967907  [ 1248/ 3200]\n",
            "loss: 1.101503  [ 1264/ 3200]\n",
            "loss: 0.909655  [ 1280/ 3200]\n",
            "loss: 0.838696  [ 1296/ 3200]\n",
            "loss: 1.145022  [ 1312/ 3200]\n",
            "loss: 0.795729  [ 1328/ 3200]\n",
            "loss: 1.033553  [ 1344/ 3200]\n",
            "loss: 1.041946  [ 1360/ 3200]\n",
            "loss: 0.849114  [ 1376/ 3200]\n",
            "loss: 1.297371  [ 1392/ 3200]\n",
            "loss: 0.785223  [ 1408/ 3200]\n",
            "loss: 1.240746  [ 1424/ 3200]\n",
            "loss: 0.873390  [ 1440/ 3200]\n",
            "loss: 1.026431  [ 1456/ 3200]\n",
            "loss: 0.846622  [ 1472/ 3200]\n",
            "loss: 0.801119  [ 1488/ 3200]\n",
            "loss: 0.917446  [ 1504/ 3200]\n",
            "loss: 0.873634  [ 1520/ 3200]\n",
            "loss: 0.634599  [ 1536/ 3200]\n",
            "loss: 0.963225  [ 1552/ 3200]\n",
            "loss: 1.057850  [ 1568/ 3200]\n",
            "loss: 0.891953  [ 1584/ 3200]\n",
            "loss: 0.993448  [ 1600/ 3200]\n",
            "loss: 1.069546  [ 1616/ 3200]\n",
            "loss: 1.177187  [ 1632/ 3200]\n",
            "loss: 1.200851  [ 1648/ 3200]\n",
            "loss: 1.130280  [ 1664/ 3200]\n",
            "loss: 0.963210  [ 1680/ 3200]\n",
            "loss: 0.785553  [ 1696/ 3200]\n",
            "loss: 0.939649  [ 1712/ 3200]\n",
            "loss: 0.939283  [ 1728/ 3200]\n",
            "loss: 0.806700  [ 1744/ 3200]\n",
            "loss: 0.969145  [ 1760/ 3200]\n",
            "loss: 0.843468  [ 1776/ 3200]\n",
            "loss: 0.875058  [ 1792/ 3200]\n",
            "loss: 0.817488  [ 1808/ 3200]\n",
            "loss: 0.914568  [ 1824/ 3200]\n",
            "loss: 1.329414  [ 1840/ 3200]\n",
            "loss: 1.056675  [ 1856/ 3200]\n",
            "loss: 0.950474  [ 1872/ 3200]\n",
            "loss: 1.122028  [ 1888/ 3200]\n",
            "loss: 1.164310  [ 1904/ 3200]\n",
            "loss: 0.907143  [ 1920/ 3200]\n",
            "loss: 0.700189  [ 1936/ 3200]\n",
            "loss: 1.086771  [ 1952/ 3200]\n",
            "loss: 0.840319  [ 1968/ 3200]\n",
            "loss: 0.778292  [ 1984/ 3200]\n",
            "loss: 0.867317  [ 2000/ 3200]\n",
            "loss: 0.955650  [ 2016/ 3200]\n",
            "loss: 0.843679  [ 2032/ 3200]\n",
            "loss: 0.793659  [ 2048/ 3200]\n",
            "loss: 0.890131  [ 2064/ 3200]\n",
            "loss: 1.008737  [ 2080/ 3200]\n",
            "loss: 0.776902  [ 2096/ 3200]\n",
            "loss: 1.247001  [ 2112/ 3200]\n",
            "loss: 0.846252  [ 2128/ 3200]\n",
            "loss: 1.166660  [ 2144/ 3200]\n",
            "loss: 0.824851  [ 2160/ 3200]\n",
            "loss: 0.916814  [ 2176/ 3200]\n",
            "loss: 0.863406  [ 2192/ 3200]\n",
            "loss: 1.080904  [ 2208/ 3200]\n",
            "loss: 0.992849  [ 2224/ 3200]\n",
            "loss: 0.918282  [ 2240/ 3200]\n",
            "loss: 1.005159  [ 2256/ 3200]\n",
            "loss: 1.015494  [ 2272/ 3200]\n",
            "loss: 0.996819  [ 2288/ 3200]\n",
            "loss: 1.070121  [ 2304/ 3200]\n",
            "loss: 0.936925  [ 2320/ 3200]\n",
            "loss: 0.866231  [ 2336/ 3200]\n",
            "loss: 0.787584  [ 2352/ 3200]\n",
            "loss: 0.667958  [ 2368/ 3200]\n",
            "loss: 0.869062  [ 2384/ 3200]\n",
            "loss: 1.305422  [ 2400/ 3200]\n",
            "loss: 1.065522  [ 2416/ 3200]\n",
            "loss: 0.700321  [ 2432/ 3200]\n",
            "loss: 0.968542  [ 2448/ 3200]\n",
            "loss: 0.819999  [ 2464/ 3200]\n",
            "loss: 0.959816  [ 2480/ 3200]\n",
            "loss: 0.667579  [ 2496/ 3200]\n",
            "loss: 1.255539  [ 2512/ 3200]\n",
            "loss: 0.891713  [ 2528/ 3200]\n",
            "loss: 0.905288  [ 2544/ 3200]\n",
            "loss: 1.009446  [ 2560/ 3200]\n",
            "loss: 0.992086  [ 2576/ 3200]\n",
            "loss: 0.972310  [ 2592/ 3200]\n",
            "loss: 0.724110  [ 2608/ 3200]\n",
            "loss: 0.854617  [ 2624/ 3200]\n",
            "loss: 0.858651  [ 2640/ 3200]\n",
            "loss: 1.021148  [ 2656/ 3200]\n",
            "loss: 0.563091  [ 2672/ 3200]\n",
            "loss: 0.948748  [ 2688/ 3200]\n",
            "loss: 0.858852  [ 2704/ 3200]\n",
            "loss: 1.112692  [ 2720/ 3200]\n",
            "loss: 0.935031  [ 2736/ 3200]\n",
            "loss: 0.968180  [ 2752/ 3200]\n",
            "loss: 1.548323  [ 2768/ 3200]\n",
            "loss: 0.723851  [ 2784/ 3200]\n",
            "loss: 1.075141  [ 2800/ 3200]\n",
            "loss: 0.758162  [ 2816/ 3200]\n",
            "loss: 1.046244  [ 2832/ 3200]\n",
            "loss: 0.741280  [ 2848/ 3200]\n",
            "loss: 0.773485  [ 2864/ 3200]\n",
            "loss: 0.896684  [ 2880/ 3200]\n",
            "loss: 0.653289  [ 2896/ 3200]\n",
            "loss: 1.026499  [ 2912/ 3200]\n",
            "loss: 0.844288  [ 2928/ 3200]\n",
            "loss: 0.971666  [ 2944/ 3200]\n",
            "loss: 0.782355  [ 2960/ 3200]\n",
            "loss: 0.794002  [ 2976/ 3200]\n",
            "loss: 0.881473  [ 2992/ 3200]\n",
            "loss: 1.046277  [ 3008/ 3200]\n",
            "loss: 1.156960  [ 3024/ 3200]\n",
            "loss: 0.767046  [ 3040/ 3200]\n",
            "loss: 1.023058  [ 3056/ 3200]\n",
            "loss: 1.047006  [ 3072/ 3200]\n",
            "loss: 0.934062  [ 3088/ 3200]\n",
            "loss: 1.098624  [ 3104/ 3200]\n",
            "loss: 0.778468  [ 3120/ 3200]\n",
            "loss: 1.111557  [ 3136/ 3200]\n",
            "loss: 1.119678  [ 3152/ 3200]\n",
            "loss: 0.898959  [ 3168/ 3200]\n",
            "loss: 1.003624  [ 3184/ 3200]\n",
            "current epoch: 27\n",
            "\n",
            "loss: 1.136936  [    0/ 3200]\n",
            "loss: 0.948987  [   16/ 3200]\n",
            "loss: 1.058812  [   32/ 3200]\n",
            "loss: 0.676808  [   48/ 3200]\n",
            "loss: 0.764753  [   64/ 3200]\n",
            "loss: 1.160600  [   80/ 3200]\n",
            "loss: 1.018966  [   96/ 3200]\n",
            "loss: 1.015787  [  112/ 3200]\n",
            "loss: 1.066923  [  128/ 3200]\n",
            "loss: 0.844764  [  144/ 3200]\n",
            "loss: 1.213483  [  160/ 3200]\n",
            "loss: 1.073861  [  176/ 3200]\n",
            "loss: 0.842354  [  192/ 3200]\n",
            "loss: 1.427891  [  208/ 3200]\n",
            "loss: 1.102320  [  224/ 3200]\n",
            "loss: 1.005136  [  240/ 3200]\n",
            "loss: 0.802063  [  256/ 3200]\n",
            "loss: 0.858387  [  272/ 3200]\n",
            "loss: 0.880911  [  288/ 3200]\n",
            "loss: 0.781526  [  304/ 3200]\n",
            "loss: 1.195695  [  320/ 3200]\n",
            "loss: 0.804778  [  336/ 3200]\n",
            "loss: 0.864207  [  352/ 3200]\n",
            "loss: 0.925245  [  368/ 3200]\n",
            "loss: 1.057232  [  384/ 3200]\n",
            "loss: 1.109872  [  400/ 3200]\n",
            "loss: 1.274798  [  416/ 3200]\n",
            "loss: 1.061082  [  432/ 3200]\n",
            "loss: 0.717283  [  448/ 3200]\n",
            "loss: 0.855025  [  464/ 3200]\n",
            "loss: 1.133965  [  480/ 3200]\n",
            "loss: 1.140235  [  496/ 3200]\n",
            "loss: 0.718591  [  512/ 3200]\n",
            "loss: 0.632641  [  528/ 3200]\n",
            "loss: 1.043342  [  544/ 3200]\n",
            "loss: 0.906699  [  560/ 3200]\n",
            "loss: 0.963918  [  576/ 3200]\n",
            "loss: 0.985194  [  592/ 3200]\n",
            "loss: 1.080932  [  608/ 3200]\n",
            "loss: 0.956124  [  624/ 3200]\n",
            "loss: 1.013254  [  640/ 3200]\n",
            "loss: 0.817768  [  656/ 3200]\n",
            "loss: 0.982850  [  672/ 3200]\n",
            "loss: 0.791173  [  688/ 3200]\n",
            "loss: 0.960877  [  704/ 3200]\n",
            "loss: 0.816714  [  720/ 3200]\n",
            "loss: 1.072262  [  736/ 3200]\n",
            "loss: 0.811261  [  752/ 3200]\n",
            "loss: 0.983024  [  768/ 3200]\n",
            "loss: 0.857737  [  784/ 3200]\n",
            "loss: 0.702906  [  800/ 3200]\n",
            "loss: 1.095971  [  816/ 3200]\n",
            "loss: 0.956919  [  832/ 3200]\n",
            "loss: 0.612218  [  848/ 3200]\n",
            "loss: 1.142157  [  864/ 3200]\n",
            "loss: 0.786598  [  880/ 3200]\n",
            "loss: 1.071297  [  896/ 3200]\n",
            "loss: 0.996745  [  912/ 3200]\n",
            "loss: 0.614241  [  928/ 3200]\n",
            "loss: 1.007828  [  944/ 3200]\n",
            "loss: 0.854083  [  960/ 3200]\n",
            "loss: 0.869938  [  976/ 3200]\n",
            "loss: 0.758221  [  992/ 3200]\n",
            "loss: 1.087162  [ 1008/ 3200]\n",
            "loss: 0.774306  [ 1024/ 3200]\n",
            "loss: 0.756122  [ 1040/ 3200]\n",
            "loss: 1.116997  [ 1056/ 3200]\n",
            "loss: 1.187070  [ 1072/ 3200]\n",
            "loss: 1.080247  [ 1088/ 3200]\n",
            "loss: 0.898266  [ 1104/ 3200]\n",
            "loss: 0.687975  [ 1120/ 3200]\n",
            "loss: 0.883568  [ 1136/ 3200]\n",
            "loss: 0.795869  [ 1152/ 3200]\n",
            "loss: 0.982805  [ 1168/ 3200]\n",
            "loss: 0.867597  [ 1184/ 3200]\n",
            "loss: 0.671305  [ 1200/ 3200]\n",
            "loss: 0.971778  [ 1216/ 3200]\n",
            "loss: 1.099588  [ 1232/ 3200]\n",
            "loss: 0.966592  [ 1248/ 3200]\n",
            "loss: 0.941954  [ 1264/ 3200]\n",
            "loss: 0.952449  [ 1280/ 3200]\n",
            "loss: 1.198804  [ 1296/ 3200]\n",
            "loss: 1.203520  [ 1312/ 3200]\n",
            "loss: 1.098382  [ 1328/ 3200]\n",
            "loss: 0.776165  [ 1344/ 3200]\n",
            "loss: 0.722960  [ 1360/ 3200]\n",
            "loss: 0.834605  [ 1376/ 3200]\n",
            "loss: 0.712182  [ 1392/ 3200]\n",
            "loss: 1.052934  [ 1408/ 3200]\n",
            "loss: 0.834841  [ 1424/ 3200]\n",
            "loss: 0.926763  [ 1440/ 3200]\n",
            "loss: 0.894292  [ 1456/ 3200]\n",
            "loss: 1.081254  [ 1472/ 3200]\n",
            "loss: 0.759165  [ 1488/ 3200]\n",
            "loss: 1.163370  [ 1504/ 3200]\n",
            "loss: 1.015775  [ 1520/ 3200]\n",
            "loss: 1.006996  [ 1536/ 3200]\n",
            "loss: 0.997183  [ 1552/ 3200]\n",
            "loss: 0.823566  [ 1568/ 3200]\n",
            "loss: 1.049592  [ 1584/ 3200]\n",
            "loss: 0.812716  [ 1600/ 3200]\n",
            "loss: 0.912130  [ 1616/ 3200]\n",
            "loss: 1.053670  [ 1632/ 3200]\n",
            "loss: 0.845600  [ 1648/ 3200]\n",
            "loss: 0.863724  [ 1664/ 3200]\n",
            "loss: 0.924502  [ 1680/ 3200]\n",
            "loss: 0.881689  [ 1696/ 3200]\n",
            "loss: 1.117552  [ 1712/ 3200]\n",
            "loss: 0.672436  [ 1728/ 3200]\n",
            "loss: 1.003873  [ 1744/ 3200]\n",
            "loss: 0.913770  [ 1760/ 3200]\n",
            "loss: 1.378437  [ 1776/ 3200]\n",
            "loss: 0.820475  [ 1792/ 3200]\n",
            "loss: 0.976461  [ 1808/ 3200]\n",
            "loss: 0.981153  [ 1824/ 3200]\n",
            "loss: 0.974502  [ 1840/ 3200]\n",
            "loss: 0.965999  [ 1856/ 3200]\n",
            "loss: 0.674853  [ 1872/ 3200]\n",
            "loss: 1.130394  [ 1888/ 3200]\n",
            "loss: 0.717846  [ 1904/ 3200]\n",
            "loss: 0.878291  [ 1920/ 3200]\n",
            "loss: 0.946587  [ 1936/ 3200]\n",
            "loss: 1.193040  [ 1952/ 3200]\n",
            "loss: 1.234545  [ 1968/ 3200]\n",
            "loss: 0.830362  [ 1984/ 3200]\n",
            "loss: 1.370957  [ 2000/ 3200]\n",
            "loss: 1.057416  [ 2016/ 3200]\n",
            "loss: 0.996215  [ 2032/ 3200]\n",
            "loss: 0.977287  [ 2048/ 3200]\n",
            "loss: 0.936397  [ 2064/ 3200]\n",
            "loss: 1.005597  [ 2080/ 3200]\n",
            "loss: 0.662876  [ 2096/ 3200]\n",
            "loss: 0.781216  [ 2112/ 3200]\n",
            "loss: 0.820517  [ 2128/ 3200]\n",
            "loss: 1.103079  [ 2144/ 3200]\n",
            "loss: 0.740082  [ 2160/ 3200]\n",
            "loss: 1.197152  [ 2176/ 3200]\n",
            "loss: 0.749119  [ 2192/ 3200]\n",
            "loss: 1.134459  [ 2208/ 3200]\n",
            "loss: 0.873422  [ 2224/ 3200]\n",
            "loss: 1.137270  [ 2240/ 3200]\n",
            "loss: 0.722328  [ 2256/ 3200]\n",
            "loss: 1.310231  [ 2272/ 3200]\n",
            "loss: 0.790924  [ 2288/ 3200]\n",
            "loss: 0.849126  [ 2304/ 3200]\n",
            "loss: 0.857547  [ 2320/ 3200]\n",
            "loss: 0.898857  [ 2336/ 3200]\n",
            "loss: 1.255155  [ 2352/ 3200]\n",
            "loss: 1.141369  [ 2368/ 3200]\n",
            "loss: 0.963084  [ 2384/ 3200]\n",
            "loss: 1.031399  [ 2400/ 3200]\n",
            "loss: 0.889789  [ 2416/ 3200]\n",
            "loss: 0.696930  [ 2432/ 3200]\n",
            "loss: 0.767257  [ 2448/ 3200]\n",
            "loss: 1.148106  [ 2464/ 3200]\n",
            "loss: 0.706229  [ 2480/ 3200]\n",
            "loss: 1.207142  [ 2496/ 3200]\n",
            "loss: 1.143024  [ 2512/ 3200]\n",
            "loss: 0.886181  [ 2528/ 3200]\n",
            "loss: 0.880310  [ 2544/ 3200]\n",
            "loss: 0.913663  [ 2560/ 3200]\n",
            "loss: 0.894900  [ 2576/ 3200]\n",
            "loss: 1.120746  [ 2592/ 3200]\n",
            "loss: 0.910451  [ 2608/ 3200]\n",
            "loss: 1.134026  [ 2624/ 3200]\n",
            "loss: 1.150374  [ 2640/ 3200]\n",
            "loss: 1.059896  [ 2656/ 3200]\n",
            "loss: 0.866710  [ 2672/ 3200]\n",
            "loss: 1.097801  [ 2688/ 3200]\n",
            "loss: 0.912978  [ 2704/ 3200]\n",
            "loss: 0.763391  [ 2720/ 3200]\n",
            "loss: 0.967737  [ 2736/ 3200]\n",
            "loss: 0.765699  [ 2752/ 3200]\n",
            "loss: 0.912046  [ 2768/ 3200]\n",
            "loss: 1.147340  [ 2784/ 3200]\n",
            "loss: 0.817785  [ 2800/ 3200]\n",
            "loss: 1.107434  [ 2816/ 3200]\n",
            "loss: 0.775095  [ 2832/ 3200]\n",
            "loss: 1.049076  [ 2848/ 3200]\n",
            "loss: 0.907978  [ 2864/ 3200]\n",
            "loss: 0.800707  [ 2880/ 3200]\n",
            "loss: 0.852434  [ 2896/ 3200]\n",
            "loss: 0.967854  [ 2912/ 3200]\n",
            "loss: 1.238254  [ 2928/ 3200]\n",
            "loss: 0.960923  [ 2944/ 3200]\n",
            "loss: 0.619260  [ 2960/ 3200]\n",
            "loss: 0.942350  [ 2976/ 3200]\n",
            "loss: 0.693988  [ 2992/ 3200]\n",
            "loss: 0.835190  [ 3008/ 3200]\n",
            "loss: 0.811884  [ 3024/ 3200]\n",
            "loss: 0.848248  [ 3040/ 3200]\n",
            "loss: 1.633581  [ 3056/ 3200]\n",
            "loss: 1.117468  [ 3072/ 3200]\n",
            "loss: 0.796365  [ 3088/ 3200]\n",
            "loss: 1.000241  [ 3104/ 3200]\n",
            "loss: 0.815933  [ 3120/ 3200]\n",
            "loss: 0.838670  [ 3136/ 3200]\n",
            "loss: 0.923780  [ 3152/ 3200]\n",
            "loss: 0.901025  [ 3168/ 3200]\n",
            "loss: 1.171510  [ 3184/ 3200]\n",
            "current epoch: 28\n",
            "\n",
            "loss: 0.932890  [    0/ 3200]\n",
            "loss: 1.056993  [   16/ 3200]\n",
            "loss: 0.989732  [   32/ 3200]\n",
            "loss: 0.848615  [   48/ 3200]\n",
            "loss: 0.934221  [   64/ 3200]\n",
            "loss: 0.997606  [   80/ 3200]\n",
            "loss: 0.945089  [   96/ 3200]\n",
            "loss: 1.049122  [  112/ 3200]\n",
            "loss: 1.230013  [  128/ 3200]\n",
            "loss: 1.051483  [  144/ 3200]\n",
            "loss: 0.670143  [  160/ 3200]\n",
            "loss: 1.082718  [  176/ 3200]\n",
            "loss: 0.892549  [  192/ 3200]\n",
            "loss: 0.974103  [  208/ 3200]\n",
            "loss: 1.081345  [  224/ 3200]\n",
            "loss: 0.653727  [  240/ 3200]\n",
            "loss: 0.787703  [  256/ 3200]\n",
            "loss: 1.023225  [  272/ 3200]\n",
            "loss: 0.835209  [  288/ 3200]\n",
            "loss: 1.090554  [  304/ 3200]\n",
            "loss: 1.063259  [  320/ 3200]\n",
            "loss: 0.946158  [  336/ 3200]\n",
            "loss: 0.716451  [  352/ 3200]\n",
            "loss: 1.341003  [  368/ 3200]\n",
            "loss: 0.871557  [  384/ 3200]\n",
            "loss: 0.707337  [  400/ 3200]\n",
            "loss: 0.979344  [  416/ 3200]\n",
            "loss: 0.980096  [  432/ 3200]\n",
            "loss: 0.613465  [  448/ 3200]\n",
            "loss: 0.887233  [  464/ 3200]\n",
            "loss: 1.278977  [  480/ 3200]\n",
            "loss: 0.870004  [  496/ 3200]\n",
            "loss: 0.886824  [  512/ 3200]\n",
            "loss: 0.842378  [  528/ 3200]\n",
            "loss: 0.920359  [  544/ 3200]\n",
            "loss: 0.572964  [  560/ 3200]\n",
            "loss: 1.018147  [  576/ 3200]\n",
            "loss: 0.767990  [  592/ 3200]\n",
            "loss: 0.945467  [  608/ 3200]\n",
            "loss: 0.920591  [  624/ 3200]\n",
            "loss: 1.360034  [  640/ 3200]\n",
            "loss: 1.103216  [  656/ 3200]\n",
            "loss: 1.264360  [  672/ 3200]\n",
            "loss: 0.780274  [  688/ 3200]\n",
            "loss: 1.005992  [  704/ 3200]\n",
            "loss: 0.674623  [  720/ 3200]\n",
            "loss: 0.877241  [  736/ 3200]\n",
            "loss: 0.740396  [  752/ 3200]\n",
            "loss: 1.083102  [  768/ 3200]\n",
            "loss: 0.867638  [  784/ 3200]\n",
            "loss: 1.186522  [  800/ 3200]\n",
            "loss: 1.051466  [  816/ 3200]\n",
            "loss: 0.933522  [  832/ 3200]\n",
            "loss: 0.871119  [  848/ 3200]\n",
            "loss: 1.008738  [  864/ 3200]\n",
            "loss: 0.911433  [  880/ 3200]\n",
            "loss: 1.005650  [  896/ 3200]\n",
            "loss: 0.906880  [  912/ 3200]\n",
            "loss: 0.920914  [  928/ 3200]\n",
            "loss: 0.877343  [  944/ 3200]\n",
            "loss: 0.997611  [  960/ 3200]\n",
            "loss: 1.222698  [  976/ 3200]\n",
            "loss: 0.831845  [  992/ 3200]\n",
            "loss: 1.052082  [ 1008/ 3200]\n",
            "loss: 0.736514  [ 1024/ 3200]\n",
            "loss: 1.088721  [ 1040/ 3200]\n",
            "loss: 0.838912  [ 1056/ 3200]\n",
            "loss: 0.958140  [ 1072/ 3200]\n",
            "loss: 0.813862  [ 1088/ 3200]\n",
            "loss: 0.796402  [ 1104/ 3200]\n",
            "loss: 0.904278  [ 1120/ 3200]\n",
            "loss: 0.667252  [ 1136/ 3200]\n",
            "loss: 0.974991  [ 1152/ 3200]\n",
            "loss: 0.863310  [ 1168/ 3200]\n",
            "loss: 1.085874  [ 1184/ 3200]\n",
            "loss: 0.847085  [ 1200/ 3200]\n",
            "loss: 0.930821  [ 1216/ 3200]\n",
            "loss: 1.082095  [ 1232/ 3200]\n",
            "loss: 0.859981  [ 1248/ 3200]\n",
            "loss: 0.825735  [ 1264/ 3200]\n",
            "loss: 0.970475  [ 1280/ 3200]\n",
            "loss: 1.081603  [ 1296/ 3200]\n",
            "loss: 1.107331  [ 1312/ 3200]\n",
            "loss: 1.177819  [ 1328/ 3200]\n",
            "loss: 1.013486  [ 1344/ 3200]\n",
            "loss: 0.849328  [ 1360/ 3200]\n",
            "loss: 1.334765  [ 1376/ 3200]\n",
            "loss: 0.700908  [ 1392/ 3200]\n",
            "loss: 0.955045  [ 1408/ 3200]\n",
            "loss: 0.827076  [ 1424/ 3200]\n",
            "loss: 0.781829  [ 1440/ 3200]\n",
            "loss: 0.915201  [ 1456/ 3200]\n",
            "loss: 0.918438  [ 1472/ 3200]\n",
            "loss: 0.747887  [ 1488/ 3200]\n",
            "loss: 0.927348  [ 1504/ 3200]\n",
            "loss: 0.790239  [ 1520/ 3200]\n",
            "loss: 0.638620  [ 1536/ 3200]\n",
            "loss: 0.616447  [ 1552/ 3200]\n",
            "loss: 1.109120  [ 1568/ 3200]\n",
            "loss: 1.080845  [ 1584/ 3200]\n",
            "loss: 1.119017  [ 1600/ 3200]\n",
            "loss: 1.120745  [ 1616/ 3200]\n",
            "loss: 0.863374  [ 1632/ 3200]\n",
            "loss: 0.755451  [ 1648/ 3200]\n",
            "loss: 0.827521  [ 1664/ 3200]\n",
            "loss: 0.790332  [ 1680/ 3200]\n",
            "loss: 0.897690  [ 1696/ 3200]\n",
            "loss: 0.822665  [ 1712/ 3200]\n",
            "loss: 0.923601  [ 1728/ 3200]\n",
            "loss: 1.202303  [ 1744/ 3200]\n",
            "loss: 0.881496  [ 1760/ 3200]\n",
            "loss: 0.726887  [ 1776/ 3200]\n",
            "loss: 1.100336  [ 1792/ 3200]\n",
            "loss: 1.135996  [ 1808/ 3200]\n",
            "loss: 0.935258  [ 1824/ 3200]\n",
            "loss: 1.023320  [ 1840/ 3200]\n",
            "loss: 1.045371  [ 1856/ 3200]\n",
            "loss: 0.916698  [ 1872/ 3200]\n",
            "loss: 0.678096  [ 1888/ 3200]\n",
            "loss: 0.815056  [ 1904/ 3200]\n",
            "loss: 1.050643  [ 1920/ 3200]\n",
            "loss: 1.040158  [ 1936/ 3200]\n",
            "loss: 1.334838  [ 1952/ 3200]\n",
            "loss: 0.974663  [ 1968/ 3200]\n",
            "loss: 1.006101  [ 1984/ 3200]\n",
            "loss: 0.689001  [ 2000/ 3200]\n",
            "loss: 0.940981  [ 2016/ 3200]\n",
            "loss: 0.890915  [ 2032/ 3200]\n",
            "loss: 1.050845  [ 2048/ 3200]\n",
            "loss: 1.135443  [ 2064/ 3200]\n",
            "loss: 0.771085  [ 2080/ 3200]\n",
            "loss: 1.086715  [ 2096/ 3200]\n",
            "loss: 1.059058  [ 2112/ 3200]\n",
            "loss: 0.908354  [ 2128/ 3200]\n",
            "loss: 1.133875  [ 2144/ 3200]\n",
            "loss: 0.934948  [ 2160/ 3200]\n",
            "loss: 0.966668  [ 2176/ 3200]\n",
            "loss: 0.701061  [ 2192/ 3200]\n",
            "loss: 0.992521  [ 2208/ 3200]\n",
            "loss: 0.663355  [ 2224/ 3200]\n",
            "loss: 0.645662  [ 2240/ 3200]\n",
            "loss: 1.062345  [ 2256/ 3200]\n",
            "loss: 1.502852  [ 2272/ 3200]\n",
            "loss: 0.950639  [ 2288/ 3200]\n",
            "loss: 1.200624  [ 2304/ 3200]\n",
            "loss: 0.847159  [ 2320/ 3200]\n",
            "loss: 1.119735  [ 2336/ 3200]\n",
            "loss: 1.319404  [ 2352/ 3200]\n",
            "loss: 0.900538  [ 2368/ 3200]\n",
            "loss: 1.006472  [ 2384/ 3200]\n",
            "loss: 0.780071  [ 2400/ 3200]\n",
            "loss: 0.669208  [ 2416/ 3200]\n",
            "loss: 0.707123  [ 2432/ 3200]\n",
            "loss: 0.817991  [ 2448/ 3200]\n",
            "loss: 1.167314  [ 2464/ 3200]\n",
            "loss: 0.934590  [ 2480/ 3200]\n",
            "loss: 1.276521  [ 2496/ 3200]\n",
            "loss: 1.225595  [ 2512/ 3200]\n",
            "loss: 0.852202  [ 2528/ 3200]\n",
            "loss: 0.831934  [ 2544/ 3200]\n",
            "loss: 1.010522  [ 2560/ 3200]\n",
            "loss: 0.599567  [ 2576/ 3200]\n",
            "loss: 0.767863  [ 2592/ 3200]\n",
            "loss: 0.791347  [ 2608/ 3200]\n",
            "loss: 0.786891  [ 2624/ 3200]\n",
            "loss: 1.021982  [ 2640/ 3200]\n",
            "loss: 0.896793  [ 2656/ 3200]\n",
            "loss: 0.779373  [ 2672/ 3200]\n",
            "loss: 0.925114  [ 2688/ 3200]\n",
            "loss: 0.910670  [ 2704/ 3200]\n",
            "loss: 1.074260  [ 2720/ 3200]\n",
            "loss: 0.866801  [ 2736/ 3200]\n",
            "loss: 0.978497  [ 2752/ 3200]\n",
            "loss: 1.115404  [ 2768/ 3200]\n",
            "loss: 1.434300  [ 2784/ 3200]\n",
            "loss: 0.794208  [ 2800/ 3200]\n",
            "loss: 1.040238  [ 2816/ 3200]\n",
            "loss: 0.701530  [ 2832/ 3200]\n",
            "loss: 0.889006  [ 2848/ 3200]\n",
            "loss: 0.942192  [ 2864/ 3200]\n",
            "loss: 1.100785  [ 2880/ 3200]\n",
            "loss: 0.828177  [ 2896/ 3200]\n",
            "loss: 0.718328  [ 2912/ 3200]\n",
            "loss: 0.985981  [ 2928/ 3200]\n",
            "loss: 1.309462  [ 2944/ 3200]\n",
            "loss: 0.883234  [ 2960/ 3200]\n",
            "loss: 0.818824  [ 2976/ 3200]\n",
            "loss: 0.791721  [ 2992/ 3200]\n",
            "loss: 0.614313  [ 3008/ 3200]\n",
            "loss: 0.946903  [ 3024/ 3200]\n",
            "loss: 0.828316  [ 3040/ 3200]\n",
            "loss: 1.291147  [ 3056/ 3200]\n",
            "loss: 0.951814  [ 3072/ 3200]\n",
            "loss: 0.936187  [ 3088/ 3200]\n",
            "loss: 1.274312  [ 3104/ 3200]\n",
            "loss: 0.919335  [ 3120/ 3200]\n",
            "loss: 0.768025  [ 3136/ 3200]\n",
            "loss: 0.750571  [ 3152/ 3200]\n",
            "loss: 0.964459  [ 3168/ 3200]\n",
            "loss: 1.159649  [ 3184/ 3200]\n",
            "current epoch: 29\n",
            "\n",
            "loss: 1.047668  [    0/ 3200]\n",
            "loss: 0.883299  [   16/ 3200]\n",
            "loss: 0.774101  [   32/ 3200]\n",
            "loss: 0.680350  [   48/ 3200]\n",
            "loss: 0.808852  [   64/ 3200]\n",
            "loss: 1.038099  [   80/ 3200]\n",
            "loss: 1.047089  [   96/ 3200]\n",
            "loss: 1.055965  [  112/ 3200]\n",
            "loss: 1.089155  [  128/ 3200]\n",
            "loss: 0.799694  [  144/ 3200]\n",
            "loss: 0.935330  [  160/ 3200]\n",
            "loss: 0.682745  [  176/ 3200]\n",
            "loss: 1.216723  [  192/ 3200]\n",
            "loss: 0.803050  [  208/ 3200]\n",
            "loss: 1.016305  [  224/ 3200]\n",
            "loss: 1.230434  [  240/ 3200]\n",
            "loss: 0.614482  [  256/ 3200]\n",
            "loss: 0.835300  [  272/ 3200]\n",
            "loss: 0.933145  [  288/ 3200]\n",
            "loss: 0.667041  [  304/ 3200]\n",
            "loss: 0.755129  [  320/ 3200]\n",
            "loss: 0.685344  [  336/ 3200]\n",
            "loss: 0.995092  [  352/ 3200]\n",
            "loss: 0.694387  [  368/ 3200]\n",
            "loss: 1.090689  [  384/ 3200]\n",
            "loss: 0.899284  [  400/ 3200]\n",
            "loss: 0.987394  [  416/ 3200]\n",
            "loss: 0.767036  [  432/ 3200]\n",
            "loss: 1.009102  [  448/ 3200]\n",
            "loss: 0.925871  [  464/ 3200]\n",
            "loss: 0.892022  [  480/ 3200]\n",
            "loss: 1.260343  [  496/ 3200]\n",
            "loss: 0.928842  [  512/ 3200]\n",
            "loss: 0.734369  [  528/ 3200]\n",
            "loss: 0.893998  [  544/ 3200]\n",
            "loss: 0.922280  [  560/ 3200]\n",
            "loss: 0.823623  [  576/ 3200]\n",
            "loss: 0.629719  [  592/ 3200]\n",
            "loss: 0.920119  [  608/ 3200]\n",
            "loss: 0.771661  [  624/ 3200]\n",
            "loss: 0.949313  [  640/ 3200]\n",
            "loss: 0.860130  [  656/ 3200]\n",
            "loss: 1.274029  [  672/ 3200]\n",
            "loss: 0.747364  [  688/ 3200]\n",
            "loss: 0.922207  [  704/ 3200]\n",
            "loss: 0.896942  [  720/ 3200]\n",
            "loss: 1.003844  [  736/ 3200]\n",
            "loss: 0.925681  [  752/ 3200]\n",
            "loss: 1.006522  [  768/ 3200]\n",
            "loss: 1.080795  [  784/ 3200]\n",
            "loss: 1.106154  [  800/ 3200]\n",
            "loss: 0.946561  [  816/ 3200]\n",
            "loss: 1.149299  [  832/ 3200]\n",
            "loss: 0.856452  [  848/ 3200]\n",
            "loss: 0.812455  [  864/ 3200]\n",
            "loss: 0.808160  [  880/ 3200]\n",
            "loss: 1.120472  [  896/ 3200]\n",
            "loss: 0.799167  [  912/ 3200]\n",
            "loss: 1.110693  [  928/ 3200]\n",
            "loss: 1.044394  [  944/ 3200]\n",
            "loss: 0.899219  [  960/ 3200]\n",
            "loss: 0.779741  [  976/ 3200]\n",
            "loss: 0.972862  [  992/ 3200]\n",
            "loss: 0.943060  [ 1008/ 3200]\n",
            "loss: 0.864431  [ 1024/ 3200]\n",
            "loss: 1.035803  [ 1040/ 3200]\n",
            "loss: 1.061250  [ 1056/ 3200]\n",
            "loss: 0.716169  [ 1072/ 3200]\n",
            "loss: 0.739576  [ 1088/ 3200]\n",
            "loss: 0.748561  [ 1104/ 3200]\n",
            "loss: 1.027031  [ 1120/ 3200]\n",
            "loss: 0.903028  [ 1136/ 3200]\n",
            "loss: 0.859776  [ 1152/ 3200]\n",
            "loss: 1.334018  [ 1168/ 3200]\n",
            "loss: 0.957042  [ 1184/ 3200]\n",
            "loss: 0.800661  [ 1200/ 3200]\n",
            "loss: 1.136414  [ 1216/ 3200]\n",
            "loss: 1.108300  [ 1232/ 3200]\n",
            "loss: 0.698288  [ 1248/ 3200]\n",
            "loss: 0.921585  [ 1264/ 3200]\n",
            "loss: 0.933258  [ 1280/ 3200]\n",
            "loss: 1.201043  [ 1296/ 3200]\n",
            "loss: 1.038708  [ 1312/ 3200]\n",
            "loss: 0.830442  [ 1328/ 3200]\n",
            "loss: 1.006669  [ 1344/ 3200]\n",
            "loss: 1.338280  [ 1360/ 3200]\n",
            "loss: 0.697018  [ 1376/ 3200]\n",
            "loss: 0.874807  [ 1392/ 3200]\n",
            "loss: 0.755037  [ 1408/ 3200]\n",
            "loss: 0.870620  [ 1424/ 3200]\n",
            "loss: 1.280565  [ 1440/ 3200]\n",
            "loss: 0.973291  [ 1456/ 3200]\n",
            "loss: 0.919890  [ 1472/ 3200]\n",
            "loss: 0.827069  [ 1488/ 3200]\n",
            "loss: 0.817989  [ 1504/ 3200]\n",
            "loss: 0.725113  [ 1520/ 3200]\n",
            "loss: 0.891966  [ 1536/ 3200]\n",
            "loss: 0.897945  [ 1552/ 3200]\n",
            "loss: 1.432654  [ 1568/ 3200]\n",
            "loss: 1.384356  [ 1584/ 3200]\n",
            "loss: 0.933339  [ 1600/ 3200]\n",
            "loss: 1.412425  [ 1616/ 3200]\n",
            "loss: 0.874793  [ 1632/ 3200]\n",
            "loss: 1.379898  [ 1648/ 3200]\n",
            "loss: 0.641398  [ 1664/ 3200]\n",
            "loss: 1.709371  [ 1680/ 3200]\n",
            "loss: 0.792521  [ 1696/ 3200]\n",
            "loss: 0.931355  [ 1712/ 3200]\n",
            "loss: 1.135838  [ 1728/ 3200]\n",
            "loss: 0.788419  [ 1744/ 3200]\n",
            "loss: 1.090159  [ 1760/ 3200]\n",
            "loss: 0.932550  [ 1776/ 3200]\n",
            "loss: 0.789334  [ 1792/ 3200]\n",
            "loss: 0.957730  [ 1808/ 3200]\n",
            "loss: 1.071745  [ 1824/ 3200]\n",
            "loss: 0.680193  [ 1840/ 3200]\n",
            "loss: 0.856087  [ 1856/ 3200]\n",
            "loss: 0.881781  [ 1872/ 3200]\n",
            "loss: 1.116394  [ 1888/ 3200]\n",
            "loss: 0.886635  [ 1904/ 3200]\n",
            "loss: 1.152141  [ 1920/ 3200]\n",
            "loss: 0.879162  [ 1936/ 3200]\n",
            "loss: 0.952149  [ 1952/ 3200]\n",
            "loss: 1.064625  [ 1968/ 3200]\n",
            "loss: 1.008445  [ 1984/ 3200]\n",
            "loss: 0.836469  [ 2000/ 3200]\n",
            "loss: 1.160409  [ 2016/ 3200]\n",
            "loss: 1.259042  [ 2032/ 3200]\n",
            "loss: 0.884649  [ 2048/ 3200]\n",
            "loss: 1.031921  [ 2064/ 3200]\n",
            "loss: 0.742420  [ 2080/ 3200]\n",
            "loss: 0.929853  [ 2096/ 3200]\n",
            "loss: 0.930041  [ 2112/ 3200]\n",
            "loss: 0.951457  [ 2128/ 3200]\n",
            "loss: 0.990291  [ 2144/ 3200]\n",
            "loss: 0.956151  [ 2160/ 3200]\n",
            "loss: 0.777862  [ 2176/ 3200]\n",
            "loss: 0.857683  [ 2192/ 3200]\n",
            "loss: 0.867912  [ 2208/ 3200]\n",
            "loss: 1.008612  [ 2224/ 3200]\n",
            "loss: 0.922267  [ 2240/ 3200]\n",
            "loss: 0.952775  [ 2256/ 3200]\n",
            "loss: 1.036237  [ 2272/ 3200]\n",
            "loss: 0.917588  [ 2288/ 3200]\n",
            "loss: 0.840363  [ 2304/ 3200]\n",
            "loss: 0.670496  [ 2320/ 3200]\n",
            "loss: 0.793673  [ 2336/ 3200]\n",
            "loss: 0.940516  [ 2352/ 3200]\n",
            "loss: 0.913381  [ 2368/ 3200]\n",
            "loss: 1.109696  [ 2384/ 3200]\n",
            "loss: 0.648067  [ 2400/ 3200]\n",
            "loss: 1.161188  [ 2416/ 3200]\n",
            "loss: 0.592923  [ 2432/ 3200]\n",
            "loss: 0.956072  [ 2448/ 3200]\n",
            "loss: 1.286158  [ 2464/ 3200]\n",
            "loss: 0.755285  [ 2480/ 3200]\n",
            "loss: 0.908476  [ 2496/ 3200]\n",
            "loss: 0.928193  [ 2512/ 3200]\n",
            "loss: 0.575231  [ 2528/ 3200]\n",
            "loss: 0.653922  [ 2544/ 3200]\n",
            "loss: 0.864083  [ 2560/ 3200]\n",
            "loss: 0.702396  [ 2576/ 3200]\n",
            "loss: 0.967452  [ 2592/ 3200]\n",
            "loss: 0.880646  [ 2608/ 3200]\n",
            "loss: 1.047936  [ 2624/ 3200]\n",
            "loss: 0.852598  [ 2640/ 3200]\n",
            "loss: 0.807031  [ 2656/ 3200]\n",
            "loss: 0.701936  [ 2672/ 3200]\n",
            "loss: 1.014338  [ 2688/ 3200]\n",
            "loss: 1.051383  [ 2704/ 3200]\n",
            "loss: 0.792016  [ 2720/ 3200]\n",
            "loss: 1.151178  [ 2736/ 3200]\n",
            "loss: 1.171247  [ 2752/ 3200]\n",
            "loss: 0.941217  [ 2768/ 3200]\n",
            "loss: 1.400309  [ 2784/ 3200]\n",
            "loss: 1.195732  [ 2800/ 3200]\n",
            "loss: 1.201586  [ 2816/ 3200]\n",
            "loss: 0.783979  [ 2832/ 3200]\n",
            "loss: 1.031421  [ 2848/ 3200]\n",
            "loss: 0.903441  [ 2864/ 3200]\n",
            "loss: 0.807828  [ 2880/ 3200]\n",
            "loss: 1.047943  [ 2896/ 3200]\n",
            "loss: 0.881383  [ 2912/ 3200]\n",
            "loss: 0.899336  [ 2928/ 3200]\n",
            "loss: 0.962218  [ 2944/ 3200]\n",
            "loss: 0.821695  [ 2960/ 3200]\n",
            "loss: 1.025808  [ 2976/ 3200]\n",
            "loss: 0.755386  [ 2992/ 3200]\n",
            "loss: 0.960996  [ 3008/ 3200]\n",
            "loss: 0.909167  [ 3024/ 3200]\n",
            "loss: 1.035707  [ 3040/ 3200]\n",
            "loss: 0.780681  [ 3056/ 3200]\n",
            "loss: 0.994540  [ 3072/ 3200]\n",
            "loss: 0.592957  [ 3088/ 3200]\n",
            "loss: 0.800386  [ 3104/ 3200]\n",
            "loss: 0.794179  [ 3120/ 3200]\n",
            "loss: 0.839834  [ 3136/ 3200]\n",
            "loss: 0.875008  [ 3152/ 3200]\n",
            "loss: 0.813607  [ 3168/ 3200]\n",
            "loss: 0.954793  [ 3184/ 3200]\n",
            "\n",
            "Total Time for Training in CPU: 13.109955549240112\n",
            "Avg Accuracy: 57.000000%, Avg loss: 0.953134\n",
            "F1 score is: 0.5163120359182358\n",
            "Confusion Matrix:\n",
            "[[113  17  67   3]\n",
            " [ 20  38 118  24]\n",
            " [  1   2 192   5]\n",
            " [  1  21  65 113]]\n"
          ]
        }
      ],
      "source": [
        "device = \"cpu\"\n",
        "model = NeuralNetwork().to(device)\n",
        "cost_func = nn.CrossEntropyLoss()\n",
        "learning_rate = 0.002\n",
        "num_epochs = 30\n",
        "batch_size = 16\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
        "start_time = time.time()\n",
        "model = train(num_epochs, optimizer, train_dataloader,cost_func,model,device)\n",
        "print(f\"\\nTotal Time for Training in CPU: {time.time() - start_time}\")\n",
        "test_loss, test_f1, test_acc, confmatrix = evaluate(val_dataloader, cost_func, model,device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6 Comparing Processing Time against GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eq6sUf55dj1i",
        "outputId": "513ceb42-bc1c-4b14-a931-2c88f4082dd8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "loss: 1.245351  [  912/ 3200]\n",
            "loss: 1.275699  [  928/ 3200]\n",
            "loss: 1.349842  [  944/ 3200]\n",
            "loss: 1.329913  [  960/ 3200]\n",
            "loss: 1.250380  [  976/ 3200]\n",
            "loss: 1.356080  [  992/ 3200]\n",
            "loss: 1.286289  [ 1008/ 3200]\n",
            "loss: 1.282448  [ 1024/ 3200]\n",
            "loss: 1.284261  [ 1040/ 3200]\n",
            "loss: 1.312157  [ 1056/ 3200]\n",
            "loss: 1.331797  [ 1072/ 3200]\n",
            "loss: 1.268752  [ 1088/ 3200]\n",
            "loss: 1.269939  [ 1104/ 3200]\n",
            "loss: 1.294107  [ 1120/ 3200]\n",
            "loss: 1.265244  [ 1136/ 3200]\n",
            "loss: 1.345123  [ 1152/ 3200]\n",
            "loss: 1.325344  [ 1168/ 3200]\n",
            "loss: 1.253829  [ 1184/ 3200]\n",
            "loss: 1.369187  [ 1200/ 3200]\n",
            "loss: 1.307675  [ 1216/ 3200]\n",
            "loss: 1.316044  [ 1232/ 3200]\n",
            "loss: 1.218068  [ 1248/ 3200]\n",
            "loss: 1.319752  [ 1264/ 3200]\n",
            "loss: 1.273467  [ 1280/ 3200]\n",
            "loss: 1.235621  [ 1296/ 3200]\n",
            "loss: 1.336444  [ 1312/ 3200]\n",
            "loss: 1.321035  [ 1328/ 3200]\n",
            "loss: 1.244218  [ 1344/ 3200]\n",
            "loss: 1.269391  [ 1360/ 3200]\n",
            "loss: 1.231372  [ 1376/ 3200]\n",
            "loss: 1.293603  [ 1392/ 3200]\n",
            "loss: 1.291709  [ 1408/ 3200]\n",
            "loss: 1.277950  [ 1424/ 3200]\n",
            "loss: 1.241016  [ 1440/ 3200]\n",
            "loss: 1.316302  [ 1456/ 3200]\n",
            "loss: 1.325363  [ 1472/ 3200]\n",
            "loss: 1.288674  [ 1488/ 3200]\n",
            "loss: 1.239421  [ 1504/ 3200]\n",
            "loss: 1.271952  [ 1520/ 3200]\n",
            "loss: 1.247047  [ 1536/ 3200]\n",
            "loss: 1.277447  [ 1552/ 3200]\n",
            "loss: 1.224869  [ 1568/ 3200]\n",
            "loss: 1.230970  [ 1584/ 3200]\n",
            "loss: 1.210600  [ 1600/ 3200]\n",
            "loss: 1.278300  [ 1616/ 3200]\n",
            "loss: 1.258109  [ 1632/ 3200]\n",
            "loss: 1.175939  [ 1648/ 3200]\n",
            "loss: 1.318002  [ 1664/ 3200]\n",
            "loss: 1.246092  [ 1680/ 3200]\n",
            "loss: 1.425101  [ 1696/ 3200]\n",
            "loss: 1.226662  [ 1712/ 3200]\n",
            "loss: 1.226007  [ 1728/ 3200]\n",
            "loss: 1.271810  [ 1744/ 3200]\n",
            "loss: 1.223596  [ 1760/ 3200]\n",
            "loss: 1.265361  [ 1776/ 3200]\n",
            "loss: 1.252713  [ 1792/ 3200]\n",
            "loss: 1.297463  [ 1808/ 3200]\n",
            "loss: 1.367989  [ 1824/ 3200]\n",
            "loss: 1.301667  [ 1840/ 3200]\n",
            "loss: 1.246863  [ 1856/ 3200]\n",
            "loss: 1.286597  [ 1872/ 3200]\n",
            "loss: 1.255253  [ 1888/ 3200]\n",
            "loss: 1.242101  [ 1904/ 3200]\n",
            "loss: 1.215842  [ 1920/ 3200]\n",
            "loss: 1.237561  [ 1936/ 3200]\n",
            "loss: 1.289590  [ 1952/ 3200]\n",
            "loss: 1.324517  [ 1968/ 3200]\n",
            "loss: 1.270745  [ 1984/ 3200]\n",
            "loss: 1.306445  [ 2000/ 3200]\n",
            "loss: 1.286410  [ 2016/ 3200]\n",
            "loss: 1.251688  [ 2032/ 3200]\n",
            "loss: 1.173208  [ 2048/ 3200]\n",
            "loss: 1.246837  [ 2064/ 3200]\n",
            "loss: 1.332057  [ 2080/ 3200]\n",
            "loss: 1.353912  [ 2096/ 3200]\n",
            "loss: 1.253485  [ 2112/ 3200]\n",
            "loss: 1.316588  [ 2128/ 3200]\n",
            "loss: 1.352051  [ 2144/ 3200]\n",
            "loss: 1.294743  [ 2160/ 3200]\n",
            "loss: 1.236121  [ 2176/ 3200]\n",
            "loss: 1.275285  [ 2192/ 3200]\n",
            "loss: 1.294729  [ 2208/ 3200]\n",
            "loss: 1.282009  [ 2224/ 3200]\n",
            "loss: 1.368042  [ 2240/ 3200]\n",
            "loss: 1.282613  [ 2256/ 3200]\n",
            "loss: 1.249123  [ 2272/ 3200]\n",
            "loss: 1.311569  [ 2288/ 3200]\n",
            "loss: 1.235740  [ 2304/ 3200]\n",
            "loss: 1.215726  [ 2320/ 3200]\n",
            "loss: 1.290689  [ 2336/ 3200]\n",
            "loss: 1.260479  [ 2352/ 3200]\n",
            "loss: 1.240439  [ 2368/ 3200]\n",
            "loss: 1.267159  [ 2384/ 3200]\n",
            "loss: 1.377260  [ 2400/ 3200]\n",
            "loss: 1.343977  [ 2416/ 3200]\n",
            "loss: 1.338043  [ 2432/ 3200]\n",
            "loss: 1.260126  [ 2448/ 3200]\n",
            "loss: 1.270515  [ 2464/ 3200]\n",
            "loss: 1.207642  [ 2480/ 3200]\n",
            "loss: 1.208502  [ 2496/ 3200]\n",
            "loss: 1.318276  [ 2512/ 3200]\n",
            "loss: 1.224110  [ 2528/ 3200]\n",
            "loss: 1.165699  [ 2544/ 3200]\n",
            "loss: 1.207045  [ 2560/ 3200]\n",
            "loss: 1.415915  [ 2576/ 3200]\n",
            "loss: 1.395290  [ 2592/ 3200]\n",
            "loss: 1.247441  [ 2608/ 3200]\n",
            "loss: 1.303195  [ 2624/ 3200]\n",
            "loss: 1.272728  [ 2640/ 3200]\n",
            "loss: 1.224082  [ 2656/ 3200]\n",
            "loss: 1.240488  [ 2672/ 3200]\n",
            "loss: 1.223503  [ 2688/ 3200]\n",
            "loss: 1.288375  [ 2704/ 3200]\n",
            "loss: 1.263050  [ 2720/ 3200]\n",
            "loss: 1.288627  [ 2736/ 3200]\n",
            "loss: 1.288740  [ 2752/ 3200]\n",
            "loss: 1.258245  [ 2768/ 3200]\n",
            "loss: 1.172603  [ 2784/ 3200]\n",
            "loss: 1.222216  [ 2800/ 3200]\n",
            "loss: 1.347278  [ 2816/ 3200]\n",
            "loss: 1.262908  [ 2832/ 3200]\n",
            "loss: 1.227261  [ 2848/ 3200]\n",
            "loss: 1.202727  [ 2864/ 3200]\n",
            "loss: 1.312522  [ 2880/ 3200]\n",
            "loss: 1.265835  [ 2896/ 3200]\n",
            "loss: 1.259972  [ 2912/ 3200]\n",
            "loss: 1.171116  [ 2928/ 3200]\n",
            "loss: 1.222513  [ 2944/ 3200]\n",
            "loss: 1.285493  [ 2960/ 3200]\n",
            "loss: 1.331246  [ 2976/ 3200]\n",
            "loss: 1.216135  [ 2992/ 3200]\n",
            "loss: 1.267229  [ 3008/ 3200]\n",
            "loss: 1.276604  [ 3024/ 3200]\n",
            "loss: 1.302498  [ 3040/ 3200]\n",
            "loss: 1.328023  [ 3056/ 3200]\n",
            "loss: 1.157304  [ 3072/ 3200]\n",
            "loss: 1.275463  [ 3088/ 3200]\n",
            "loss: 1.201190  [ 3104/ 3200]\n",
            "loss: 1.294678  [ 3120/ 3200]\n",
            "loss: 1.315685  [ 3136/ 3200]\n",
            "loss: 1.326086  [ 3152/ 3200]\n",
            "loss: 1.290127  [ 3168/ 3200]\n",
            "loss: 1.317452  [ 3184/ 3200]\n",
            "current epoch: 6\n",
            "\n",
            "loss: 1.249344  [    0/ 3200]\n",
            "loss: 1.243037  [   16/ 3200]\n",
            "loss: 1.227439  [   32/ 3200]\n",
            "loss: 1.266423  [   48/ 3200]\n",
            "loss: 1.160313  [   64/ 3200]\n",
            "loss: 1.301116  [   80/ 3200]\n",
            "loss: 1.248313  [   96/ 3200]\n",
            "loss: 1.245563  [  112/ 3200]\n",
            "loss: 1.338401  [  128/ 3200]\n",
            "loss: 1.325788  [  144/ 3200]\n",
            "loss: 1.196653  [  160/ 3200]\n",
            "loss: 1.275229  [  176/ 3200]\n",
            "loss: 1.274814  [  192/ 3200]\n",
            "loss: 1.262401  [  208/ 3200]\n",
            "loss: 1.259418  [  224/ 3200]\n",
            "loss: 1.280133  [  240/ 3200]\n",
            "loss: 1.270175  [  256/ 3200]\n",
            "loss: 1.331365  [  272/ 3200]\n",
            "loss: 1.262010  [  288/ 3200]\n",
            "loss: 1.274793  [  304/ 3200]\n",
            "loss: 1.296429  [  320/ 3200]\n",
            "loss: 1.332468  [  336/ 3200]\n",
            "loss: 1.284274  [  352/ 3200]\n",
            "loss: 1.183463  [  368/ 3200]\n",
            "loss: 1.258156  [  384/ 3200]\n",
            "loss: 1.216852  [  400/ 3200]\n",
            "loss: 1.246647  [  416/ 3200]\n",
            "loss: 1.198552  [  432/ 3200]\n",
            "loss: 1.247769  [  448/ 3200]\n",
            "loss: 1.408850  [  464/ 3200]\n",
            "loss: 1.208156  [  480/ 3200]\n",
            "loss: 1.285872  [  496/ 3200]\n",
            "loss: 1.334999  [  512/ 3200]\n",
            "loss: 1.219135  [  528/ 3200]\n",
            "loss: 1.243025  [  544/ 3200]\n",
            "loss: 1.239850  [  560/ 3200]\n",
            "loss: 1.174684  [  576/ 3200]\n",
            "loss: 1.195515  [  592/ 3200]\n",
            "loss: 1.237514  [  608/ 3200]\n",
            "loss: 1.161093  [  624/ 3200]\n",
            "loss: 1.217067  [  640/ 3200]\n",
            "loss: 1.235343  [  656/ 3200]\n",
            "loss: 1.294775  [  672/ 3200]\n",
            "loss: 1.144570  [  688/ 3200]\n",
            "loss: 1.290085  [  704/ 3200]\n",
            "loss: 1.312509  [  720/ 3200]\n",
            "loss: 1.151954  [  736/ 3200]\n",
            "loss: 1.249361  [  752/ 3200]\n",
            "loss: 1.340574  [  768/ 3200]\n",
            "loss: 1.279522  [  784/ 3200]\n",
            "loss: 1.289459  [  800/ 3200]\n",
            "loss: 1.204555  [  816/ 3200]\n",
            "loss: 1.338462  [  832/ 3200]\n",
            "loss: 1.254544  [  848/ 3200]\n",
            "loss: 1.332867  [  864/ 3200]\n",
            "loss: 1.236739  [  880/ 3200]\n",
            "loss: 1.263455  [  896/ 3200]\n",
            "loss: 1.278962  [  912/ 3200]\n",
            "loss: 1.245786  [  928/ 3200]\n",
            "loss: 1.250229  [  944/ 3200]\n",
            "loss: 1.269869  [  960/ 3200]\n",
            "loss: 1.296112  [  976/ 3200]\n",
            "loss: 1.266498  [  992/ 3200]\n",
            "loss: 1.167614  [ 1008/ 3200]\n",
            "loss: 1.230058  [ 1024/ 3200]\n",
            "loss: 1.316602  [ 1040/ 3200]\n",
            "loss: 1.329333  [ 1056/ 3200]\n",
            "loss: 1.129380  [ 1072/ 3200]\n",
            "loss: 1.180804  [ 1088/ 3200]\n",
            "loss: 1.221288  [ 1104/ 3200]\n",
            "loss: 1.352790  [ 1120/ 3200]\n",
            "loss: 1.151721  [ 1136/ 3200]\n",
            "loss: 1.355406  [ 1152/ 3200]\n",
            "loss: 1.182943  [ 1168/ 3200]\n",
            "loss: 1.247186  [ 1184/ 3200]\n",
            "loss: 1.392603  [ 1200/ 3200]\n",
            "loss: 1.232914  [ 1216/ 3200]\n",
            "loss: 1.272299  [ 1232/ 3200]\n",
            "loss: 1.342660  [ 1248/ 3200]\n",
            "loss: 1.349337  [ 1264/ 3200]\n",
            "loss: 1.280150  [ 1280/ 3200]\n",
            "loss: 1.297547  [ 1296/ 3200]\n",
            "loss: 1.232465  [ 1312/ 3200]\n",
            "loss: 1.278258  [ 1328/ 3200]\n",
            "loss: 1.279310  [ 1344/ 3200]\n",
            "loss: 1.226696  [ 1360/ 3200]\n",
            "loss: 1.358829  [ 1376/ 3200]\n",
            "loss: 1.241434  [ 1392/ 3200]\n",
            "loss: 1.265080  [ 1408/ 3200]\n",
            "loss: 1.237107  [ 1424/ 3200]\n",
            "loss: 1.282177  [ 1440/ 3200]\n",
            "loss: 1.300709  [ 1456/ 3200]\n",
            "loss: 1.203765  [ 1472/ 3200]\n",
            "loss: 1.382261  [ 1488/ 3200]\n",
            "loss: 1.209072  [ 1504/ 3200]\n",
            "loss: 1.219434  [ 1520/ 3200]\n",
            "loss: 1.242177  [ 1536/ 3200]\n",
            "loss: 1.292260  [ 1552/ 3200]\n",
            "loss: 1.228162  [ 1568/ 3200]\n",
            "loss: 1.174796  [ 1584/ 3200]\n",
            "loss: 1.201692  [ 1600/ 3200]\n",
            "loss: 1.238374  [ 1616/ 3200]\n",
            "loss: 1.273770  [ 1632/ 3200]\n",
            "loss: 1.173992  [ 1648/ 3200]\n",
            "loss: 1.295842  [ 1664/ 3200]\n",
            "loss: 1.235716  [ 1680/ 3200]\n",
            "loss: 1.239004  [ 1696/ 3200]\n",
            "loss: 1.216143  [ 1712/ 3200]\n",
            "loss: 1.221961  [ 1728/ 3200]\n",
            "loss: 1.209209  [ 1744/ 3200]\n",
            "loss: 1.297155  [ 1760/ 3200]\n",
            "loss: 1.192860  [ 1776/ 3200]\n",
            "loss: 1.260507  [ 1792/ 3200]\n",
            "loss: 1.351749  [ 1808/ 3200]\n",
            "loss: 1.296996  [ 1824/ 3200]\n",
            "loss: 1.234626  [ 1840/ 3200]\n",
            "loss: 1.238183  [ 1856/ 3200]\n",
            "loss: 1.275021  [ 1872/ 3200]\n",
            "loss: 1.280823  [ 1888/ 3200]\n",
            "loss: 1.257623  [ 1904/ 3200]\n",
            "loss: 1.201449  [ 1920/ 3200]\n",
            "loss: 1.276269  [ 1936/ 3200]\n",
            "loss: 1.280947  [ 1952/ 3200]\n",
            "loss: 1.205771  [ 1968/ 3200]\n",
            "loss: 1.252585  [ 1984/ 3200]\n",
            "loss: 1.290749  [ 2000/ 3200]\n",
            "loss: 1.304108  [ 2016/ 3200]\n",
            "loss: 1.270157  [ 2032/ 3200]\n",
            "loss: 1.296638  [ 2048/ 3200]\n",
            "loss: 1.228093  [ 2064/ 3200]\n",
            "loss: 1.324880  [ 2080/ 3200]\n",
            "loss: 1.258033  [ 2096/ 3200]\n",
            "loss: 1.243930  [ 2112/ 3200]\n",
            "loss: 1.226605  [ 2128/ 3200]\n",
            "loss: 1.304058  [ 2144/ 3200]\n",
            "loss: 1.139791  [ 2160/ 3200]\n",
            "loss: 1.276504  [ 2176/ 3200]\n",
            "loss: 1.257987  [ 2192/ 3200]\n",
            "loss: 1.248413  [ 2208/ 3200]\n",
            "loss: 1.328412  [ 2224/ 3200]\n",
            "loss: 1.235087  [ 2240/ 3200]\n",
            "loss: 1.311287  [ 2256/ 3200]\n",
            "loss: 1.192049  [ 2272/ 3200]\n",
            "loss: 1.221346  [ 2288/ 3200]\n",
            "loss: 1.263890  [ 2304/ 3200]\n",
            "loss: 1.159375  [ 2320/ 3200]\n",
            "loss: 1.321097  [ 2336/ 3200]\n",
            "loss: 1.265856  [ 2352/ 3200]\n",
            "loss: 1.232661  [ 2368/ 3200]\n",
            "loss: 1.170601  [ 2384/ 3200]\n",
            "loss: 1.235435  [ 2400/ 3200]\n",
            "loss: 1.189574  [ 2416/ 3200]\n",
            "loss: 1.262497  [ 2432/ 3200]\n",
            "loss: 1.273874  [ 2448/ 3200]\n",
            "loss: 1.230170  [ 2464/ 3200]\n",
            "loss: 1.247485  [ 2480/ 3200]\n",
            "loss: 1.198161  [ 2496/ 3200]\n",
            "loss: 1.173727  [ 2512/ 3200]\n",
            "loss: 1.341658  [ 2528/ 3200]\n",
            "loss: 1.267724  [ 2544/ 3200]\n",
            "loss: 1.164674  [ 2560/ 3200]\n",
            "loss: 1.296030  [ 2576/ 3200]\n",
            "loss: 1.252852  [ 2592/ 3200]\n",
            "loss: 1.202443  [ 2608/ 3200]\n",
            "loss: 1.247871  [ 2624/ 3200]\n",
            "loss: 1.319467  [ 2640/ 3200]\n",
            "loss: 1.176425  [ 2656/ 3200]\n",
            "loss: 1.364181  [ 2672/ 3200]\n",
            "loss: 1.217690  [ 2688/ 3200]\n",
            "loss: 1.206896  [ 2704/ 3200]\n",
            "loss: 1.180548  [ 2720/ 3200]\n",
            "loss: 1.292992  [ 2736/ 3200]\n",
            "loss: 1.146314  [ 2752/ 3200]\n",
            "loss: 1.224596  [ 2768/ 3200]\n",
            "loss: 1.342943  [ 2784/ 3200]\n",
            "loss: 1.237754  [ 2800/ 3200]\n",
            "loss: 1.281342  [ 2816/ 3200]\n",
            "loss: 1.185801  [ 2832/ 3200]\n",
            "loss: 1.294634  [ 2848/ 3200]\n",
            "loss: 1.352225  [ 2864/ 3200]\n",
            "loss: 1.183968  [ 2880/ 3200]\n",
            "loss: 1.344715  [ 2896/ 3200]\n",
            "loss: 1.244060  [ 2912/ 3200]\n",
            "loss: 1.328050  [ 2928/ 3200]\n",
            "loss: 1.197177  [ 2944/ 3200]\n",
            "loss: 1.225928  [ 2960/ 3200]\n",
            "loss: 1.205100  [ 2976/ 3200]\n",
            "loss: 1.352730  [ 2992/ 3200]\n",
            "loss: 1.243565  [ 3008/ 3200]\n",
            "loss: 1.214818  [ 3024/ 3200]\n",
            "loss: 1.260869  [ 3040/ 3200]\n",
            "loss: 1.220968  [ 3056/ 3200]\n",
            "loss: 1.307592  [ 3072/ 3200]\n",
            "loss: 1.242373  [ 3088/ 3200]\n",
            "loss: 1.312583  [ 3104/ 3200]\n",
            "loss: 1.251770  [ 3120/ 3200]\n",
            "loss: 1.171472  [ 3136/ 3200]\n",
            "loss: 1.319680  [ 3152/ 3200]\n",
            "loss: 1.232057  [ 3168/ 3200]\n",
            "loss: 1.255186  [ 3184/ 3200]\n",
            "current epoch: 7\n",
            "\n",
            "loss: 1.192965  [    0/ 3200]\n",
            "loss: 1.305198  [   16/ 3200]\n",
            "loss: 1.246384  [   32/ 3200]\n",
            "loss: 1.133322  [   48/ 3200]\n",
            "loss: 1.302890  [   64/ 3200]\n",
            "loss: 1.224782  [   80/ 3200]\n",
            "loss: 1.306579  [   96/ 3200]\n",
            "loss: 1.241124  [  112/ 3200]\n",
            "loss: 1.082798  [  128/ 3200]\n",
            "loss: 1.272889  [  144/ 3200]\n",
            "loss: 1.155751  [  160/ 3200]\n",
            "loss: 1.247810  [  176/ 3200]\n",
            "loss: 1.269194  [  192/ 3200]\n",
            "loss: 1.234070  [  208/ 3200]\n",
            "loss: 1.233635  [  224/ 3200]\n",
            "loss: 1.137253  [  240/ 3200]\n",
            "loss: 1.133539  [  256/ 3200]\n",
            "loss: 1.212287  [  272/ 3200]\n",
            "loss: 1.243551  [  288/ 3200]\n",
            "loss: 1.140870  [  304/ 3200]\n",
            "loss: 1.341504  [  320/ 3200]\n",
            "loss: 1.324721  [  336/ 3200]\n",
            "loss: 1.276959  [  352/ 3200]\n",
            "loss: 1.233739  [  368/ 3200]\n",
            "loss: 1.292481  [  384/ 3200]\n",
            "loss: 1.251860  [  400/ 3200]\n",
            "loss: 1.212470  [  416/ 3200]\n",
            "loss: 1.177232  [  432/ 3200]\n",
            "loss: 1.217733  [  448/ 3200]\n",
            "loss: 1.328939  [  464/ 3200]\n",
            "loss: 1.289587  [  480/ 3200]\n",
            "loss: 1.135552  [  496/ 3200]\n",
            "loss: 1.243649  [  512/ 3200]\n",
            "loss: 1.185178  [  528/ 3200]\n",
            "loss: 1.273163  [  544/ 3200]\n",
            "loss: 1.256412  [  560/ 3200]\n",
            "loss: 1.183877  [  576/ 3200]\n",
            "loss: 1.179169  [  592/ 3200]\n",
            "loss: 1.184016  [  608/ 3200]\n",
            "loss: 1.175041  [  624/ 3200]\n",
            "loss: 1.362481  [  640/ 3200]\n",
            "loss: 1.300146  [  656/ 3200]\n",
            "loss: 1.233138  [  672/ 3200]\n",
            "loss: 1.158734  [  688/ 3200]\n",
            "loss: 1.134017  [  704/ 3200]\n",
            "loss: 1.189326  [  720/ 3200]\n",
            "loss: 1.260230  [  736/ 3200]\n",
            "loss: 1.242676  [  752/ 3200]\n",
            "loss: 1.298015  [  768/ 3200]\n",
            "loss: 1.256374  [  784/ 3200]\n",
            "loss: 1.305397  [  800/ 3200]\n",
            "loss: 1.261479  [  816/ 3200]\n",
            "loss: 1.190252  [  832/ 3200]\n",
            "loss: 1.242867  [  848/ 3200]\n",
            "loss: 1.178097  [  864/ 3200]\n",
            "loss: 1.384400  [  880/ 3200]\n",
            "loss: 1.257915  [  896/ 3200]\n",
            "loss: 1.342989  [  912/ 3200]\n",
            "loss: 1.077660  [  928/ 3200]\n",
            "loss: 1.243088  [  944/ 3200]\n",
            "loss: 1.369890  [  960/ 3200]\n",
            "loss: 1.272772  [  976/ 3200]\n",
            "loss: 1.142701  [  992/ 3200]\n",
            "loss: 1.274962  [ 1008/ 3200]\n",
            "loss: 1.168032  [ 1024/ 3200]\n",
            "loss: 1.229997  [ 1040/ 3200]\n",
            "loss: 1.177518  [ 1056/ 3200]\n",
            "loss: 1.343536  [ 1072/ 3200]\n",
            "loss: 1.298200  [ 1088/ 3200]\n",
            "loss: 1.233905  [ 1104/ 3200]\n",
            "loss: 1.150318  [ 1120/ 3200]\n",
            "loss: 1.311746  [ 1136/ 3200]\n",
            "loss: 1.203771  [ 1152/ 3200]\n",
            "loss: 1.292408  [ 1168/ 3200]\n",
            "loss: 1.217515  [ 1184/ 3200]\n",
            "loss: 1.275777  [ 1200/ 3200]\n",
            "loss: 1.378926  [ 1216/ 3200]\n",
            "loss: 1.250010  [ 1232/ 3200]\n",
            "loss: 1.216106  [ 1248/ 3200]\n",
            "loss: 1.262329  [ 1264/ 3200]\n",
            "loss: 1.165942  [ 1280/ 3200]\n",
            "loss: 1.194412  [ 1296/ 3200]\n",
            "loss: 1.343658  [ 1312/ 3200]\n",
            "loss: 1.241590  [ 1328/ 3200]\n",
            "loss: 1.027492  [ 1344/ 3200]\n",
            "loss: 1.286454  [ 1360/ 3200]\n",
            "loss: 1.155694  [ 1376/ 3200]\n",
            "loss: 1.269999  [ 1392/ 3200]\n",
            "loss: 1.253961  [ 1408/ 3200]\n",
            "loss: 1.185901  [ 1424/ 3200]\n",
            "loss: 1.252603  [ 1440/ 3200]\n",
            "loss: 1.202096  [ 1456/ 3200]\n",
            "loss: 1.173695  [ 1472/ 3200]\n",
            "loss: 1.263742  [ 1488/ 3200]\n",
            "loss: 1.212187  [ 1504/ 3200]\n",
            "loss: 1.141969  [ 1520/ 3200]\n",
            "loss: 1.301911  [ 1536/ 3200]\n",
            "loss: 1.333763  [ 1552/ 3200]\n",
            "loss: 1.234698  [ 1568/ 3200]\n",
            "loss: 1.197466  [ 1584/ 3200]\n",
            "loss: 1.215003  [ 1600/ 3200]\n",
            "loss: 1.230636  [ 1616/ 3200]\n",
            "loss: 1.217153  [ 1632/ 3200]\n",
            "loss: 1.214495  [ 1648/ 3200]\n",
            "loss: 1.122063  [ 1664/ 3200]\n",
            "loss: 1.275276  [ 1680/ 3200]\n",
            "loss: 1.199635  [ 1696/ 3200]\n",
            "loss: 1.195236  [ 1712/ 3200]\n",
            "loss: 1.263773  [ 1728/ 3200]\n",
            "loss: 1.267308  [ 1744/ 3200]\n",
            "loss: 1.249893  [ 1760/ 3200]\n",
            "loss: 1.190722  [ 1776/ 3200]\n",
            "loss: 1.179989  [ 1792/ 3200]\n",
            "loss: 1.290369  [ 1808/ 3200]\n",
            "loss: 1.243530  [ 1824/ 3200]\n",
            "loss: 1.206161  [ 1840/ 3200]\n",
            "loss: 1.163742  [ 1856/ 3200]\n",
            "loss: 1.258323  [ 1872/ 3200]\n",
            "loss: 1.199332  [ 1888/ 3200]\n",
            "loss: 1.284184  [ 1904/ 3200]\n",
            "loss: 1.221569  [ 1920/ 3200]\n",
            "loss: 1.308572  [ 1936/ 3200]\n",
            "loss: 1.210934  [ 1952/ 3200]\n",
            "loss: 1.301006  [ 1968/ 3200]\n",
            "loss: 1.243549  [ 1984/ 3200]\n",
            "loss: 1.302237  [ 2000/ 3200]\n",
            "loss: 1.210982  [ 2016/ 3200]\n",
            "loss: 1.169032  [ 2032/ 3200]\n",
            "loss: 1.294663  [ 2048/ 3200]\n",
            "loss: 1.146571  [ 2064/ 3200]\n",
            "loss: 1.242613  [ 2080/ 3200]\n",
            "loss: 1.232010  [ 2096/ 3200]\n",
            "loss: 1.309576  [ 2112/ 3200]\n",
            "loss: 1.123771  [ 2128/ 3200]\n",
            "loss: 1.119878  [ 2144/ 3200]\n",
            "loss: 1.199447  [ 2160/ 3200]\n",
            "loss: 1.188185  [ 2176/ 3200]\n",
            "loss: 1.201373  [ 2192/ 3200]\n",
            "loss: 1.105725  [ 2208/ 3200]\n",
            "loss: 1.332891  [ 2224/ 3200]\n",
            "loss: 1.214983  [ 2240/ 3200]\n",
            "loss: 1.199936  [ 2256/ 3200]\n",
            "loss: 1.193737  [ 2272/ 3200]\n",
            "loss: 1.199088  [ 2288/ 3200]\n",
            "loss: 1.240480  [ 2304/ 3200]\n",
            "loss: 1.260444  [ 2320/ 3200]\n",
            "loss: 1.163058  [ 2336/ 3200]\n",
            "loss: 1.183077  [ 2352/ 3200]\n",
            "loss: 1.198550  [ 2368/ 3200]\n",
            "loss: 1.235632  [ 2384/ 3200]\n",
            "loss: 1.223325  [ 2400/ 3200]\n",
            "loss: 1.071286  [ 2416/ 3200]\n",
            "loss: 1.306030  [ 2432/ 3200]\n",
            "loss: 1.275819  [ 2448/ 3200]\n",
            "loss: 1.247941  [ 2464/ 3200]\n",
            "loss: 1.096123  [ 2480/ 3200]\n",
            "loss: 1.422170  [ 2496/ 3200]\n",
            "loss: 1.130132  [ 2512/ 3200]\n",
            "loss: 1.376194  [ 2528/ 3200]\n",
            "loss: 1.236104  [ 2544/ 3200]\n",
            "loss: 1.178647  [ 2560/ 3200]\n",
            "loss: 1.000919  [ 2576/ 3200]\n",
            "loss: 1.297320  [ 2592/ 3200]\n",
            "loss: 1.299540  [ 2608/ 3200]\n",
            "loss: 1.220733  [ 2624/ 3200]\n",
            "loss: 1.358558  [ 2640/ 3200]\n",
            "loss: 1.165829  [ 2656/ 3200]\n",
            "loss: 1.141846  [ 2672/ 3200]\n",
            "loss: 1.307039  [ 2688/ 3200]\n",
            "loss: 1.091502  [ 2704/ 3200]\n",
            "loss: 1.307382  [ 2720/ 3200]\n",
            "loss: 1.268509  [ 2736/ 3200]\n",
            "loss: 1.243677  [ 2752/ 3200]\n",
            "loss: 1.276876  [ 2768/ 3200]\n",
            "loss: 1.295183  [ 2784/ 3200]\n",
            "loss: 1.181774  [ 2800/ 3200]\n",
            "loss: 1.094685  [ 2816/ 3200]\n",
            "loss: 1.063630  [ 2832/ 3200]\n",
            "loss: 1.211411  [ 2848/ 3200]\n",
            "loss: 1.273344  [ 2864/ 3200]\n",
            "loss: 1.309248  [ 2880/ 3200]\n",
            "loss: 1.105653  [ 2896/ 3200]\n",
            "loss: 1.303208  [ 2912/ 3200]\n",
            "loss: 1.208028  [ 2928/ 3200]\n",
            "loss: 1.088073  [ 2944/ 3200]\n",
            "loss: 1.138765  [ 2960/ 3200]\n",
            "loss: 1.184046  [ 2976/ 3200]\n",
            "loss: 1.246239  [ 2992/ 3200]\n",
            "loss: 1.263177  [ 3008/ 3200]\n",
            "loss: 1.192901  [ 3024/ 3200]\n",
            "loss: 1.079553  [ 3040/ 3200]\n",
            "loss: 1.085803  [ 3056/ 3200]\n",
            "loss: 1.394880  [ 3072/ 3200]\n",
            "loss: 1.355927  [ 3088/ 3200]\n",
            "loss: 1.139842  [ 3104/ 3200]\n",
            "loss: 1.337811  [ 3120/ 3200]\n",
            "loss: 1.013351  [ 3136/ 3200]\n",
            "loss: 1.214830  [ 3152/ 3200]\n",
            "loss: 1.163201  [ 3168/ 3200]\n",
            "loss: 1.192917  [ 3184/ 3200]\n",
            "current epoch: 8\n",
            "\n",
            "loss: 1.213890  [    0/ 3200]\n",
            "loss: 1.203364  [   16/ 3200]\n",
            "loss: 1.196944  [   32/ 3200]\n",
            "loss: 1.238225  [   48/ 3200]\n",
            "loss: 1.019260  [   64/ 3200]\n",
            "loss: 1.197872  [   80/ 3200]\n",
            "loss: 1.051841  [   96/ 3200]\n",
            "loss: 1.141037  [  112/ 3200]\n",
            "loss: 1.210580  [  128/ 3200]\n",
            "loss: 1.163978  [  144/ 3200]\n",
            "loss: 1.203908  [  160/ 3200]\n",
            "loss: 1.212203  [  176/ 3200]\n",
            "loss: 1.338547  [  192/ 3200]\n",
            "loss: 1.215984  [  208/ 3200]\n",
            "loss: 1.135901  [  224/ 3200]\n",
            "loss: 1.198103  [  240/ 3200]\n",
            "loss: 1.234609  [  256/ 3200]\n",
            "loss: 1.372051  [  272/ 3200]\n",
            "loss: 1.240600  [  288/ 3200]\n",
            "loss: 1.246462  [  304/ 3200]\n",
            "loss: 1.149499  [  320/ 3200]\n",
            "loss: 1.271963  [  336/ 3200]\n",
            "loss: 1.273571  [  352/ 3200]\n",
            "loss: 1.273729  [  368/ 3200]\n",
            "loss: 1.259200  [  384/ 3200]\n",
            "loss: 1.229502  [  400/ 3200]\n",
            "loss: 1.190826  [  416/ 3200]\n",
            "loss: 1.233458  [  432/ 3200]\n",
            "loss: 1.128154  [  448/ 3200]\n",
            "loss: 1.271555  [  464/ 3200]\n",
            "loss: 1.145590  [  480/ 3200]\n",
            "loss: 1.207291  [  496/ 3200]\n",
            "loss: 1.247597  [  512/ 3200]\n",
            "loss: 1.160952  [  528/ 3200]\n",
            "loss: 1.290479  [  544/ 3200]\n",
            "loss: 1.158911  [  560/ 3200]\n",
            "loss: 1.224618  [  576/ 3200]\n",
            "loss: 1.191691  [  592/ 3200]\n",
            "loss: 1.136415  [  608/ 3200]\n",
            "loss: 1.272640  [  624/ 3200]\n",
            "loss: 1.211867  [  640/ 3200]\n",
            "loss: 1.218716  [  656/ 3200]\n",
            "loss: 1.155778  [  672/ 3200]\n",
            "loss: 1.158923  [  688/ 3200]\n",
            "loss: 1.208446  [  704/ 3200]\n",
            "loss: 1.149255  [  720/ 3200]\n",
            "loss: 1.194437  [  736/ 3200]\n",
            "loss: 1.217067  [  752/ 3200]\n",
            "loss: 1.124222  [  768/ 3200]\n",
            "loss: 1.230817  [  784/ 3200]\n",
            "loss: 1.208694  [  800/ 3200]\n",
            "loss: 1.181910  [  816/ 3200]\n",
            "loss: 1.246668  [  832/ 3200]\n",
            "loss: 1.182516  [  848/ 3200]\n",
            "loss: 1.231419  [  864/ 3200]\n",
            "loss: 1.221526  [  880/ 3200]\n",
            "loss: 1.166841  [  896/ 3200]\n",
            "loss: 1.067005  [  912/ 3200]\n",
            "loss: 1.300348  [  928/ 3200]\n",
            "loss: 1.112918  [  944/ 3200]\n",
            "loss: 1.181890  [  960/ 3200]\n",
            "loss: 1.248387  [  976/ 3200]\n",
            "loss: 1.168095  [  992/ 3200]\n",
            "loss: 1.282689  [ 1008/ 3200]\n",
            "loss: 1.238079  [ 1024/ 3200]\n",
            "loss: 1.141904  [ 1040/ 3200]\n",
            "loss: 1.092333  [ 1056/ 3200]\n",
            "loss: 1.085369  [ 1072/ 3200]\n",
            "loss: 1.195135  [ 1088/ 3200]\n",
            "loss: 1.199630  [ 1104/ 3200]\n",
            "loss: 1.158658  [ 1120/ 3200]\n",
            "loss: 1.322032  [ 1136/ 3200]\n",
            "loss: 1.318405  [ 1152/ 3200]\n",
            "loss: 1.184049  [ 1168/ 3200]\n",
            "loss: 1.307970  [ 1184/ 3200]\n",
            "loss: 1.160080  [ 1200/ 3200]\n",
            "loss: 1.149118  [ 1216/ 3200]\n",
            "loss: 1.255908  [ 1232/ 3200]\n",
            "loss: 1.213779  [ 1248/ 3200]\n",
            "loss: 1.171182  [ 1264/ 3200]\n",
            "loss: 1.315884  [ 1280/ 3200]\n",
            "loss: 1.198684  [ 1296/ 3200]\n",
            "loss: 1.211042  [ 1312/ 3200]\n",
            "loss: 1.191345  [ 1328/ 3200]\n",
            "loss: 1.268360  [ 1344/ 3200]\n",
            "loss: 1.178194  [ 1360/ 3200]\n",
            "loss: 1.101667  [ 1376/ 3200]\n",
            "loss: 1.324309  [ 1392/ 3200]\n",
            "loss: 1.297084  [ 1408/ 3200]\n",
            "loss: 1.286757  [ 1424/ 3200]\n",
            "loss: 1.224753  [ 1440/ 3200]\n",
            "loss: 1.219890  [ 1456/ 3200]\n",
            "loss: 1.204773  [ 1472/ 3200]\n",
            "loss: 1.182324  [ 1488/ 3200]\n",
            "loss: 1.174982  [ 1504/ 3200]\n",
            "loss: 1.282158  [ 1520/ 3200]\n",
            "loss: 1.123571  [ 1536/ 3200]\n",
            "loss: 1.242918  [ 1552/ 3200]\n",
            "loss: 1.213920  [ 1568/ 3200]\n",
            "loss: 1.149597  [ 1584/ 3200]\n",
            "loss: 1.260352  [ 1600/ 3200]\n",
            "loss: 1.241071  [ 1616/ 3200]\n",
            "loss: 1.183150  [ 1632/ 3200]\n",
            "loss: 1.231714  [ 1648/ 3200]\n",
            "loss: 1.145690  [ 1664/ 3200]\n",
            "loss: 1.140303  [ 1680/ 3200]\n",
            "loss: 1.253865  [ 1696/ 3200]\n",
            "loss: 1.123386  [ 1712/ 3200]\n",
            "loss: 1.192265  [ 1728/ 3200]\n",
            "loss: 1.272016  [ 1744/ 3200]\n",
            "loss: 1.214885  [ 1760/ 3200]\n",
            "loss: 1.149747  [ 1776/ 3200]\n",
            "loss: 1.126875  [ 1792/ 3200]\n",
            "loss: 1.210800  [ 1808/ 3200]\n",
            "loss: 1.066245  [ 1824/ 3200]\n",
            "loss: 1.117491  [ 1840/ 3200]\n",
            "loss: 1.142774  [ 1856/ 3200]\n",
            "loss: 1.124666  [ 1872/ 3200]\n",
            "loss: 1.177526  [ 1888/ 3200]\n",
            "loss: 1.305759  [ 1904/ 3200]\n",
            "loss: 1.148804  [ 1920/ 3200]\n",
            "loss: 1.252539  [ 1936/ 3200]\n",
            "loss: 1.287486  [ 1952/ 3200]\n",
            "loss: 1.165343  [ 1968/ 3200]\n",
            "loss: 1.146431  [ 1984/ 3200]\n",
            "loss: 1.226692  [ 2000/ 3200]\n",
            "loss: 1.252528  [ 2016/ 3200]\n",
            "loss: 1.136388  [ 2032/ 3200]\n",
            "loss: 1.291531  [ 2048/ 3200]\n",
            "loss: 1.163700  [ 2064/ 3200]\n",
            "loss: 1.200605  [ 2080/ 3200]\n",
            "loss: 1.124504  [ 2096/ 3200]\n",
            "loss: 1.248186  [ 2112/ 3200]\n",
            "loss: 1.268195  [ 2128/ 3200]\n",
            "loss: 1.284258  [ 2144/ 3200]\n",
            "loss: 1.096449  [ 2160/ 3200]\n",
            "loss: 1.195936  [ 2176/ 3200]\n",
            "loss: 1.208810  [ 2192/ 3200]\n",
            "loss: 1.273446  [ 2208/ 3200]\n",
            "loss: 1.175295  [ 2224/ 3200]\n",
            "loss: 1.091232  [ 2240/ 3200]\n",
            "loss: 1.274361  [ 2256/ 3200]\n",
            "loss: 1.217318  [ 2272/ 3200]\n",
            "loss: 1.120676  [ 2288/ 3200]\n",
            "loss: 1.180383  [ 2304/ 3200]\n",
            "loss: 1.131611  [ 2320/ 3200]\n",
            "loss: 1.170550  [ 2336/ 3200]\n",
            "loss: 1.241190  [ 2352/ 3200]\n",
            "loss: 1.229824  [ 2368/ 3200]\n",
            "loss: 1.154561  [ 2384/ 3200]\n",
            "loss: 1.218181  [ 2400/ 3200]\n",
            "loss: 1.151206  [ 2416/ 3200]\n",
            "loss: 1.210783  [ 2432/ 3200]\n",
            "loss: 1.151301  [ 2448/ 3200]\n",
            "loss: 1.244037  [ 2464/ 3200]\n",
            "loss: 1.103178  [ 2480/ 3200]\n",
            "loss: 1.060400  [ 2496/ 3200]\n",
            "loss: 1.148893  [ 2512/ 3200]\n",
            "loss: 1.199444  [ 2528/ 3200]\n",
            "loss: 1.252838  [ 2544/ 3200]\n",
            "loss: 1.162478  [ 2560/ 3200]\n",
            "loss: 1.265844  [ 2576/ 3200]\n",
            "loss: 1.212365  [ 2592/ 3200]\n",
            "loss: 1.237352  [ 2608/ 3200]\n",
            "loss: 1.182780  [ 2624/ 3200]\n",
            "loss: 1.290723  [ 2640/ 3200]\n",
            "loss: 1.149371  [ 2656/ 3200]\n",
            "loss: 1.233492  [ 2672/ 3200]\n",
            "loss: 1.162897  [ 2688/ 3200]\n",
            "loss: 1.053427  [ 2704/ 3200]\n",
            "loss: 1.190606  [ 2720/ 3200]\n",
            "loss: 1.147556  [ 2736/ 3200]\n",
            "loss: 1.118876  [ 2752/ 3200]\n",
            "loss: 1.177837  [ 2768/ 3200]\n",
            "loss: 1.277867  [ 2784/ 3200]\n",
            "loss: 1.143185  [ 2800/ 3200]\n",
            "loss: 1.172942  [ 2816/ 3200]\n",
            "loss: 1.091520  [ 2832/ 3200]\n",
            "loss: 1.326074  [ 2848/ 3200]\n",
            "loss: 1.124650  [ 2864/ 3200]\n",
            "loss: 1.237324  [ 2880/ 3200]\n",
            "loss: 1.157977  [ 2896/ 3200]\n",
            "loss: 1.197460  [ 2912/ 3200]\n",
            "loss: 1.075540  [ 2928/ 3200]\n",
            "loss: 1.187550  [ 2944/ 3200]\n",
            "loss: 1.163040  [ 2960/ 3200]\n",
            "loss: 1.271304  [ 2976/ 3200]\n",
            "loss: 1.257927  [ 2992/ 3200]\n",
            "loss: 1.202955  [ 3008/ 3200]\n",
            "loss: 1.235360  [ 3024/ 3200]\n",
            "loss: 1.133363  [ 3040/ 3200]\n",
            "loss: 1.104963  [ 3056/ 3200]\n",
            "loss: 1.256920  [ 3072/ 3200]\n",
            "loss: 1.257762  [ 3088/ 3200]\n",
            "loss: 1.222969  [ 3104/ 3200]\n",
            "loss: 1.075107  [ 3120/ 3200]\n",
            "loss: 1.154227  [ 3136/ 3200]\n",
            "loss: 1.290707  [ 3152/ 3200]\n",
            "loss: 1.198743  [ 3168/ 3200]\n",
            "loss: 1.189712  [ 3184/ 3200]\n",
            "current epoch: 9\n",
            "\n",
            "loss: 1.089982  [    0/ 3200]\n",
            "loss: 1.229232  [   16/ 3200]\n",
            "loss: 1.239359  [   32/ 3200]\n",
            "loss: 1.173505  [   48/ 3200]\n",
            "loss: 1.140767  [   64/ 3200]\n",
            "loss: 1.169147  [   80/ 3200]\n",
            "loss: 1.186662  [   96/ 3200]\n",
            "loss: 1.098126  [  112/ 3200]\n",
            "loss: 1.142876  [  128/ 3200]\n",
            "loss: 1.163196  [  144/ 3200]\n",
            "loss: 1.180137  [  160/ 3200]\n",
            "loss: 1.268697  [  176/ 3200]\n",
            "loss: 1.212294  [  192/ 3200]\n",
            "loss: 1.151342  [  208/ 3200]\n",
            "loss: 1.161829  [  224/ 3200]\n",
            "loss: 1.205193  [  240/ 3200]\n",
            "loss: 1.024635  [  256/ 3200]\n",
            "loss: 1.103363  [  272/ 3200]\n",
            "loss: 1.282428  [  288/ 3200]\n",
            "loss: 1.150024  [  304/ 3200]\n",
            "loss: 1.063894  [  320/ 3200]\n",
            "loss: 1.201438  [  336/ 3200]\n",
            "loss: 1.213275  [  352/ 3200]\n",
            "loss: 1.156713  [  368/ 3200]\n",
            "loss: 1.346341  [  384/ 3200]\n",
            "loss: 1.100912  [  400/ 3200]\n",
            "loss: 1.112384  [  416/ 3200]\n",
            "loss: 1.102057  [  432/ 3200]\n",
            "loss: 1.095838  [  448/ 3200]\n",
            "loss: 1.228517  [  464/ 3200]\n",
            "loss: 1.340076  [  480/ 3200]\n",
            "loss: 1.166582  [  496/ 3200]\n",
            "loss: 1.271091  [  512/ 3200]\n",
            "loss: 1.213147  [  528/ 3200]\n",
            "loss: 1.179031  [  544/ 3200]\n",
            "loss: 1.161016  [  560/ 3200]\n",
            "loss: 1.191646  [  576/ 3200]\n",
            "loss: 1.228933  [  592/ 3200]\n",
            "loss: 1.113577  [  608/ 3200]\n",
            "loss: 1.199672  [  624/ 3200]\n",
            "loss: 1.111019  [  640/ 3200]\n",
            "loss: 1.236175  [  656/ 3200]\n",
            "loss: 1.083170  [  672/ 3200]\n",
            "loss: 1.146188  [  688/ 3200]\n",
            "loss: 1.155522  [  704/ 3200]\n",
            "loss: 1.238459  [  720/ 3200]\n",
            "loss: 1.127435  [  736/ 3200]\n",
            "loss: 1.195095  [  752/ 3200]\n",
            "loss: 1.169458  [  768/ 3200]\n",
            "loss: 1.097014  [  784/ 3200]\n",
            "loss: 1.303097  [  800/ 3200]\n",
            "loss: 1.078204  [  816/ 3200]\n",
            "loss: 1.211149  [  832/ 3200]\n",
            "loss: 1.152208  [  848/ 3200]\n",
            "loss: 1.294349  [  864/ 3200]\n",
            "loss: 1.205128  [  880/ 3200]\n",
            "loss: 1.220071  [  896/ 3200]\n",
            "loss: 1.185225  [  912/ 3200]\n",
            "loss: 1.149336  [  928/ 3200]\n",
            "loss: 1.312952  [  944/ 3200]\n",
            "loss: 1.115038  [  960/ 3200]\n",
            "loss: 1.087493  [  976/ 3200]\n",
            "loss: 1.201693  [  992/ 3200]\n",
            "loss: 1.108771  [ 1008/ 3200]\n",
            "loss: 1.080147  [ 1024/ 3200]\n",
            "loss: 1.188720  [ 1040/ 3200]\n",
            "loss: 1.259797  [ 1056/ 3200]\n",
            "loss: 1.273578  [ 1072/ 3200]\n",
            "loss: 1.246264  [ 1088/ 3200]\n",
            "loss: 1.251994  [ 1104/ 3200]\n",
            "loss: 1.156219  [ 1120/ 3200]\n",
            "loss: 1.196949  [ 1136/ 3200]\n",
            "loss: 1.134626  [ 1152/ 3200]\n",
            "loss: 1.129078  [ 1168/ 3200]\n",
            "loss: 1.308778  [ 1184/ 3200]\n",
            "loss: 1.083365  [ 1200/ 3200]\n",
            "loss: 1.144701  [ 1216/ 3200]\n",
            "loss: 1.115581  [ 1232/ 3200]\n",
            "loss: 1.159240  [ 1248/ 3200]\n",
            "loss: 1.198466  [ 1264/ 3200]\n",
            "loss: 1.080883  [ 1280/ 3200]\n",
            "loss: 1.181247  [ 1296/ 3200]\n",
            "loss: 1.272845  [ 1312/ 3200]\n",
            "loss: 1.168091  [ 1328/ 3200]\n",
            "loss: 1.154611  [ 1344/ 3200]\n",
            "loss: 1.128072  [ 1360/ 3200]\n",
            "loss: 1.199717  [ 1376/ 3200]\n",
            "loss: 1.077452  [ 1392/ 3200]\n",
            "loss: 1.160661  [ 1408/ 3200]\n",
            "loss: 1.285454  [ 1424/ 3200]\n",
            "loss: 1.105142  [ 1440/ 3200]\n",
            "loss: 1.090972  [ 1456/ 3200]\n",
            "loss: 1.181409  [ 1472/ 3200]\n",
            "loss: 1.099233  [ 1488/ 3200]\n",
            "loss: 1.191155  [ 1504/ 3200]\n",
            "loss: 1.167107  [ 1520/ 3200]\n",
            "loss: 1.166367  [ 1536/ 3200]\n",
            "loss: 1.172394  [ 1552/ 3200]\n",
            "loss: 1.219453  [ 1568/ 3200]\n",
            "loss: 1.216377  [ 1584/ 3200]\n",
            "loss: 1.172261  [ 1600/ 3200]\n",
            "loss: 1.113466  [ 1616/ 3200]\n",
            "loss: 1.309945  [ 1632/ 3200]\n",
            "loss: 1.337446  [ 1648/ 3200]\n",
            "loss: 1.165601  [ 1664/ 3200]\n",
            "loss: 1.172903  [ 1680/ 3200]\n",
            "loss: 1.020929  [ 1696/ 3200]\n",
            "loss: 1.137245  [ 1712/ 3200]\n",
            "loss: 1.243929  [ 1728/ 3200]\n",
            "loss: 1.001890  [ 1744/ 3200]\n",
            "loss: 1.227757  [ 1760/ 3200]\n",
            "loss: 1.233351  [ 1776/ 3200]\n",
            "loss: 1.213537  [ 1792/ 3200]\n",
            "loss: 1.225640  [ 1808/ 3200]\n",
            "loss: 1.144165  [ 1824/ 3200]\n",
            "loss: 1.079983  [ 1840/ 3200]\n",
            "loss: 1.218216  [ 1856/ 3200]\n",
            "loss: 1.204947  [ 1872/ 3200]\n",
            "loss: 1.267362  [ 1888/ 3200]\n",
            "loss: 1.196644  [ 1904/ 3200]\n",
            "loss: 1.370131  [ 1920/ 3200]\n",
            "loss: 1.073246  [ 1936/ 3200]\n",
            "loss: 1.356790  [ 1952/ 3200]\n",
            "loss: 1.127420  [ 1968/ 3200]\n",
            "loss: 1.243726  [ 1984/ 3200]\n",
            "loss: 1.089653  [ 2000/ 3200]\n",
            "loss: 1.170271  [ 2016/ 3200]\n",
            "loss: 1.152751  [ 2032/ 3200]\n",
            "loss: 1.149217  [ 2048/ 3200]\n",
            "loss: 1.183116  [ 2064/ 3200]\n",
            "loss: 1.225968  [ 2080/ 3200]\n",
            "loss: 1.281335  [ 2096/ 3200]\n",
            "loss: 1.099996  [ 2112/ 3200]\n",
            "loss: 1.145639  [ 2128/ 3200]\n",
            "loss: 1.134849  [ 2144/ 3200]\n",
            "loss: 1.111504  [ 2160/ 3200]\n",
            "loss: 0.987083  [ 2176/ 3200]\n",
            "loss: 1.182700  [ 2192/ 3200]\n",
            "loss: 1.275782  [ 2208/ 3200]\n",
            "loss: 1.226964  [ 2224/ 3200]\n",
            "loss: 1.172755  [ 2240/ 3200]\n",
            "loss: 1.138894  [ 2256/ 3200]\n",
            "loss: 1.118347  [ 2272/ 3200]\n",
            "loss: 1.152189  [ 2288/ 3200]\n",
            "loss: 1.121182  [ 2304/ 3200]\n",
            "loss: 1.078174  [ 2320/ 3200]\n",
            "loss: 1.226478  [ 2336/ 3200]\n",
            "loss: 1.210913  [ 2352/ 3200]\n",
            "loss: 1.184119  [ 2368/ 3200]\n",
            "loss: 1.268165  [ 2384/ 3200]\n",
            "loss: 1.208358  [ 2400/ 3200]\n",
            "loss: 1.126558  [ 2416/ 3200]\n",
            "loss: 1.098309  [ 2432/ 3200]\n",
            "loss: 1.153966  [ 2448/ 3200]\n",
            "loss: 1.055495  [ 2464/ 3200]\n",
            "loss: 1.230939  [ 2480/ 3200]\n",
            "loss: 1.380825  [ 2496/ 3200]\n",
            "loss: 1.130291  [ 2512/ 3200]\n",
            "loss: 1.295927  [ 2528/ 3200]\n",
            "loss: 1.024444  [ 2544/ 3200]\n",
            "loss: 1.200464  [ 2560/ 3200]\n",
            "loss: 1.246752  [ 2576/ 3200]\n",
            "loss: 1.170732  [ 2592/ 3200]\n",
            "loss: 1.120701  [ 2608/ 3200]\n",
            "loss: 1.032437  [ 2624/ 3200]\n",
            "loss: 1.183458  [ 2640/ 3200]\n",
            "loss: 1.139848  [ 2656/ 3200]\n",
            "loss: 1.182938  [ 2672/ 3200]\n",
            "loss: 1.090504  [ 2688/ 3200]\n",
            "loss: 1.147208  [ 2704/ 3200]\n",
            "loss: 1.156636  [ 2720/ 3200]\n",
            "loss: 1.034829  [ 2736/ 3200]\n",
            "loss: 1.107552  [ 2752/ 3200]\n",
            "loss: 1.150524  [ 2768/ 3200]\n",
            "loss: 1.249938  [ 2784/ 3200]\n",
            "loss: 1.207155  [ 2800/ 3200]\n",
            "loss: 1.029979  [ 2816/ 3200]\n",
            "loss: 1.199729  [ 2832/ 3200]\n",
            "loss: 1.183170  [ 2848/ 3200]\n",
            "loss: 1.289473  [ 2864/ 3200]\n",
            "loss: 1.169127  [ 2880/ 3200]\n",
            "loss: 1.077336  [ 2896/ 3200]\n",
            "loss: 1.147208  [ 2912/ 3200]\n",
            "loss: 1.163241  [ 2928/ 3200]\n",
            "loss: 1.072083  [ 2944/ 3200]\n",
            "loss: 1.146931  [ 2960/ 3200]\n",
            "loss: 1.130337  [ 2976/ 3200]\n",
            "loss: 1.192081  [ 2992/ 3200]\n",
            "loss: 1.111312  [ 3008/ 3200]\n",
            "loss: 1.115072  [ 3024/ 3200]\n",
            "loss: 1.185415  [ 3040/ 3200]\n",
            "loss: 1.220160  [ 3056/ 3200]\n",
            "loss: 1.129045  [ 3072/ 3200]\n",
            "loss: 1.181962  [ 3088/ 3200]\n",
            "loss: 1.173200  [ 3104/ 3200]\n",
            "loss: 1.096390  [ 3120/ 3200]\n",
            "loss: 1.163155  [ 3136/ 3200]\n",
            "loss: 1.048141  [ 3152/ 3200]\n",
            "loss: 1.203178  [ 3168/ 3200]\n",
            "loss: 1.121745  [ 3184/ 3200]\n",
            "current epoch: 10\n",
            "\n",
            "loss: 1.140262  [    0/ 3200]\n",
            "loss: 1.019403  [   16/ 3200]\n",
            "loss: 1.193803  [   32/ 3200]\n",
            "loss: 1.225992  [   48/ 3200]\n",
            "loss: 1.093575  [   64/ 3200]\n",
            "loss: 1.227155  [   80/ 3200]\n",
            "loss: 1.129813  [   96/ 3200]\n",
            "loss: 1.226305  [  112/ 3200]\n",
            "loss: 1.045042  [  128/ 3200]\n",
            "loss: 1.053086  [  144/ 3200]\n",
            "loss: 1.190258  [  160/ 3200]\n",
            "loss: 1.234903  [  176/ 3200]\n",
            "loss: 1.301306  [  192/ 3200]\n",
            "loss: 1.040109  [  208/ 3200]\n",
            "loss: 1.370315  [  224/ 3200]\n",
            "loss: 1.194317  [  240/ 3200]\n",
            "loss: 1.331045  [  256/ 3200]\n",
            "loss: 0.987448  [  272/ 3200]\n",
            "loss: 1.108825  [  288/ 3200]\n",
            "loss: 1.161533  [  304/ 3200]\n",
            "loss: 1.220636  [  320/ 3200]\n",
            "loss: 1.134573  [  336/ 3200]\n",
            "loss: 1.160548  [  352/ 3200]\n",
            "loss: 1.198768  [  368/ 3200]\n",
            "loss: 1.218063  [  384/ 3200]\n",
            "loss: 1.065351  [  400/ 3200]\n",
            "loss: 1.055938  [  416/ 3200]\n",
            "loss: 1.105967  [  432/ 3200]\n",
            "loss: 1.164985  [  448/ 3200]\n",
            "loss: 1.177806  [  464/ 3200]\n",
            "loss: 1.272090  [  480/ 3200]\n",
            "loss: 1.167257  [  496/ 3200]\n",
            "loss: 1.272067  [  512/ 3200]\n",
            "loss: 1.180613  [  528/ 3200]\n",
            "loss: 1.108752  [  544/ 3200]\n",
            "loss: 1.096275  [  560/ 3200]\n",
            "loss: 0.960284  [  576/ 3200]\n",
            "loss: 1.202244  [  592/ 3200]\n",
            "loss: 1.218332  [  608/ 3200]\n",
            "loss: 1.211368  [  624/ 3200]\n",
            "loss: 1.216597  [  640/ 3200]\n",
            "loss: 1.263079  [  656/ 3200]\n",
            "loss: 1.212359  [  672/ 3200]\n",
            "loss: 1.180417  [  688/ 3200]\n",
            "loss: 1.053255  [  704/ 3200]\n",
            "loss: 1.151144  [  720/ 3200]\n",
            "loss: 1.056976  [  736/ 3200]\n",
            "loss: 1.186702  [  752/ 3200]\n",
            "loss: 1.146779  [  768/ 3200]\n",
            "loss: 1.061070  [  784/ 3200]\n",
            "loss: 1.157035  [  800/ 3200]\n",
            "loss: 1.211393  [  816/ 3200]\n",
            "loss: 1.070288  [  832/ 3200]\n",
            "loss: 1.166991  [  848/ 3200]\n",
            "loss: 1.060318  [  864/ 3200]\n",
            "loss: 1.181294  [  880/ 3200]\n",
            "loss: 1.133314  [  896/ 3200]\n",
            "loss: 1.119204  [  912/ 3200]\n",
            "loss: 0.953996  [  928/ 3200]\n",
            "loss: 1.211324  [  944/ 3200]\n",
            "loss: 1.159302  [  960/ 3200]\n",
            "loss: 1.290404  [  976/ 3200]\n",
            "loss: 1.016557  [  992/ 3200]\n",
            "loss: 1.280983  [ 1008/ 3200]\n",
            "loss: 1.068498  [ 1024/ 3200]\n",
            "loss: 1.190563  [ 1040/ 3200]\n",
            "loss: 1.260598  [ 1056/ 3200]\n",
            "loss: 1.208720  [ 1072/ 3200]\n",
            "loss: 1.046058  [ 1088/ 3200]\n",
            "loss: 1.144623  [ 1104/ 3200]\n",
            "loss: 1.163759  [ 1120/ 3200]\n",
            "loss: 0.975210  [ 1136/ 3200]\n",
            "loss: 1.148923  [ 1152/ 3200]\n",
            "loss: 1.109739  [ 1168/ 3200]\n",
            "loss: 1.126563  [ 1184/ 3200]\n",
            "loss: 1.165872  [ 1200/ 3200]\n",
            "loss: 1.143893  [ 1216/ 3200]\n",
            "loss: 1.116923  [ 1232/ 3200]\n",
            "loss: 1.114878  [ 1248/ 3200]\n",
            "loss: 1.022918  [ 1264/ 3200]\n",
            "loss: 1.142166  [ 1280/ 3200]\n",
            "loss: 1.160377  [ 1296/ 3200]\n",
            "loss: 1.039645  [ 1312/ 3200]\n",
            "loss: 1.092441  [ 1328/ 3200]\n",
            "loss: 1.175276  [ 1344/ 3200]\n",
            "loss: 1.204763  [ 1360/ 3200]\n",
            "loss: 1.175428  [ 1376/ 3200]\n",
            "loss: 1.081516  [ 1392/ 3200]\n",
            "loss: 1.039010  [ 1408/ 3200]\n",
            "loss: 1.150509  [ 1424/ 3200]\n",
            "loss: 1.182373  [ 1440/ 3200]\n",
            "loss: 1.106125  [ 1456/ 3200]\n",
            "loss: 1.192967  [ 1472/ 3200]\n",
            "loss: 1.115880  [ 1488/ 3200]\n",
            "loss: 1.090460  [ 1504/ 3200]\n",
            "loss: 1.071356  [ 1520/ 3200]\n",
            "loss: 1.122589  [ 1536/ 3200]\n",
            "loss: 1.091101  [ 1552/ 3200]\n",
            "loss: 1.363523  [ 1568/ 3200]\n",
            "loss: 1.187335  [ 1584/ 3200]\n",
            "loss: 1.082410  [ 1600/ 3200]\n",
            "loss: 1.281582  [ 1616/ 3200]\n",
            "loss: 1.045326  [ 1632/ 3200]\n",
            "loss: 1.409563  [ 1648/ 3200]\n",
            "loss: 1.195526  [ 1664/ 3200]\n",
            "loss: 1.179191  [ 1680/ 3200]\n",
            "loss: 0.997437  [ 1696/ 3200]\n",
            "loss: 1.364308  [ 1712/ 3200]\n",
            "loss: 1.131332  [ 1728/ 3200]\n",
            "loss: 1.079691  [ 1744/ 3200]\n",
            "loss: 1.148825  [ 1760/ 3200]\n",
            "loss: 1.172766  [ 1776/ 3200]\n",
            "loss: 1.079190  [ 1792/ 3200]\n",
            "loss: 1.117743  [ 1808/ 3200]\n",
            "loss: 1.081946  [ 1824/ 3200]\n",
            "loss: 1.241093  [ 1840/ 3200]\n",
            "loss: 1.191437  [ 1856/ 3200]\n",
            "loss: 1.202543  [ 1872/ 3200]\n",
            "loss: 1.178108  [ 1888/ 3200]\n",
            "loss: 1.168136  [ 1904/ 3200]\n",
            "loss: 1.192328  [ 1920/ 3200]\n",
            "loss: 1.184220  [ 1936/ 3200]\n",
            "loss: 1.144358  [ 1952/ 3200]\n",
            "loss: 1.090795  [ 1968/ 3200]\n",
            "loss: 1.064405  [ 1984/ 3200]\n",
            "loss: 1.278578  [ 2000/ 3200]\n",
            "loss: 1.132062  [ 2016/ 3200]\n",
            "loss: 1.044011  [ 2032/ 3200]\n",
            "loss: 1.197933  [ 2048/ 3200]\n",
            "loss: 1.108301  [ 2064/ 3200]\n",
            "loss: 1.110555  [ 2080/ 3200]\n",
            "loss: 1.202213  [ 2096/ 3200]\n",
            "loss: 1.080231  [ 2112/ 3200]\n",
            "loss: 1.195646  [ 2128/ 3200]\n",
            "loss: 1.181490  [ 2144/ 3200]\n",
            "loss: 1.222993  [ 2160/ 3200]\n",
            "loss: 1.175140  [ 2176/ 3200]\n",
            "loss: 1.107727  [ 2192/ 3200]\n",
            "loss: 1.151577  [ 2208/ 3200]\n",
            "loss: 1.044523  [ 2224/ 3200]\n",
            "loss: 1.118020  [ 2240/ 3200]\n",
            "loss: 1.055568  [ 2256/ 3200]\n",
            "loss: 1.030124  [ 2272/ 3200]\n",
            "loss: 1.179259  [ 2288/ 3200]\n",
            "loss: 1.138062  [ 2304/ 3200]\n",
            "loss: 1.100370  [ 2320/ 3200]\n",
            "loss: 1.177794  [ 2336/ 3200]\n",
            "loss: 1.247798  [ 2352/ 3200]\n",
            "loss: 1.197735  [ 2368/ 3200]\n",
            "loss: 1.000739  [ 2384/ 3200]\n",
            "loss: 1.097292  [ 2400/ 3200]\n",
            "loss: 1.186865  [ 2416/ 3200]\n",
            "loss: 1.041499  [ 2432/ 3200]\n",
            "loss: 1.200203  [ 2448/ 3200]\n",
            "loss: 0.974858  [ 2464/ 3200]\n",
            "loss: 1.248846  [ 2480/ 3200]\n",
            "loss: 1.081847  [ 2496/ 3200]\n",
            "loss: 1.107813  [ 2512/ 3200]\n",
            "loss: 1.173454  [ 2528/ 3200]\n",
            "loss: 1.227873  [ 2544/ 3200]\n",
            "loss: 1.018659  [ 2560/ 3200]\n",
            "loss: 1.099053  [ 2576/ 3200]\n",
            "loss: 1.156495  [ 2592/ 3200]\n",
            "loss: 1.119466  [ 2608/ 3200]\n",
            "loss: 1.084911  [ 2624/ 3200]\n",
            "loss: 1.134454  [ 2640/ 3200]\n",
            "loss: 1.144794  [ 2656/ 3200]\n",
            "loss: 1.100929  [ 2672/ 3200]\n",
            "loss: 1.126940  [ 2688/ 3200]\n",
            "loss: 1.034429  [ 2704/ 3200]\n",
            "loss: 1.179897  [ 2720/ 3200]\n",
            "loss: 1.117937  [ 2736/ 3200]\n",
            "loss: 1.026614  [ 2752/ 3200]\n",
            "loss: 1.043169  [ 2768/ 3200]\n",
            "loss: 1.099291  [ 2784/ 3200]\n",
            "loss: 1.067565  [ 2800/ 3200]\n",
            "loss: 1.180966  [ 2816/ 3200]\n",
            "loss: 1.099191  [ 2832/ 3200]\n",
            "loss: 1.109471  [ 2848/ 3200]\n",
            "loss: 1.176225  [ 2864/ 3200]\n",
            "loss: 1.062706  [ 2880/ 3200]\n",
            "loss: 1.310892  [ 2896/ 3200]\n",
            "loss: 1.139225  [ 2912/ 3200]\n",
            "loss: 0.944750  [ 2928/ 3200]\n",
            "loss: 1.182744  [ 2944/ 3200]\n",
            "loss: 0.981922  [ 2960/ 3200]\n",
            "loss: 0.971679  [ 2976/ 3200]\n",
            "loss: 1.081823  [ 2992/ 3200]\n",
            "loss: 1.124400  [ 3008/ 3200]\n",
            "loss: 1.086509  [ 3024/ 3200]\n",
            "loss: 1.330522  [ 3040/ 3200]\n",
            "loss: 1.268862  [ 3056/ 3200]\n",
            "loss: 1.073912  [ 3072/ 3200]\n",
            "loss: 1.131714  [ 3088/ 3200]\n",
            "loss: 1.157049  [ 3104/ 3200]\n",
            "loss: 1.092618  [ 3120/ 3200]\n",
            "loss: 1.217625  [ 3136/ 3200]\n",
            "loss: 1.199230  [ 3152/ 3200]\n",
            "loss: 1.096522  [ 3168/ 3200]\n",
            "loss: 1.170869  [ 3184/ 3200]\n",
            "current epoch: 11\n",
            "\n",
            "loss: 1.007187  [    0/ 3200]\n",
            "loss: 1.161302  [   16/ 3200]\n",
            "loss: 1.145254  [   32/ 3200]\n",
            "loss: 1.117148  [   48/ 3200]\n",
            "loss: 1.176221  [   64/ 3200]\n",
            "loss: 1.035985  [   80/ 3200]\n",
            "loss: 1.061640  [   96/ 3200]\n",
            "loss: 1.178149  [  112/ 3200]\n",
            "loss: 1.151971  [  128/ 3200]\n",
            "loss: 1.037535  [  144/ 3200]\n",
            "loss: 1.065434  [  160/ 3200]\n",
            "loss: 1.089753  [  176/ 3200]\n",
            "loss: 1.134189  [  192/ 3200]\n",
            "loss: 1.105321  [  208/ 3200]\n",
            "loss: 0.969259  [  224/ 3200]\n",
            "loss: 1.251636  [  240/ 3200]\n",
            "loss: 1.143284  [  256/ 3200]\n",
            "loss: 1.258868  [  272/ 3200]\n",
            "loss: 1.086280  [  288/ 3200]\n",
            "loss: 1.132532  [  304/ 3200]\n",
            "loss: 1.123125  [  320/ 3200]\n",
            "loss: 1.028108  [  336/ 3200]\n",
            "loss: 1.098444  [  352/ 3200]\n",
            "loss: 1.088323  [  368/ 3200]\n",
            "loss: 1.141868  [  384/ 3200]\n",
            "loss: 1.190789  [  400/ 3200]\n",
            "loss: 1.132467  [  416/ 3200]\n",
            "loss: 1.258704  [  432/ 3200]\n",
            "loss: 1.222978  [  448/ 3200]\n",
            "loss: 1.082270  [  464/ 3200]\n",
            "loss: 1.136883  [  480/ 3200]\n",
            "loss: 1.038427  [  496/ 3200]\n",
            "loss: 1.124474  [  512/ 3200]\n",
            "loss: 1.091977  [  528/ 3200]\n",
            "loss: 0.975155  [  544/ 3200]\n",
            "loss: 1.037383  [  560/ 3200]\n",
            "loss: 1.067710  [  576/ 3200]\n",
            "loss: 1.182741  [  592/ 3200]\n",
            "loss: 1.169261  [  608/ 3200]\n",
            "loss: 1.030854  [  624/ 3200]\n",
            "loss: 1.132456  [  640/ 3200]\n",
            "loss: 1.080561  [  656/ 3200]\n",
            "loss: 1.124246  [  672/ 3200]\n",
            "loss: 1.223096  [  688/ 3200]\n",
            "loss: 1.255690  [  704/ 3200]\n",
            "loss: 0.957067  [  720/ 3200]\n",
            "loss: 1.142630  [  736/ 3200]\n",
            "loss: 1.175053  [  752/ 3200]\n",
            "loss: 0.984034  [  768/ 3200]\n",
            "loss: 1.199467  [  784/ 3200]\n",
            "loss: 1.145118  [  800/ 3200]\n",
            "loss: 1.174655  [  816/ 3200]\n",
            "loss: 1.236646  [  832/ 3200]\n",
            "loss: 1.164062  [  848/ 3200]\n",
            "loss: 1.113440  [  864/ 3200]\n",
            "loss: 1.259089  [  880/ 3200]\n",
            "loss: 1.076478  [  896/ 3200]\n",
            "loss: 1.239109  [  912/ 3200]\n",
            "loss: 1.253201  [  928/ 3200]\n",
            "loss: 1.252118  [  944/ 3200]\n",
            "loss: 1.059878  [  960/ 3200]\n",
            "loss: 0.989999  [  976/ 3200]\n",
            "loss: 1.132025  [  992/ 3200]\n",
            "loss: 1.108697  [ 1008/ 3200]\n",
            "loss: 1.041831  [ 1024/ 3200]\n",
            "loss: 1.123672  [ 1040/ 3200]\n",
            "loss: 1.014768  [ 1056/ 3200]\n",
            "loss: 1.075986  [ 1072/ 3200]\n",
            "loss: 1.069428  [ 1088/ 3200]\n",
            "loss: 1.221105  [ 1104/ 3200]\n",
            "loss: 1.195330  [ 1120/ 3200]\n",
            "loss: 1.152997  [ 1136/ 3200]\n",
            "loss: 0.984911  [ 1152/ 3200]\n",
            "loss: 1.115614  [ 1168/ 3200]\n",
            "loss: 1.080837  [ 1184/ 3200]\n",
            "loss: 1.095965  [ 1200/ 3200]\n",
            "loss: 1.133759  [ 1216/ 3200]\n",
            "loss: 1.153674  [ 1232/ 3200]\n",
            "loss: 0.932560  [ 1248/ 3200]\n",
            "loss: 1.145960  [ 1264/ 3200]\n",
            "loss: 1.064067  [ 1280/ 3200]\n",
            "loss: 1.153048  [ 1296/ 3200]\n",
            "loss: 1.275417  [ 1312/ 3200]\n",
            "loss: 1.234811  [ 1328/ 3200]\n",
            "loss: 1.052320  [ 1344/ 3200]\n",
            "loss: 1.260758  [ 1360/ 3200]\n",
            "loss: 1.047454  [ 1376/ 3200]\n",
            "loss: 1.056042  [ 1392/ 3200]\n",
            "loss: 1.102463  [ 1408/ 3200]\n",
            "loss: 1.176436  [ 1424/ 3200]\n",
            "loss: 1.086657  [ 1440/ 3200]\n",
            "loss: 0.983996  [ 1456/ 3200]\n",
            "loss: 1.178763  [ 1472/ 3200]\n",
            "loss: 1.107071  [ 1488/ 3200]\n",
            "loss: 1.187707  [ 1504/ 3200]\n",
            "loss: 0.920711  [ 1520/ 3200]\n",
            "loss: 1.234312  [ 1536/ 3200]\n",
            "loss: 1.126554  [ 1552/ 3200]\n",
            "loss: 1.294979  [ 1568/ 3200]\n",
            "loss: 1.064525  [ 1584/ 3200]\n",
            "loss: 1.140967  [ 1600/ 3200]\n",
            "loss: 0.956354  [ 1616/ 3200]\n",
            "loss: 1.107882  [ 1632/ 3200]\n",
            "loss: 0.953957  [ 1648/ 3200]\n",
            "loss: 1.130314  [ 1664/ 3200]\n",
            "loss: 1.099241  [ 1680/ 3200]\n",
            "loss: 1.182038  [ 1696/ 3200]\n",
            "loss: 1.305182  [ 1712/ 3200]\n",
            "loss: 1.223209  [ 1728/ 3200]\n",
            "loss: 1.235317  [ 1744/ 3200]\n",
            "loss: 1.077921  [ 1760/ 3200]\n",
            "loss: 1.077703  [ 1776/ 3200]\n",
            "loss: 1.208529  [ 1792/ 3200]\n",
            "loss: 1.056861  [ 1808/ 3200]\n",
            "loss: 1.138844  [ 1824/ 3200]\n",
            "loss: 1.090034  [ 1840/ 3200]\n",
            "loss: 1.090648  [ 1856/ 3200]\n",
            "loss: 1.195411  [ 1872/ 3200]\n",
            "loss: 1.102867  [ 1888/ 3200]\n",
            "loss: 0.987911  [ 1904/ 3200]\n",
            "loss: 1.139943  [ 1920/ 3200]\n",
            "loss: 0.888277  [ 1936/ 3200]\n",
            "loss: 1.255558  [ 1952/ 3200]\n",
            "loss: 1.032828  [ 1968/ 3200]\n",
            "loss: 1.098860  [ 1984/ 3200]\n",
            "loss: 0.986937  [ 2000/ 3200]\n",
            "loss: 1.033607  [ 2016/ 3200]\n",
            "loss: 1.159236  [ 2032/ 3200]\n",
            "loss: 1.052909  [ 2048/ 3200]\n",
            "loss: 1.032724  [ 2064/ 3200]\n",
            "loss: 1.218278  [ 2080/ 3200]\n",
            "loss: 1.148524  [ 2096/ 3200]\n",
            "loss: 1.134293  [ 2112/ 3200]\n",
            "loss: 1.054832  [ 2128/ 3200]\n",
            "loss: 1.192955  [ 2144/ 3200]\n",
            "loss: 1.167500  [ 2160/ 3200]\n",
            "loss: 1.095240  [ 2176/ 3200]\n",
            "loss: 1.097598  [ 2192/ 3200]\n",
            "loss: 1.054866  [ 2208/ 3200]\n",
            "loss: 0.930480  [ 2224/ 3200]\n",
            "loss: 0.999154  [ 2240/ 3200]\n",
            "loss: 0.997435  [ 2256/ 3200]\n",
            "loss: 1.119285  [ 2272/ 3200]\n",
            "loss: 1.171921  [ 2288/ 3200]\n",
            "loss: 1.149846  [ 2304/ 3200]\n",
            "loss: 1.053769  [ 2320/ 3200]\n",
            "loss: 1.153698  [ 2336/ 3200]\n",
            "loss: 1.204180  [ 2352/ 3200]\n",
            "loss: 0.988296  [ 2368/ 3200]\n",
            "loss: 1.027603  [ 2384/ 3200]\n",
            "loss: 0.936166  [ 2400/ 3200]\n",
            "loss: 1.153026  [ 2416/ 3200]\n",
            "loss: 1.141376  [ 2432/ 3200]\n",
            "loss: 1.168254  [ 2448/ 3200]\n",
            "loss: 1.072974  [ 2464/ 3200]\n",
            "loss: 1.160261  [ 2480/ 3200]\n",
            "loss: 1.178026  [ 2496/ 3200]\n",
            "loss: 0.963565  [ 2512/ 3200]\n",
            "loss: 1.301911  [ 2528/ 3200]\n",
            "loss: 1.209725  [ 2544/ 3200]\n",
            "loss: 1.110692  [ 2560/ 3200]\n",
            "loss: 1.211354  [ 2576/ 3200]\n",
            "loss: 1.110810  [ 2592/ 3200]\n",
            "loss: 1.176122  [ 2608/ 3200]\n",
            "loss: 1.044206  [ 2624/ 3200]\n",
            "loss: 1.050029  [ 2640/ 3200]\n",
            "loss: 1.317126  [ 2656/ 3200]\n",
            "loss: 1.106953  [ 2672/ 3200]\n",
            "loss: 1.074363  [ 2688/ 3200]\n",
            "loss: 1.158531  [ 2704/ 3200]\n",
            "loss: 1.322391  [ 2720/ 3200]\n",
            "loss: 1.039301  [ 2736/ 3200]\n",
            "loss: 1.021281  [ 2752/ 3200]\n",
            "loss: 1.278190  [ 2768/ 3200]\n",
            "loss: 1.107728  [ 2784/ 3200]\n",
            "loss: 1.088315  [ 2800/ 3200]\n",
            "loss: 1.121472  [ 2816/ 3200]\n",
            "loss: 1.085146  [ 2832/ 3200]\n",
            "loss: 1.113728  [ 2848/ 3200]\n",
            "loss: 1.078576  [ 2864/ 3200]\n",
            "loss: 1.003230  [ 2880/ 3200]\n",
            "loss: 1.094769  [ 2896/ 3200]\n",
            "loss: 1.216413  [ 2912/ 3200]\n",
            "loss: 1.184556  [ 2928/ 3200]\n",
            "loss: 1.153795  [ 2944/ 3200]\n",
            "loss: 0.980582  [ 2960/ 3200]\n",
            "loss: 1.114036  [ 2976/ 3200]\n",
            "loss: 1.187649  [ 2992/ 3200]\n",
            "loss: 1.033219  [ 3008/ 3200]\n",
            "loss: 1.053036  [ 3024/ 3200]\n",
            "loss: 0.954473  [ 3040/ 3200]\n",
            "loss: 1.012063  [ 3056/ 3200]\n",
            "loss: 1.100353  [ 3072/ 3200]\n",
            "loss: 1.087400  [ 3088/ 3200]\n",
            "loss: 1.132584  [ 3104/ 3200]\n",
            "loss: 1.124417  [ 3120/ 3200]\n",
            "loss: 1.138093  [ 3136/ 3200]\n",
            "loss: 1.043476  [ 3152/ 3200]\n",
            "loss: 1.157302  [ 3168/ 3200]\n",
            "loss: 1.060931  [ 3184/ 3200]\n",
            "current epoch: 12\n",
            "\n",
            "loss: 1.277085  [    0/ 3200]\n",
            "loss: 0.967095  [   16/ 3200]\n",
            "loss: 0.954846  [   32/ 3200]\n",
            "loss: 1.187333  [   48/ 3200]\n",
            "loss: 1.368378  [   64/ 3200]\n",
            "loss: 1.251320  [   80/ 3200]\n",
            "loss: 1.195836  [   96/ 3200]\n",
            "loss: 1.116409  [  112/ 3200]\n",
            "loss: 1.199389  [  128/ 3200]\n",
            "loss: 0.982976  [  144/ 3200]\n",
            "loss: 1.195006  [  160/ 3200]\n",
            "loss: 1.205391  [  176/ 3200]\n",
            "loss: 1.137408  [  192/ 3200]\n",
            "loss: 1.153312  [  208/ 3200]\n",
            "loss: 1.102897  [  224/ 3200]\n",
            "loss: 1.067377  [  240/ 3200]\n",
            "loss: 1.075002  [  256/ 3200]\n",
            "loss: 1.043904  [  272/ 3200]\n",
            "loss: 1.209484  [  288/ 3200]\n",
            "loss: 0.914575  [  304/ 3200]\n",
            "loss: 1.090811  [  320/ 3200]\n",
            "loss: 0.989660  [  336/ 3200]\n",
            "loss: 1.028517  [  352/ 3200]\n",
            "loss: 0.981695  [  368/ 3200]\n",
            "loss: 1.355557  [  384/ 3200]\n",
            "loss: 1.151256  [  400/ 3200]\n",
            "loss: 1.115397  [  416/ 3200]\n",
            "loss: 1.176309  [  432/ 3200]\n",
            "loss: 1.183035  [  448/ 3200]\n",
            "loss: 1.052397  [  464/ 3200]\n",
            "loss: 1.168577  [  480/ 3200]\n",
            "loss: 1.023135  [  496/ 3200]\n",
            "loss: 1.028655  [  512/ 3200]\n",
            "loss: 1.095426  [  528/ 3200]\n",
            "loss: 0.949612  [  544/ 3200]\n",
            "loss: 1.260419  [  560/ 3200]\n",
            "loss: 1.173543  [  576/ 3200]\n",
            "loss: 1.105699  [  592/ 3200]\n",
            "loss: 1.081258  [  608/ 3200]\n",
            "loss: 0.921927  [  624/ 3200]\n",
            "loss: 1.136592  [  640/ 3200]\n",
            "loss: 1.180535  [  656/ 3200]\n",
            "loss: 0.981744  [  672/ 3200]\n",
            "loss: 0.974662  [  688/ 3200]\n",
            "loss: 1.230965  [  704/ 3200]\n",
            "loss: 1.039107  [  720/ 3200]\n",
            "loss: 1.182025  [  736/ 3200]\n",
            "loss: 1.179802  [  752/ 3200]\n",
            "loss: 1.134933  [  768/ 3200]\n",
            "loss: 1.041341  [  784/ 3200]\n",
            "loss: 1.157852  [  800/ 3200]\n",
            "loss: 1.089399  [  816/ 3200]\n",
            "loss: 1.115543  [  832/ 3200]\n",
            "loss: 1.184506  [  848/ 3200]\n",
            "loss: 1.110319  [  864/ 3200]\n",
            "loss: 1.137698  [  880/ 3200]\n",
            "loss: 1.193827  [  896/ 3200]\n",
            "loss: 1.077665  [  912/ 3200]\n",
            "loss: 1.145215  [  928/ 3200]\n",
            "loss: 1.034518  [  944/ 3200]\n",
            "loss: 1.147735  [  960/ 3200]\n",
            "loss: 0.971189  [  976/ 3200]\n",
            "loss: 1.132709  [  992/ 3200]\n",
            "loss: 0.990499  [ 1008/ 3200]\n",
            "loss: 0.929866  [ 1024/ 3200]\n",
            "loss: 0.964321  [ 1040/ 3200]\n",
            "loss: 1.042289  [ 1056/ 3200]\n",
            "loss: 1.198430  [ 1072/ 3200]\n",
            "loss: 1.023284  [ 1088/ 3200]\n",
            "loss: 1.000729  [ 1104/ 3200]\n",
            "loss: 1.160122  [ 1120/ 3200]\n",
            "loss: 1.139695  [ 1136/ 3200]\n",
            "loss: 1.042216  [ 1152/ 3200]\n",
            "loss: 1.238699  [ 1168/ 3200]\n",
            "loss: 1.117355  [ 1184/ 3200]\n",
            "loss: 1.082156  [ 1200/ 3200]\n",
            "loss: 1.009741  [ 1216/ 3200]\n",
            "loss: 1.071167  [ 1232/ 3200]\n",
            "loss: 1.069312  [ 1248/ 3200]\n",
            "loss: 1.005604  [ 1264/ 3200]\n",
            "loss: 1.217495  [ 1280/ 3200]\n",
            "loss: 1.049809  [ 1296/ 3200]\n",
            "loss: 1.185006  [ 1312/ 3200]\n",
            "loss: 1.188368  [ 1328/ 3200]\n",
            "loss: 1.070689  [ 1344/ 3200]\n",
            "loss: 0.837815  [ 1360/ 3200]\n",
            "loss: 0.919559  [ 1376/ 3200]\n",
            "loss: 0.985180  [ 1392/ 3200]\n",
            "loss: 1.216096  [ 1408/ 3200]\n",
            "loss: 0.994000  [ 1424/ 3200]\n",
            "loss: 1.204209  [ 1440/ 3200]\n",
            "loss: 1.022141  [ 1456/ 3200]\n",
            "loss: 1.052737  [ 1472/ 3200]\n",
            "loss: 0.962403  [ 1488/ 3200]\n",
            "loss: 1.011382  [ 1504/ 3200]\n",
            "loss: 1.173529  [ 1520/ 3200]\n",
            "loss: 1.068497  [ 1536/ 3200]\n",
            "loss: 1.088645  [ 1552/ 3200]\n",
            "loss: 1.160655  [ 1568/ 3200]\n",
            "loss: 1.110275  [ 1584/ 3200]\n",
            "loss: 1.328472  [ 1600/ 3200]\n",
            "loss: 1.017021  [ 1616/ 3200]\n",
            "loss: 1.242867  [ 1632/ 3200]\n",
            "loss: 1.014393  [ 1648/ 3200]\n",
            "loss: 1.154122  [ 1664/ 3200]\n",
            "loss: 1.131079  [ 1680/ 3200]\n",
            "loss: 1.084154  [ 1696/ 3200]\n",
            "loss: 1.085126  [ 1712/ 3200]\n",
            "loss: 1.128284  [ 1728/ 3200]\n",
            "loss: 1.056158  [ 1744/ 3200]\n",
            "loss: 1.159385  [ 1760/ 3200]\n",
            "loss: 1.182768  [ 1776/ 3200]\n",
            "loss: 1.056218  [ 1792/ 3200]\n",
            "loss: 1.085043  [ 1808/ 3200]\n",
            "loss: 1.339569  [ 1824/ 3200]\n",
            "loss: 0.971456  [ 1840/ 3200]\n",
            "loss: 1.098762  [ 1856/ 3200]\n",
            "loss: 0.922082  [ 1872/ 3200]\n",
            "loss: 1.087576  [ 1888/ 3200]\n",
            "loss: 1.078722  [ 1904/ 3200]\n",
            "loss: 1.227320  [ 1920/ 3200]\n",
            "loss: 1.049960  [ 1936/ 3200]\n",
            "loss: 0.949682  [ 1952/ 3200]\n",
            "loss: 0.918592  [ 1968/ 3200]\n",
            "loss: 1.022010  [ 1984/ 3200]\n",
            "loss: 1.083336  [ 2000/ 3200]\n",
            "loss: 1.109222  [ 2016/ 3200]\n",
            "loss: 1.401360  [ 2032/ 3200]\n",
            "loss: 1.130846  [ 2048/ 3200]\n",
            "loss: 0.961261  [ 2064/ 3200]\n",
            "loss: 1.185070  [ 2080/ 3200]\n",
            "loss: 0.952171  [ 2096/ 3200]\n",
            "loss: 0.941142  [ 2112/ 3200]\n",
            "loss: 1.113187  [ 2128/ 3200]\n",
            "loss: 0.921742  [ 2144/ 3200]\n",
            "loss: 0.908694  [ 2160/ 3200]\n",
            "loss: 0.978333  [ 2176/ 3200]\n",
            "loss: 1.031464  [ 2192/ 3200]\n",
            "loss: 1.096789  [ 2208/ 3200]\n",
            "loss: 1.027009  [ 2224/ 3200]\n",
            "loss: 1.133809  [ 2240/ 3200]\n",
            "loss: 1.018510  [ 2256/ 3200]\n",
            "loss: 1.141369  [ 2272/ 3200]\n",
            "loss: 1.077867  [ 2288/ 3200]\n",
            "loss: 1.012581  [ 2304/ 3200]\n",
            "loss: 1.284306  [ 2320/ 3200]\n",
            "loss: 1.179348  [ 2336/ 3200]\n",
            "loss: 0.966794  [ 2352/ 3200]\n",
            "loss: 1.188016  [ 2368/ 3200]\n",
            "loss: 1.206767  [ 2384/ 3200]\n",
            "loss: 1.269917  [ 2400/ 3200]\n",
            "loss: 0.952815  [ 2416/ 3200]\n",
            "loss: 1.107118  [ 2432/ 3200]\n",
            "loss: 1.117862  [ 2448/ 3200]\n",
            "loss: 1.207732  [ 2464/ 3200]\n",
            "loss: 0.993878  [ 2480/ 3200]\n",
            "loss: 1.136111  [ 2496/ 3200]\n",
            "loss: 1.019844  [ 2512/ 3200]\n",
            "loss: 1.119065  [ 2528/ 3200]\n",
            "loss: 1.116340  [ 2544/ 3200]\n",
            "loss: 1.024495  [ 2560/ 3200]\n",
            "loss: 1.151250  [ 2576/ 3200]\n",
            "loss: 1.028570  [ 2592/ 3200]\n",
            "loss: 1.030100  [ 2608/ 3200]\n",
            "loss: 1.139855  [ 2624/ 3200]\n",
            "loss: 1.087754  [ 2640/ 3200]\n",
            "loss: 1.134629  [ 2656/ 3200]\n",
            "loss: 1.157596  [ 2672/ 3200]\n",
            "loss: 1.234450  [ 2688/ 3200]\n",
            "loss: 1.176745  [ 2704/ 3200]\n",
            "loss: 1.142685  [ 2720/ 3200]\n",
            "loss: 0.958447  [ 2736/ 3200]\n",
            "loss: 0.864234  [ 2752/ 3200]\n",
            "loss: 1.031189  [ 2768/ 3200]\n",
            "loss: 1.053919  [ 2784/ 3200]\n",
            "loss: 1.240894  [ 2800/ 3200]\n",
            "loss: 0.950721  [ 2816/ 3200]\n",
            "loss: 1.048144  [ 2832/ 3200]\n",
            "loss: 1.068013  [ 2848/ 3200]\n",
            "loss: 1.021762  [ 2864/ 3200]\n",
            "loss: 1.025176  [ 2880/ 3200]\n",
            "loss: 0.987296  [ 2896/ 3200]\n",
            "loss: 1.129244  [ 2912/ 3200]\n",
            "loss: 1.117330  [ 2928/ 3200]\n",
            "loss: 1.093020  [ 2944/ 3200]\n",
            "loss: 0.998667  [ 2960/ 3200]\n",
            "loss: 1.127715  [ 2976/ 3200]\n",
            "loss: 1.084438  [ 2992/ 3200]\n",
            "loss: 0.992437  [ 3008/ 3200]\n",
            "loss: 1.131456  [ 3024/ 3200]\n",
            "loss: 1.096590  [ 3040/ 3200]\n",
            "loss: 0.937296  [ 3056/ 3200]\n",
            "loss: 0.953673  [ 3072/ 3200]\n",
            "loss: 0.999393  [ 3088/ 3200]\n",
            "loss: 1.042106  [ 3104/ 3200]\n",
            "loss: 1.124460  [ 3120/ 3200]\n",
            "loss: 0.990540  [ 3136/ 3200]\n",
            "loss: 1.226707  [ 3152/ 3200]\n",
            "loss: 1.067765  [ 3168/ 3200]\n",
            "loss: 0.998160  [ 3184/ 3200]\n",
            "current epoch: 13\n",
            "\n",
            "loss: 1.299815  [    0/ 3200]\n",
            "loss: 0.997748  [   16/ 3200]\n",
            "loss: 0.953665  [   32/ 3200]\n",
            "loss: 1.002863  [   48/ 3200]\n",
            "loss: 1.068355  [   64/ 3200]\n",
            "loss: 1.199818  [   80/ 3200]\n",
            "loss: 1.203971  [   96/ 3200]\n",
            "loss: 1.094226  [  112/ 3200]\n",
            "loss: 1.094152  [  128/ 3200]\n",
            "loss: 1.127772  [  144/ 3200]\n",
            "loss: 1.174941  [  160/ 3200]\n",
            "loss: 1.137703  [  176/ 3200]\n",
            "loss: 1.063088  [  192/ 3200]\n",
            "loss: 1.067602  [  208/ 3200]\n",
            "loss: 1.129396  [  224/ 3200]\n",
            "loss: 0.938409  [  240/ 3200]\n",
            "loss: 0.953592  [  256/ 3200]\n",
            "loss: 0.983238  [  272/ 3200]\n",
            "loss: 0.910830  [  288/ 3200]\n",
            "loss: 1.204195  [  304/ 3200]\n",
            "loss: 1.041516  [  320/ 3200]\n",
            "loss: 1.139402  [  336/ 3200]\n",
            "loss: 0.900555  [  352/ 3200]\n",
            "loss: 1.352921  [  368/ 3200]\n",
            "loss: 1.021781  [  384/ 3200]\n",
            "loss: 1.207184  [  400/ 3200]\n",
            "loss: 1.032460  [  416/ 3200]\n",
            "loss: 1.011933  [  432/ 3200]\n",
            "loss: 0.905231  [  448/ 3200]\n",
            "loss: 1.031995  [  464/ 3200]\n",
            "loss: 1.253601  [  480/ 3200]\n",
            "loss: 1.138996  [  496/ 3200]\n",
            "loss: 1.051289  [  512/ 3200]\n",
            "loss: 1.155801  [  528/ 3200]\n",
            "loss: 1.116156  [  544/ 3200]\n",
            "loss: 0.971348  [  560/ 3200]\n",
            "loss: 0.981313  [  576/ 3200]\n",
            "loss: 1.055161  [  592/ 3200]\n",
            "loss: 1.058484  [  608/ 3200]\n",
            "loss: 1.198504  [  624/ 3200]\n",
            "loss: 1.067135  [  640/ 3200]\n",
            "loss: 1.037042  [  656/ 3200]\n",
            "loss: 1.151009  [  672/ 3200]\n",
            "loss: 1.070489  [  688/ 3200]\n",
            "loss: 0.990823  [  704/ 3200]\n",
            "loss: 1.135953  [  720/ 3200]\n",
            "loss: 1.122164  [  736/ 3200]\n",
            "loss: 0.964855  [  752/ 3200]\n",
            "loss: 1.044058  [  768/ 3200]\n",
            "loss: 1.115795  [  784/ 3200]\n",
            "loss: 1.136080  [  800/ 3200]\n",
            "loss: 1.232005  [  816/ 3200]\n",
            "loss: 1.071398  [  832/ 3200]\n",
            "loss: 0.929548  [  848/ 3200]\n",
            "loss: 1.183596  [  864/ 3200]\n",
            "loss: 1.054314  [  880/ 3200]\n",
            "loss: 0.969730  [  896/ 3200]\n",
            "loss: 1.054681  [  912/ 3200]\n",
            "loss: 1.284310  [  928/ 3200]\n",
            "loss: 1.051102  [  944/ 3200]\n",
            "loss: 0.985918  [  960/ 3200]\n",
            "loss: 1.169592  [  976/ 3200]\n",
            "loss: 1.150826  [  992/ 3200]\n",
            "loss: 1.220458  [ 1008/ 3200]\n",
            "loss: 1.121957  [ 1024/ 3200]\n",
            "loss: 1.228933  [ 1040/ 3200]\n",
            "loss: 1.158287  [ 1056/ 3200]\n",
            "loss: 1.143941  [ 1072/ 3200]\n",
            "loss: 0.997637  [ 1088/ 3200]\n",
            "loss: 0.895488  [ 1104/ 3200]\n",
            "loss: 1.129855  [ 1120/ 3200]\n",
            "loss: 1.032887  [ 1136/ 3200]\n",
            "loss: 0.975520  [ 1152/ 3200]\n",
            "loss: 0.871762  [ 1168/ 3200]\n",
            "loss: 1.089227  [ 1184/ 3200]\n",
            "loss: 1.212799  [ 1200/ 3200]\n",
            "loss: 1.008923  [ 1216/ 3200]\n",
            "loss: 1.031062  [ 1232/ 3200]\n",
            "loss: 1.159113  [ 1248/ 3200]\n",
            "loss: 1.069157  [ 1264/ 3200]\n",
            "loss: 1.025579  [ 1280/ 3200]\n",
            "loss: 1.188322  [ 1296/ 3200]\n",
            "loss: 1.054102  [ 1312/ 3200]\n",
            "loss: 1.256868  [ 1328/ 3200]\n",
            "loss: 1.011621  [ 1344/ 3200]\n",
            "loss: 1.210192  [ 1360/ 3200]\n",
            "loss: 0.868530  [ 1376/ 3200]\n",
            "loss: 0.964946  [ 1392/ 3200]\n",
            "loss: 1.078591  [ 1408/ 3200]\n",
            "loss: 0.876674  [ 1424/ 3200]\n",
            "loss: 1.127987  [ 1440/ 3200]\n",
            "loss: 1.136082  [ 1456/ 3200]\n",
            "loss: 1.140211  [ 1472/ 3200]\n",
            "loss: 0.936508  [ 1488/ 3200]\n",
            "loss: 0.948942  [ 1504/ 3200]\n",
            "loss: 0.887374  [ 1520/ 3200]\n",
            "loss: 1.020666  [ 1536/ 3200]\n",
            "loss: 1.025267  [ 1552/ 3200]\n",
            "loss: 1.311989  [ 1568/ 3200]\n",
            "loss: 1.078761  [ 1584/ 3200]\n",
            "loss: 1.089863  [ 1600/ 3200]\n",
            "loss: 1.121858  [ 1616/ 3200]\n",
            "loss: 1.108238  [ 1632/ 3200]\n",
            "loss: 1.140687  [ 1648/ 3200]\n",
            "loss: 0.891564  [ 1664/ 3200]\n",
            "loss: 0.950575  [ 1680/ 3200]\n",
            "loss: 1.016335  [ 1696/ 3200]\n",
            "loss: 1.043019  [ 1712/ 3200]\n",
            "loss: 1.161572  [ 1728/ 3200]\n",
            "loss: 0.974042  [ 1744/ 3200]\n",
            "loss: 0.895400  [ 1760/ 3200]\n",
            "loss: 0.966232  [ 1776/ 3200]\n",
            "loss: 1.248818  [ 1792/ 3200]\n",
            "loss: 1.082556  [ 1808/ 3200]\n",
            "loss: 1.109634  [ 1824/ 3200]\n",
            "loss: 1.337351  [ 1840/ 3200]\n",
            "loss: 1.323886  [ 1856/ 3200]\n",
            "loss: 1.195637  [ 1872/ 3200]\n",
            "loss: 0.980798  [ 1888/ 3200]\n",
            "loss: 0.939277  [ 1904/ 3200]\n",
            "loss: 1.203014  [ 1920/ 3200]\n",
            "loss: 0.900690  [ 1936/ 3200]\n",
            "loss: 1.044207  [ 1952/ 3200]\n",
            "loss: 1.218435  [ 1968/ 3200]\n",
            "loss: 1.072864  [ 1984/ 3200]\n",
            "loss: 0.996194  [ 2000/ 3200]\n",
            "loss: 1.000127  [ 2016/ 3200]\n",
            "loss: 0.965940  [ 2032/ 3200]\n",
            "loss: 0.984239  [ 2048/ 3200]\n",
            "loss: 1.157680  [ 2064/ 3200]\n",
            "loss: 1.192168  [ 2080/ 3200]\n",
            "loss: 0.994832  [ 2096/ 3200]\n",
            "loss: 0.964339  [ 2112/ 3200]\n",
            "loss: 1.064501  [ 2128/ 3200]\n",
            "loss: 1.187992  [ 2144/ 3200]\n",
            "loss: 1.112010  [ 2160/ 3200]\n",
            "loss: 1.234383  [ 2176/ 3200]\n",
            "loss: 1.188387  [ 2192/ 3200]\n",
            "loss: 1.065333  [ 2208/ 3200]\n",
            "loss: 1.128237  [ 2224/ 3200]\n",
            "loss: 0.946592  [ 2240/ 3200]\n",
            "loss: 1.193581  [ 2256/ 3200]\n",
            "loss: 0.975335  [ 2272/ 3200]\n",
            "loss: 1.060872  [ 2288/ 3200]\n",
            "loss: 1.161988  [ 2304/ 3200]\n",
            "loss: 1.146200  [ 2320/ 3200]\n",
            "loss: 1.020474  [ 2336/ 3200]\n",
            "loss: 0.998634  [ 2352/ 3200]\n",
            "loss: 1.011107  [ 2368/ 3200]\n",
            "loss: 1.237247  [ 2384/ 3200]\n",
            "loss: 1.071782  [ 2400/ 3200]\n",
            "loss: 1.045339  [ 2416/ 3200]\n",
            "loss: 1.026130  [ 2432/ 3200]\n",
            "loss: 1.016131  [ 2448/ 3200]\n",
            "loss: 1.004478  [ 2464/ 3200]\n",
            "loss: 0.984000  [ 2480/ 3200]\n",
            "loss: 1.142914  [ 2496/ 3200]\n",
            "loss: 1.033093  [ 2512/ 3200]\n",
            "loss: 0.952683  [ 2528/ 3200]\n",
            "loss: 1.070326  [ 2544/ 3200]\n",
            "loss: 1.136908  [ 2560/ 3200]\n",
            "loss: 1.149111  [ 2576/ 3200]\n",
            "loss: 0.909583  [ 2592/ 3200]\n",
            "loss: 0.969263  [ 2608/ 3200]\n",
            "loss: 1.169940  [ 2624/ 3200]\n",
            "loss: 1.079853  [ 2640/ 3200]\n",
            "loss: 0.766041  [ 2656/ 3200]\n",
            "loss: 1.147041  [ 2672/ 3200]\n",
            "loss: 1.064126  [ 2688/ 3200]\n",
            "loss: 1.112402  [ 2704/ 3200]\n",
            "loss: 0.991239  [ 2720/ 3200]\n",
            "loss: 1.138237  [ 2736/ 3200]\n",
            "loss: 1.046768  [ 2752/ 3200]\n",
            "loss: 0.938439  [ 2768/ 3200]\n",
            "loss: 1.023490  [ 2784/ 3200]\n",
            "loss: 1.042677  [ 2800/ 3200]\n",
            "loss: 0.997868  [ 2816/ 3200]\n",
            "loss: 1.122472  [ 2832/ 3200]\n",
            "loss: 1.183379  [ 2848/ 3200]\n",
            "loss: 1.168421  [ 2864/ 3200]\n",
            "loss: 1.060955  [ 2880/ 3200]\n",
            "loss: 1.137649  [ 2896/ 3200]\n",
            "loss: 0.971296  [ 2912/ 3200]\n",
            "loss: 1.144844  [ 2928/ 3200]\n",
            "loss: 1.244854  [ 2944/ 3200]\n",
            "loss: 0.890522  [ 2960/ 3200]\n",
            "loss: 1.135698  [ 2976/ 3200]\n",
            "loss: 0.915116  [ 2992/ 3200]\n",
            "loss: 0.996829  [ 3008/ 3200]\n",
            "loss: 1.040782  [ 3024/ 3200]\n",
            "loss: 0.825538  [ 3040/ 3200]\n",
            "loss: 0.852367  [ 3056/ 3200]\n",
            "loss: 1.195114  [ 3072/ 3200]\n",
            "loss: 1.025738  [ 3088/ 3200]\n",
            "loss: 1.141250  [ 3104/ 3200]\n",
            "loss: 1.045019  [ 3120/ 3200]\n",
            "loss: 0.799979  [ 3136/ 3200]\n",
            "loss: 0.815344  [ 3152/ 3200]\n",
            "loss: 1.076131  [ 3168/ 3200]\n",
            "loss: 1.084066  [ 3184/ 3200]\n",
            "current epoch: 14\n",
            "\n",
            "loss: 1.239220  [    0/ 3200]\n",
            "loss: 1.087171  [   16/ 3200]\n",
            "loss: 1.100645  [   32/ 3200]\n",
            "loss: 1.296852  [   48/ 3200]\n",
            "loss: 0.984823  [   64/ 3200]\n",
            "loss: 1.100588  [   80/ 3200]\n",
            "loss: 1.065885  [   96/ 3200]\n",
            "loss: 1.233599  [  112/ 3200]\n",
            "loss: 1.265861  [  128/ 3200]\n",
            "loss: 1.008230  [  144/ 3200]\n",
            "loss: 1.060649  [  160/ 3200]\n",
            "loss: 0.956989  [  176/ 3200]\n",
            "loss: 1.102621  [  192/ 3200]\n",
            "loss: 0.966651  [  208/ 3200]\n",
            "loss: 0.983194  [  224/ 3200]\n",
            "loss: 1.194862  [  240/ 3200]\n",
            "loss: 0.982408  [  256/ 3200]\n",
            "loss: 1.065694  [  272/ 3200]\n",
            "loss: 1.143283  [  288/ 3200]\n",
            "loss: 1.030957  [  304/ 3200]\n",
            "loss: 0.992440  [  320/ 3200]\n",
            "loss: 1.120954  [  336/ 3200]\n",
            "loss: 1.139313  [  352/ 3200]\n",
            "loss: 0.966829  [  368/ 3200]\n",
            "loss: 0.994154  [  384/ 3200]\n",
            "loss: 0.864342  [  400/ 3200]\n",
            "loss: 1.094740  [  416/ 3200]\n",
            "loss: 0.963341  [  432/ 3200]\n",
            "loss: 1.077254  [  448/ 3200]\n",
            "loss: 1.185495  [  464/ 3200]\n",
            "loss: 1.033333  [  480/ 3200]\n",
            "loss: 1.045658  [  496/ 3200]\n",
            "loss: 1.030045  [  512/ 3200]\n",
            "loss: 1.127930  [  528/ 3200]\n",
            "loss: 0.954837  [  544/ 3200]\n",
            "loss: 1.183732  [  560/ 3200]\n",
            "loss: 1.070434  [  576/ 3200]\n",
            "loss: 0.999685  [  592/ 3200]\n",
            "loss: 1.231014  [  608/ 3200]\n",
            "loss: 1.178855  [  624/ 3200]\n",
            "loss: 1.035924  [  640/ 3200]\n",
            "loss: 1.236893  [  656/ 3200]\n",
            "loss: 1.201749  [  672/ 3200]\n",
            "loss: 1.131886  [  688/ 3200]\n",
            "loss: 0.872596  [  704/ 3200]\n",
            "loss: 1.035260  [  720/ 3200]\n",
            "loss: 0.841788  [  736/ 3200]\n",
            "loss: 0.988175  [  752/ 3200]\n",
            "loss: 0.970579  [  768/ 3200]\n",
            "loss: 1.019762  [  784/ 3200]\n",
            "loss: 0.955365  [  800/ 3200]\n",
            "loss: 1.238777  [  816/ 3200]\n",
            "loss: 1.020766  [  832/ 3200]\n",
            "loss: 0.972332  [  848/ 3200]\n",
            "loss: 1.045375  [  864/ 3200]\n",
            "loss: 1.058825  [  880/ 3200]\n",
            "loss: 1.016063  [  896/ 3200]\n",
            "loss: 0.980223  [  912/ 3200]\n",
            "loss: 1.059622  [  928/ 3200]\n",
            "loss: 1.185409  [  944/ 3200]\n",
            "loss: 0.951360  [  960/ 3200]\n",
            "loss: 1.104847  [  976/ 3200]\n",
            "loss: 1.173828  [  992/ 3200]\n",
            "loss: 1.096442  [ 1008/ 3200]\n",
            "loss: 0.792529  [ 1024/ 3200]\n",
            "loss: 1.124766  [ 1040/ 3200]\n",
            "loss: 1.173667  [ 1056/ 3200]\n",
            "loss: 1.021207  [ 1072/ 3200]\n",
            "loss: 0.992121  [ 1088/ 3200]\n",
            "loss: 1.274843  [ 1104/ 3200]\n",
            "loss: 1.231268  [ 1120/ 3200]\n",
            "loss: 0.999051  [ 1136/ 3200]\n",
            "loss: 0.904245  [ 1152/ 3200]\n",
            "loss: 0.898544  [ 1168/ 3200]\n",
            "loss: 0.997542  [ 1184/ 3200]\n",
            "loss: 1.386887  [ 1200/ 3200]\n",
            "loss: 0.836849  [ 1216/ 3200]\n",
            "loss: 1.029282  [ 1232/ 3200]\n",
            "loss: 1.092977  [ 1248/ 3200]\n",
            "loss: 0.948679  [ 1264/ 3200]\n",
            "loss: 0.800725  [ 1280/ 3200]\n",
            "loss: 0.942421  [ 1296/ 3200]\n",
            "loss: 1.047354  [ 1312/ 3200]\n",
            "loss: 1.062474  [ 1328/ 3200]\n",
            "loss: 1.103691  [ 1344/ 3200]\n",
            "loss: 1.081704  [ 1360/ 3200]\n",
            "loss: 1.034885  [ 1376/ 3200]\n",
            "loss: 1.102764  [ 1392/ 3200]\n",
            "loss: 0.857524  [ 1408/ 3200]\n",
            "loss: 1.041554  [ 1424/ 3200]\n",
            "loss: 1.090923  [ 1440/ 3200]\n",
            "loss: 1.333754  [ 1456/ 3200]\n",
            "loss: 0.879632  [ 1472/ 3200]\n",
            "loss: 1.169615  [ 1488/ 3200]\n",
            "loss: 1.011049  [ 1504/ 3200]\n",
            "loss: 0.921592  [ 1520/ 3200]\n",
            "loss: 0.913978  [ 1536/ 3200]\n",
            "loss: 1.079987  [ 1552/ 3200]\n",
            "loss: 0.886984  [ 1568/ 3200]\n",
            "loss: 1.088352  [ 1584/ 3200]\n",
            "loss: 1.098558  [ 1600/ 3200]\n",
            "loss: 1.192029  [ 1616/ 3200]\n",
            "loss: 0.881275  [ 1632/ 3200]\n",
            "loss: 0.935023  [ 1648/ 3200]\n",
            "loss: 1.038733  [ 1664/ 3200]\n",
            "loss: 1.139324  [ 1680/ 3200]\n",
            "loss: 1.156824  [ 1696/ 3200]\n",
            "loss: 1.131592  [ 1712/ 3200]\n",
            "loss: 0.967032  [ 1728/ 3200]\n",
            "loss: 1.015183  [ 1744/ 3200]\n",
            "loss: 1.183386  [ 1760/ 3200]\n",
            "loss: 1.110227  [ 1776/ 3200]\n",
            "loss: 1.058261  [ 1792/ 3200]\n",
            "loss: 1.045986  [ 1808/ 3200]\n",
            "loss: 1.094259  [ 1824/ 3200]\n",
            "loss: 1.048157  [ 1840/ 3200]\n",
            "loss: 0.945083  [ 1856/ 3200]\n",
            "loss: 1.082137  [ 1872/ 3200]\n",
            "loss: 0.942121  [ 1888/ 3200]\n",
            "loss: 1.037535  [ 1904/ 3200]\n",
            "loss: 1.121413  [ 1920/ 3200]\n",
            "loss: 1.290334  [ 1936/ 3200]\n",
            "loss: 1.360574  [ 1952/ 3200]\n",
            "loss: 0.882955  [ 1968/ 3200]\n",
            "loss: 1.270896  [ 1984/ 3200]\n",
            "loss: 0.942060  [ 2000/ 3200]\n",
            "loss: 0.890144  [ 2016/ 3200]\n",
            "loss: 0.929051  [ 2032/ 3200]\n",
            "loss: 0.936148  [ 2048/ 3200]\n",
            "loss: 1.255183  [ 2064/ 3200]\n",
            "loss: 1.207957  [ 2080/ 3200]\n",
            "loss: 1.265395  [ 2096/ 3200]\n",
            "loss: 1.164441  [ 2112/ 3200]\n",
            "loss: 0.843262  [ 2128/ 3200]\n",
            "loss: 0.835401  [ 2144/ 3200]\n",
            "loss: 0.902218  [ 2160/ 3200]\n",
            "loss: 1.080339  [ 2176/ 3200]\n",
            "loss: 1.131547  [ 2192/ 3200]\n",
            "loss: 1.149682  [ 2208/ 3200]\n",
            "loss: 0.986082  [ 2224/ 3200]\n",
            "loss: 0.949488  [ 2240/ 3200]\n",
            "loss: 1.035979  [ 2256/ 3200]\n",
            "loss: 1.127258  [ 2272/ 3200]\n",
            "loss: 0.828006  [ 2288/ 3200]\n",
            "loss: 1.247496  [ 2304/ 3200]\n",
            "loss: 1.182422  [ 2320/ 3200]\n",
            "loss: 1.120589  [ 2336/ 3200]\n",
            "loss: 0.971180  [ 2352/ 3200]\n",
            "loss: 0.900786  [ 2368/ 3200]\n",
            "loss: 0.806049  [ 2384/ 3200]\n",
            "loss: 0.820255  [ 2400/ 3200]\n",
            "loss: 1.214828  [ 2416/ 3200]\n",
            "loss: 0.984579  [ 2432/ 3200]\n",
            "loss: 1.033180  [ 2448/ 3200]\n",
            "loss: 0.803723  [ 2464/ 3200]\n",
            "loss: 1.173050  [ 2480/ 3200]\n",
            "loss: 1.060812  [ 2496/ 3200]\n",
            "loss: 1.004781  [ 2512/ 3200]\n",
            "loss: 1.033806  [ 2528/ 3200]\n",
            "loss: 1.147362  [ 2544/ 3200]\n",
            "loss: 1.182975  [ 2560/ 3200]\n",
            "loss: 0.865455  [ 2576/ 3200]\n",
            "loss: 0.969825  [ 2592/ 3200]\n",
            "loss: 1.056137  [ 2608/ 3200]\n",
            "loss: 1.174585  [ 2624/ 3200]\n",
            "loss: 1.201468  [ 2640/ 3200]\n",
            "loss: 1.088362  [ 2656/ 3200]\n",
            "loss: 1.039768  [ 2672/ 3200]\n",
            "loss: 0.930952  [ 2688/ 3200]\n",
            "loss: 1.149609  [ 2704/ 3200]\n",
            "loss: 0.822681  [ 2720/ 3200]\n",
            "loss: 1.015643  [ 2736/ 3200]\n",
            "loss: 1.010076  [ 2752/ 3200]\n",
            "loss: 0.991889  [ 2768/ 3200]\n",
            "loss: 1.294323  [ 2784/ 3200]\n",
            "loss: 0.847312  [ 2800/ 3200]\n",
            "loss: 0.963886  [ 2816/ 3200]\n",
            "loss: 0.948095  [ 2832/ 3200]\n",
            "loss: 0.932517  [ 2848/ 3200]\n",
            "loss: 1.178778  [ 2864/ 3200]\n",
            "loss: 0.958546  [ 2880/ 3200]\n",
            "loss: 0.942759  [ 2896/ 3200]\n",
            "loss: 0.821323  [ 2912/ 3200]\n",
            "loss: 0.980510  [ 2928/ 3200]\n",
            "loss: 1.150503  [ 2944/ 3200]\n",
            "loss: 0.892691  [ 2960/ 3200]\n",
            "loss: 1.268080  [ 2976/ 3200]\n",
            "loss: 1.063983  [ 2992/ 3200]\n",
            "loss: 1.049497  [ 3008/ 3200]\n",
            "loss: 1.068055  [ 3024/ 3200]\n",
            "loss: 1.056967  [ 3040/ 3200]\n",
            "loss: 1.141318  [ 3056/ 3200]\n",
            "loss: 0.978759  [ 3072/ 3200]\n",
            "loss: 1.005340  [ 3088/ 3200]\n",
            "loss: 1.187292  [ 3104/ 3200]\n",
            "loss: 0.971219  [ 3120/ 3200]\n",
            "loss: 1.043406  [ 3136/ 3200]\n",
            "loss: 0.967928  [ 3152/ 3200]\n",
            "loss: 1.124650  [ 3168/ 3200]\n",
            "loss: 1.117774  [ 3184/ 3200]\n",
            "current epoch: 15\n",
            "\n",
            "loss: 0.941866  [    0/ 3200]\n",
            "loss: 1.177792  [   16/ 3200]\n",
            "loss: 0.955369  [   32/ 3200]\n",
            "loss: 0.826292  [   48/ 3200]\n",
            "loss: 0.983240  [   64/ 3200]\n",
            "loss: 1.229580  [   80/ 3200]\n",
            "loss: 1.044019  [   96/ 3200]\n",
            "loss: 1.025317  [  112/ 3200]\n",
            "loss: 1.125748  [  128/ 3200]\n",
            "loss: 1.118372  [  144/ 3200]\n",
            "loss: 0.845645  [  160/ 3200]\n",
            "loss: 1.156388  [  176/ 3200]\n",
            "loss: 1.068975  [  192/ 3200]\n",
            "loss: 0.844001  [  208/ 3200]\n",
            "loss: 0.978239  [  224/ 3200]\n",
            "loss: 0.889128  [  240/ 3200]\n",
            "loss: 0.918572  [  256/ 3200]\n",
            "loss: 1.092689  [  272/ 3200]\n",
            "loss: 0.950043  [  288/ 3200]\n",
            "loss: 1.024574  [  304/ 3200]\n",
            "loss: 0.946642  [  320/ 3200]\n",
            "loss: 1.167426  [  336/ 3200]\n",
            "loss: 1.127773  [  352/ 3200]\n",
            "loss: 1.035482  [  368/ 3200]\n",
            "loss: 0.992997  [  384/ 3200]\n",
            "loss: 1.044981  [  400/ 3200]\n",
            "loss: 0.956723  [  416/ 3200]\n",
            "loss: 1.093078  [  432/ 3200]\n",
            "loss: 0.952541  [  448/ 3200]\n",
            "loss: 1.082915  [  464/ 3200]\n",
            "loss: 0.883656  [  480/ 3200]\n",
            "loss: 1.068811  [  496/ 3200]\n",
            "loss: 1.191786  [  512/ 3200]\n",
            "loss: 0.955751  [  528/ 3200]\n",
            "loss: 0.923538  [  544/ 3200]\n",
            "loss: 1.065818  [  560/ 3200]\n",
            "loss: 0.907165  [  576/ 3200]\n",
            "loss: 1.255696  [  592/ 3200]\n",
            "loss: 0.981361  [  608/ 3200]\n",
            "loss: 0.942444  [  624/ 3200]\n",
            "loss: 0.964969  [  640/ 3200]\n",
            "loss: 0.916486  [  656/ 3200]\n",
            "loss: 0.968879  [  672/ 3200]\n",
            "loss: 0.868812  [  688/ 3200]\n",
            "loss: 1.078987  [  704/ 3200]\n",
            "loss: 0.972890  [  720/ 3200]\n",
            "loss: 1.201808  [  736/ 3200]\n",
            "loss: 1.001954  [  752/ 3200]\n",
            "loss: 0.974450  [  768/ 3200]\n",
            "loss: 0.791374  [  784/ 3200]\n",
            "loss: 1.382687  [  800/ 3200]\n",
            "loss: 1.001751  [  816/ 3200]\n",
            "loss: 1.025644  [  832/ 3200]\n",
            "loss: 1.155480  [  848/ 3200]\n",
            "loss: 1.006628  [  864/ 3200]\n",
            "loss: 0.938311  [  880/ 3200]\n",
            "loss: 1.010001  [  896/ 3200]\n",
            "loss: 1.098233  [  912/ 3200]\n",
            "loss: 1.070636  [  928/ 3200]\n",
            "loss: 1.143206  [  944/ 3200]\n",
            "loss: 0.960582  [  960/ 3200]\n",
            "loss: 1.039466  [  976/ 3200]\n",
            "loss: 0.912293  [  992/ 3200]\n",
            "loss: 1.166898  [ 1008/ 3200]\n",
            "loss: 0.910975  [ 1024/ 3200]\n",
            "loss: 1.034907  [ 1040/ 3200]\n",
            "loss: 1.150633  [ 1056/ 3200]\n",
            "loss: 1.009030  [ 1072/ 3200]\n",
            "loss: 0.982658  [ 1088/ 3200]\n",
            "loss: 1.004610  [ 1104/ 3200]\n",
            "loss: 0.920311  [ 1120/ 3200]\n",
            "loss: 1.105353  [ 1136/ 3200]\n",
            "loss: 0.880703  [ 1152/ 3200]\n",
            "loss: 1.166328  [ 1168/ 3200]\n",
            "loss: 1.046272  [ 1184/ 3200]\n",
            "loss: 1.026052  [ 1200/ 3200]\n",
            "loss: 0.924754  [ 1216/ 3200]\n",
            "loss: 0.925704  [ 1232/ 3200]\n",
            "loss: 1.240433  [ 1248/ 3200]\n",
            "loss: 1.025315  [ 1264/ 3200]\n",
            "loss: 0.866417  [ 1280/ 3200]\n",
            "loss: 1.075580  [ 1296/ 3200]\n",
            "loss: 1.067314  [ 1312/ 3200]\n",
            "loss: 1.010536  [ 1328/ 3200]\n",
            "loss: 1.087016  [ 1344/ 3200]\n",
            "loss: 0.847085  [ 1360/ 3200]\n",
            "loss: 1.045477  [ 1376/ 3200]\n",
            "loss: 1.006418  [ 1392/ 3200]\n",
            "loss: 1.072853  [ 1408/ 3200]\n",
            "loss: 0.872621  [ 1424/ 3200]\n",
            "loss: 0.909961  [ 1440/ 3200]\n",
            "loss: 0.950150  [ 1456/ 3200]\n",
            "loss: 1.078412  [ 1472/ 3200]\n",
            "loss: 1.144270  [ 1488/ 3200]\n",
            "loss: 1.087779  [ 1504/ 3200]\n",
            "loss: 1.328806  [ 1520/ 3200]\n",
            "loss: 1.209543  [ 1536/ 3200]\n",
            "loss: 1.231086  [ 1552/ 3200]\n",
            "loss: 0.993884  [ 1568/ 3200]\n",
            "loss: 1.271814  [ 1584/ 3200]\n",
            "loss: 0.938572  [ 1600/ 3200]\n",
            "loss: 0.903872  [ 1616/ 3200]\n",
            "loss: 0.926771  [ 1632/ 3200]\n",
            "loss: 1.028782  [ 1648/ 3200]\n",
            "loss: 1.036025  [ 1664/ 3200]\n",
            "loss: 0.912338  [ 1680/ 3200]\n",
            "loss: 1.236546  [ 1696/ 3200]\n",
            "loss: 0.862375  [ 1712/ 3200]\n",
            "loss: 1.091260  [ 1728/ 3200]\n",
            "loss: 0.999840  [ 1744/ 3200]\n",
            "loss: 1.045092  [ 1760/ 3200]\n",
            "loss: 1.106209  [ 1776/ 3200]\n",
            "loss: 0.975131  [ 1792/ 3200]\n",
            "loss: 0.922487  [ 1808/ 3200]\n",
            "loss: 1.203314  [ 1824/ 3200]\n",
            "loss: 1.030959  [ 1840/ 3200]\n",
            "loss: 1.091725  [ 1856/ 3200]\n",
            "loss: 1.006002  [ 1872/ 3200]\n",
            "loss: 0.924626  [ 1888/ 3200]\n",
            "loss: 1.049531  [ 1904/ 3200]\n",
            "loss: 0.902907  [ 1920/ 3200]\n",
            "loss: 1.193875  [ 1936/ 3200]\n",
            "loss: 1.009767  [ 1952/ 3200]\n",
            "loss: 1.174903  [ 1968/ 3200]\n",
            "loss: 0.931322  [ 1984/ 3200]\n",
            "loss: 0.916611  [ 2000/ 3200]\n",
            "loss: 0.925625  [ 2016/ 3200]\n",
            "loss: 1.281801  [ 2032/ 3200]\n",
            "loss: 0.895259  [ 2048/ 3200]\n",
            "loss: 1.026714  [ 2064/ 3200]\n",
            "loss: 1.012627  [ 2080/ 3200]\n",
            "loss: 0.802133  [ 2096/ 3200]\n",
            "loss: 0.955768  [ 2112/ 3200]\n",
            "loss: 1.019902  [ 2128/ 3200]\n",
            "loss: 1.247903  [ 2144/ 3200]\n",
            "loss: 1.249699  [ 2160/ 3200]\n",
            "loss: 1.168634  [ 2176/ 3200]\n",
            "loss: 1.014172  [ 2192/ 3200]\n",
            "loss: 1.058397  [ 2208/ 3200]\n",
            "loss: 1.121929  [ 2224/ 3200]\n",
            "loss: 0.893746  [ 2240/ 3200]\n",
            "loss: 1.031541  [ 2256/ 3200]\n",
            "loss: 1.130970  [ 2272/ 3200]\n",
            "loss: 0.966971  [ 2288/ 3200]\n",
            "loss: 0.992117  [ 2304/ 3200]\n",
            "loss: 1.040042  [ 2320/ 3200]\n",
            "loss: 0.793923  [ 2336/ 3200]\n",
            "loss: 0.968536  [ 2352/ 3200]\n",
            "loss: 0.875131  [ 2368/ 3200]\n",
            "loss: 0.913756  [ 2384/ 3200]\n",
            "loss: 1.153986  [ 2400/ 3200]\n",
            "loss: 1.071104  [ 2416/ 3200]\n",
            "loss: 1.135323  [ 2432/ 3200]\n",
            "loss: 1.088273  [ 2448/ 3200]\n",
            "loss: 1.198939  [ 2464/ 3200]\n",
            "loss: 1.465460  [ 2480/ 3200]\n",
            "loss: 0.996939  [ 2496/ 3200]\n",
            "loss: 1.135734  [ 2512/ 3200]\n",
            "loss: 1.083067  [ 2528/ 3200]\n",
            "loss: 1.045187  [ 2544/ 3200]\n",
            "loss: 1.107904  [ 2560/ 3200]\n",
            "loss: 1.018449  [ 2576/ 3200]\n",
            "loss: 1.141416  [ 2592/ 3200]\n",
            "loss: 1.237875  [ 2608/ 3200]\n",
            "loss: 1.087696  [ 2624/ 3200]\n",
            "loss: 0.985526  [ 2640/ 3200]\n",
            "loss: 1.027339  [ 2656/ 3200]\n",
            "loss: 1.284238  [ 2672/ 3200]\n",
            "loss: 1.064287  [ 2688/ 3200]\n",
            "loss: 1.118357  [ 2704/ 3200]\n",
            "loss: 1.170964  [ 2720/ 3200]\n",
            "loss: 0.894379  [ 2736/ 3200]\n",
            "loss: 1.331805  [ 2752/ 3200]\n",
            "loss: 0.923897  [ 2768/ 3200]\n",
            "loss: 0.976133  [ 2784/ 3200]\n",
            "loss: 1.037312  [ 2800/ 3200]\n",
            "loss: 1.187386  [ 2816/ 3200]\n",
            "loss: 0.871786  [ 2832/ 3200]\n",
            "loss: 1.068389  [ 2848/ 3200]\n",
            "loss: 1.040509  [ 2864/ 3200]\n",
            "loss: 0.836444  [ 2880/ 3200]\n",
            "loss: 0.939025  [ 2896/ 3200]\n",
            "loss: 1.125384  [ 2912/ 3200]\n",
            "loss: 1.069695  [ 2928/ 3200]\n",
            "loss: 0.982442  [ 2944/ 3200]\n",
            "loss: 1.091165  [ 2960/ 3200]\n",
            "loss: 0.999843  [ 2976/ 3200]\n",
            "loss: 1.135378  [ 2992/ 3200]\n",
            "loss: 0.761190  [ 3008/ 3200]\n",
            "loss: 1.054476  [ 3024/ 3200]\n",
            "loss: 0.929888  [ 3040/ 3200]\n",
            "loss: 1.035349  [ 3056/ 3200]\n",
            "loss: 1.079538  [ 3072/ 3200]\n",
            "loss: 1.032466  [ 3088/ 3200]\n",
            "loss: 0.997709  [ 3104/ 3200]\n",
            "loss: 1.000877  [ 3120/ 3200]\n",
            "loss: 1.083362  [ 3136/ 3200]\n",
            "loss: 1.023479  [ 3152/ 3200]\n",
            "loss: 0.973482  [ 3168/ 3200]\n",
            "loss: 1.077803  [ 3184/ 3200]\n",
            "current epoch: 16\n",
            "\n",
            "loss: 1.066774  [    0/ 3200]\n",
            "loss: 1.135156  [   16/ 3200]\n",
            "loss: 1.115544  [   32/ 3200]\n",
            "loss: 1.146416  [   48/ 3200]\n",
            "loss: 1.026271  [   64/ 3200]\n",
            "loss: 1.019423  [   80/ 3200]\n",
            "loss: 1.000209  [   96/ 3200]\n",
            "loss: 0.879961  [  112/ 3200]\n",
            "loss: 0.759169  [  128/ 3200]\n",
            "loss: 0.999241  [  144/ 3200]\n",
            "loss: 1.172440  [  160/ 3200]\n",
            "loss: 0.943891  [  176/ 3200]\n",
            "loss: 0.955904  [  192/ 3200]\n",
            "loss: 0.914740  [  208/ 3200]\n",
            "loss: 1.114470  [  224/ 3200]\n",
            "loss: 1.072929  [  240/ 3200]\n",
            "loss: 0.878217  [  256/ 3200]\n",
            "loss: 1.036388  [  272/ 3200]\n",
            "loss: 1.092679  [  288/ 3200]\n",
            "loss: 0.911924  [  304/ 3200]\n",
            "loss: 1.060798  [  320/ 3200]\n",
            "loss: 0.827730  [  336/ 3200]\n",
            "loss: 1.181166  [  352/ 3200]\n",
            "loss: 1.128849  [  368/ 3200]\n",
            "loss: 0.974812  [  384/ 3200]\n",
            "loss: 0.926282  [  400/ 3200]\n",
            "loss: 1.060198  [  416/ 3200]\n",
            "loss: 0.808374  [  432/ 3200]\n",
            "loss: 0.977256  [  448/ 3200]\n",
            "loss: 1.056938  [  464/ 3200]\n",
            "loss: 0.945327  [  480/ 3200]\n",
            "loss: 0.934884  [  496/ 3200]\n",
            "loss: 0.956949  [  512/ 3200]\n",
            "loss: 1.087297  [  528/ 3200]\n",
            "loss: 1.454285  [  544/ 3200]\n",
            "loss: 0.782646  [  560/ 3200]\n",
            "loss: 1.099395  [  576/ 3200]\n",
            "loss: 0.934438  [  592/ 3200]\n",
            "loss: 1.332318  [  608/ 3200]\n",
            "loss: 1.090350  [  624/ 3200]\n",
            "loss: 1.120191  [  640/ 3200]\n",
            "loss: 0.924957  [  656/ 3200]\n",
            "loss: 1.134706  [  672/ 3200]\n",
            "loss: 1.099167  [  688/ 3200]\n",
            "loss: 0.944671  [  704/ 3200]\n",
            "loss: 1.068675  [  720/ 3200]\n",
            "loss: 1.020323  [  736/ 3200]\n",
            "loss: 0.985069  [  752/ 3200]\n",
            "loss: 1.143815  [  768/ 3200]\n",
            "loss: 0.835970  [  784/ 3200]\n",
            "loss: 1.018789  [  800/ 3200]\n",
            "loss: 1.013050  [  816/ 3200]\n",
            "loss: 0.903220  [  832/ 3200]\n",
            "loss: 0.763693  [  848/ 3200]\n",
            "loss: 0.887960  [  864/ 3200]\n",
            "loss: 1.063522  [  880/ 3200]\n",
            "loss: 1.152527  [  896/ 3200]\n",
            "loss: 1.231512  [  912/ 3200]\n",
            "loss: 1.038187  [  928/ 3200]\n",
            "loss: 1.060256  [  944/ 3200]\n",
            "loss: 1.104120  [  960/ 3200]\n",
            "loss: 0.997922  [  976/ 3200]\n",
            "loss: 1.050431  [  992/ 3200]\n",
            "loss: 0.723989  [ 1008/ 3200]\n",
            "loss: 1.051743  [ 1024/ 3200]\n",
            "loss: 1.005068  [ 1040/ 3200]\n",
            "loss: 0.652929  [ 1056/ 3200]\n",
            "loss: 0.877186  [ 1072/ 3200]\n",
            "loss: 0.759620  [ 1088/ 3200]\n",
            "loss: 0.821453  [ 1104/ 3200]\n",
            "loss: 1.005602  [ 1120/ 3200]\n",
            "loss: 0.934492  [ 1136/ 3200]\n",
            "loss: 0.917822  [ 1152/ 3200]\n",
            "loss: 0.932286  [ 1168/ 3200]\n",
            "loss: 0.721854  [ 1184/ 3200]\n",
            "loss: 0.876663  [ 1200/ 3200]\n",
            "loss: 1.072091  [ 1216/ 3200]\n",
            "loss: 1.074444  [ 1232/ 3200]\n",
            "loss: 0.985729  [ 1248/ 3200]\n",
            "loss: 0.809277  [ 1264/ 3200]\n",
            "loss: 0.954679  [ 1280/ 3200]\n",
            "loss: 1.047452  [ 1296/ 3200]\n",
            "loss: 1.182543  [ 1312/ 3200]\n",
            "loss: 0.938537  [ 1328/ 3200]\n",
            "loss: 0.937168  [ 1344/ 3200]\n",
            "loss: 0.855326  [ 1360/ 3200]\n",
            "loss: 0.949597  [ 1376/ 3200]\n",
            "loss: 0.999318  [ 1392/ 3200]\n",
            "loss: 0.928364  [ 1408/ 3200]\n",
            "loss: 0.851329  [ 1424/ 3200]\n",
            "loss: 0.990868  [ 1440/ 3200]\n",
            "loss: 1.134427  [ 1456/ 3200]\n",
            "loss: 1.237659  [ 1472/ 3200]\n",
            "loss: 0.991277  [ 1488/ 3200]\n",
            "loss: 1.014257  [ 1504/ 3200]\n",
            "loss: 1.072253  [ 1520/ 3200]\n",
            "loss: 1.164351  [ 1536/ 3200]\n",
            "loss: 1.049818  [ 1552/ 3200]\n",
            "loss: 1.298186  [ 1568/ 3200]\n",
            "loss: 1.005003  [ 1584/ 3200]\n",
            "loss: 0.922542  [ 1600/ 3200]\n",
            "loss: 1.043898  [ 1616/ 3200]\n",
            "loss: 1.191069  [ 1632/ 3200]\n",
            "loss: 1.037115  [ 1648/ 3200]\n",
            "loss: 0.928166  [ 1664/ 3200]\n",
            "loss: 1.002264  [ 1680/ 3200]\n",
            "loss: 1.171246  [ 1696/ 3200]\n",
            "loss: 1.113795  [ 1712/ 3200]\n",
            "loss: 0.839025  [ 1728/ 3200]\n",
            "loss: 0.937676  [ 1744/ 3200]\n",
            "loss: 1.155777  [ 1760/ 3200]\n",
            "loss: 0.924329  [ 1776/ 3200]\n",
            "loss: 1.108970  [ 1792/ 3200]\n",
            "loss: 0.829104  [ 1808/ 3200]\n",
            "loss: 1.067498  [ 1824/ 3200]\n",
            "loss: 0.929641  [ 1840/ 3200]\n",
            "loss: 0.954003  [ 1856/ 3200]\n",
            "loss: 0.897056  [ 1872/ 3200]\n",
            "loss: 1.005144  [ 1888/ 3200]\n",
            "loss: 1.443100  [ 1904/ 3200]\n",
            "loss: 1.061252  [ 1920/ 3200]\n",
            "loss: 1.166559  [ 1936/ 3200]\n",
            "loss: 1.332792  [ 1952/ 3200]\n",
            "loss: 1.147342  [ 1968/ 3200]\n",
            "loss: 0.955814  [ 1984/ 3200]\n",
            "loss: 1.268368  [ 2000/ 3200]\n",
            "loss: 1.321407  [ 2016/ 3200]\n",
            "loss: 1.121981  [ 2032/ 3200]\n",
            "loss: 1.196112  [ 2048/ 3200]\n",
            "loss: 1.285778  [ 2064/ 3200]\n",
            "loss: 0.991959  [ 2080/ 3200]\n",
            "loss: 1.141771  [ 2096/ 3200]\n",
            "loss: 1.008278  [ 2112/ 3200]\n",
            "loss: 0.886087  [ 2128/ 3200]\n",
            "loss: 1.019396  [ 2144/ 3200]\n",
            "loss: 1.073946  [ 2160/ 3200]\n",
            "loss: 0.996614  [ 2176/ 3200]\n",
            "loss: 1.074797  [ 2192/ 3200]\n",
            "loss: 0.954234  [ 2208/ 3200]\n",
            "loss: 1.066170  [ 2224/ 3200]\n",
            "loss: 0.989772  [ 2240/ 3200]\n",
            "loss: 1.203905  [ 2256/ 3200]\n",
            "loss: 1.134634  [ 2272/ 3200]\n",
            "loss: 0.781445  [ 2288/ 3200]\n",
            "loss: 0.897403  [ 2304/ 3200]\n",
            "loss: 0.957553  [ 2320/ 3200]\n",
            "loss: 1.199413  [ 2336/ 3200]\n",
            "loss: 1.102658  [ 2352/ 3200]\n",
            "loss: 1.224619  [ 2368/ 3200]\n",
            "loss: 0.851692  [ 2384/ 3200]\n",
            "loss: 0.795018  [ 2400/ 3200]\n",
            "loss: 1.079873  [ 2416/ 3200]\n",
            "loss: 0.986353  [ 2432/ 3200]\n",
            "loss: 1.051112  [ 2448/ 3200]\n",
            "loss: 1.241256  [ 2464/ 3200]\n",
            "loss: 0.944195  [ 2480/ 3200]\n",
            "loss: 0.988731  [ 2496/ 3200]\n",
            "loss: 0.984447  [ 2512/ 3200]\n",
            "loss: 0.944151  [ 2528/ 3200]\n",
            "loss: 0.874146  [ 2544/ 3200]\n",
            "loss: 1.047305  [ 2560/ 3200]\n",
            "loss: 0.922247  [ 2576/ 3200]\n",
            "loss: 0.931455  [ 2592/ 3200]\n",
            "loss: 1.112729  [ 2608/ 3200]\n",
            "loss: 0.622128  [ 2624/ 3200]\n",
            "loss: 1.121918  [ 2640/ 3200]\n",
            "loss: 1.037539  [ 2656/ 3200]\n",
            "loss: 1.127988  [ 2672/ 3200]\n",
            "loss: 0.822681  [ 2688/ 3200]\n",
            "loss: 1.316648  [ 2704/ 3200]\n",
            "loss: 0.964916  [ 2720/ 3200]\n",
            "loss: 1.017192  [ 2736/ 3200]\n",
            "loss: 0.970513  [ 2752/ 3200]\n",
            "loss: 1.017399  [ 2768/ 3200]\n",
            "loss: 1.081258  [ 2784/ 3200]\n",
            "loss: 1.045354  [ 2800/ 3200]\n",
            "loss: 1.175868  [ 2816/ 3200]\n",
            "loss: 1.190400  [ 2832/ 3200]\n",
            "loss: 0.948480  [ 2848/ 3200]\n",
            "loss: 0.773695  [ 2864/ 3200]\n",
            "loss: 1.114423  [ 2880/ 3200]\n",
            "loss: 0.983546  [ 2896/ 3200]\n",
            "loss: 1.041679  [ 2912/ 3200]\n",
            "loss: 0.850620  [ 2928/ 3200]\n",
            "loss: 1.106135  [ 2944/ 3200]\n",
            "loss: 0.937033  [ 2960/ 3200]\n",
            "loss: 1.021737  [ 2976/ 3200]\n",
            "loss: 1.164000  [ 2992/ 3200]\n",
            "loss: 1.020361  [ 3008/ 3200]\n",
            "loss: 1.137093  [ 3024/ 3200]\n",
            "loss: 1.108764  [ 3040/ 3200]\n",
            "loss: 1.213560  [ 3056/ 3200]\n",
            "loss: 0.921506  [ 3072/ 3200]\n",
            "loss: 0.896706  [ 3088/ 3200]\n",
            "loss: 1.168434  [ 3104/ 3200]\n",
            "loss: 0.814568  [ 3120/ 3200]\n",
            "loss: 1.213568  [ 3136/ 3200]\n",
            "loss: 0.878626  [ 3152/ 3200]\n",
            "loss: 0.960097  [ 3168/ 3200]\n",
            "loss: 0.905419  [ 3184/ 3200]\n",
            "current epoch: 17\n",
            "\n",
            "loss: 0.846361  [    0/ 3200]\n",
            "loss: 1.207644  [   16/ 3200]\n",
            "loss: 1.000292  [   32/ 3200]\n",
            "loss: 1.083763  [   48/ 3200]\n",
            "loss: 0.994476  [   64/ 3200]\n",
            "loss: 1.025934  [   80/ 3200]\n",
            "loss: 1.077410  [   96/ 3200]\n",
            "loss: 0.647633  [  112/ 3200]\n",
            "loss: 1.312277  [  128/ 3200]\n",
            "loss: 1.069747  [  144/ 3200]\n",
            "loss: 1.175991  [  160/ 3200]\n",
            "loss: 0.885710  [  176/ 3200]\n",
            "loss: 1.050171  [  192/ 3200]\n",
            "loss: 1.355981  [  208/ 3200]\n",
            "loss: 1.169914  [  224/ 3200]\n",
            "loss: 0.980507  [  240/ 3200]\n",
            "loss: 1.034883  [  256/ 3200]\n",
            "loss: 1.067253  [  272/ 3200]\n",
            "loss: 0.797695  [  288/ 3200]\n",
            "loss: 1.011740  [  304/ 3200]\n",
            "loss: 0.992864  [  320/ 3200]\n",
            "loss: 1.071268  [  336/ 3200]\n",
            "loss: 0.946749  [  352/ 3200]\n",
            "loss: 1.096867  [  368/ 3200]\n",
            "loss: 0.847485  [  384/ 3200]\n",
            "loss: 1.245586  [  400/ 3200]\n",
            "loss: 0.925922  [  416/ 3200]\n",
            "loss: 1.196887  [  432/ 3200]\n",
            "loss: 0.872546  [  448/ 3200]\n",
            "loss: 1.138186  [  464/ 3200]\n",
            "loss: 1.022069  [  480/ 3200]\n",
            "loss: 0.962465  [  496/ 3200]\n",
            "loss: 1.394264  [  512/ 3200]\n",
            "loss: 1.084617  [  528/ 3200]\n",
            "loss: 0.773270  [  544/ 3200]\n",
            "loss: 1.027138  [  560/ 3200]\n",
            "loss: 1.093232  [  576/ 3200]\n",
            "loss: 1.328100  [  592/ 3200]\n",
            "loss: 1.051204  [  608/ 3200]\n",
            "loss: 0.958553  [  624/ 3200]\n",
            "loss: 1.323164  [  640/ 3200]\n",
            "loss: 1.116650  [  656/ 3200]\n",
            "loss: 0.843856  [  672/ 3200]\n",
            "loss: 0.962975  [  688/ 3200]\n",
            "loss: 1.023294  [  704/ 3200]\n",
            "loss: 0.899847  [  720/ 3200]\n",
            "loss: 0.979817  [  736/ 3200]\n",
            "loss: 1.031895  [  752/ 3200]\n",
            "loss: 1.220876  [  768/ 3200]\n",
            "loss: 1.089141  [  784/ 3200]\n",
            "loss: 1.343905  [  800/ 3200]\n",
            "loss: 1.115170  [  816/ 3200]\n",
            "loss: 1.023664  [  832/ 3200]\n",
            "loss: 1.046551  [  848/ 3200]\n",
            "loss: 1.039363  [  864/ 3200]\n",
            "loss: 1.245020  [  880/ 3200]\n",
            "loss: 1.011306  [  896/ 3200]\n",
            "loss: 0.890954  [  912/ 3200]\n",
            "loss: 0.999033  [  928/ 3200]\n",
            "loss: 0.954960  [  944/ 3200]\n",
            "loss: 0.961721  [  960/ 3200]\n",
            "loss: 0.813910  [  976/ 3200]\n",
            "loss: 0.960625  [  992/ 3200]\n",
            "loss: 1.027555  [ 1008/ 3200]\n",
            "loss: 0.913939  [ 1024/ 3200]\n",
            "loss: 0.908533  [ 1040/ 3200]\n",
            "loss: 1.088685  [ 1056/ 3200]\n",
            "loss: 1.037499  [ 1072/ 3200]\n",
            "loss: 1.016130  [ 1088/ 3200]\n",
            "loss: 1.002961  [ 1104/ 3200]\n",
            "loss: 1.083380  [ 1120/ 3200]\n",
            "loss: 0.925473  [ 1136/ 3200]\n",
            "loss: 1.104890  [ 1152/ 3200]\n",
            "loss: 1.197398  [ 1168/ 3200]\n",
            "loss: 1.075413  [ 1184/ 3200]\n",
            "loss: 1.266372  [ 1200/ 3200]\n",
            "loss: 0.963006  [ 1216/ 3200]\n",
            "loss: 1.136418  [ 1232/ 3200]\n",
            "loss: 0.844279  [ 1248/ 3200]\n",
            "loss: 0.993133  [ 1264/ 3200]\n",
            "loss: 0.895870  [ 1280/ 3200]\n",
            "loss: 1.036895  [ 1296/ 3200]\n",
            "loss: 1.012959  [ 1312/ 3200]\n",
            "loss: 0.962177  [ 1328/ 3200]\n",
            "loss: 0.943952  [ 1344/ 3200]\n",
            "loss: 1.012859  [ 1360/ 3200]\n",
            "loss: 1.061139  [ 1376/ 3200]\n",
            "loss: 1.079986  [ 1392/ 3200]\n",
            "loss: 0.969831  [ 1408/ 3200]\n",
            "loss: 1.312544  [ 1424/ 3200]\n",
            "loss: 0.939376  [ 1440/ 3200]\n",
            "loss: 0.911469  [ 1456/ 3200]\n",
            "loss: 1.040203  [ 1472/ 3200]\n",
            "loss: 0.853010  [ 1488/ 3200]\n",
            "loss: 0.950492  [ 1504/ 3200]\n",
            "loss: 0.874566  [ 1520/ 3200]\n",
            "loss: 0.710373  [ 1536/ 3200]\n",
            "loss: 0.946450  [ 1552/ 3200]\n",
            "loss: 1.058729  [ 1568/ 3200]\n",
            "loss: 0.887184  [ 1584/ 3200]\n",
            "loss: 0.987283  [ 1600/ 3200]\n",
            "loss: 1.043850  [ 1616/ 3200]\n",
            "loss: 0.941181  [ 1632/ 3200]\n",
            "loss: 0.910128  [ 1648/ 3200]\n",
            "loss: 0.993257  [ 1664/ 3200]\n",
            "loss: 0.868315  [ 1680/ 3200]\n",
            "loss: 1.098363  [ 1696/ 3200]\n",
            "loss: 0.870099  [ 1712/ 3200]\n",
            "loss: 1.170046  [ 1728/ 3200]\n",
            "loss: 1.059110  [ 1744/ 3200]\n",
            "loss: 0.965559  [ 1760/ 3200]\n",
            "loss: 1.050929  [ 1776/ 3200]\n",
            "loss: 1.110802  [ 1792/ 3200]\n",
            "loss: 1.013482  [ 1808/ 3200]\n",
            "loss: 0.781384  [ 1824/ 3200]\n",
            "loss: 0.988733  [ 1840/ 3200]\n",
            "loss: 1.427614  [ 1856/ 3200]\n",
            "loss: 1.121387  [ 1872/ 3200]\n",
            "loss: 1.281665  [ 1888/ 3200]\n",
            "loss: 0.729128  [ 1904/ 3200]\n",
            "loss: 1.181618  [ 1920/ 3200]\n",
            "loss: 1.068129  [ 1936/ 3200]\n",
            "loss: 1.121129  [ 1952/ 3200]\n",
            "loss: 1.085872  [ 1968/ 3200]\n",
            "loss: 1.112513  [ 1984/ 3200]\n",
            "loss: 0.932197  [ 2000/ 3200]\n",
            "loss: 1.266119  [ 2016/ 3200]\n",
            "loss: 0.962992  [ 2032/ 3200]\n",
            "loss: 0.910728  [ 2048/ 3200]\n",
            "loss: 0.778660  [ 2064/ 3200]\n",
            "loss: 0.953329  [ 2080/ 3200]\n",
            "loss: 0.910342  [ 2096/ 3200]\n",
            "loss: 0.865599  [ 2112/ 3200]\n",
            "loss: 1.027134  [ 2128/ 3200]\n",
            "loss: 1.424641  [ 2144/ 3200]\n",
            "loss: 0.948829  [ 2160/ 3200]\n",
            "loss: 0.976744  [ 2176/ 3200]\n",
            "loss: 1.050684  [ 2192/ 3200]\n",
            "loss: 1.050834  [ 2208/ 3200]\n",
            "loss: 0.928499  [ 2224/ 3200]\n",
            "loss: 1.042980  [ 2240/ 3200]\n",
            "loss: 0.822932  [ 2256/ 3200]\n",
            "loss: 0.829182  [ 2272/ 3200]\n",
            "loss: 0.797959  [ 2288/ 3200]\n",
            "loss: 0.932128  [ 2304/ 3200]\n",
            "loss: 0.904857  [ 2320/ 3200]\n",
            "loss: 0.850685  [ 2336/ 3200]\n",
            "loss: 0.842268  [ 2352/ 3200]\n",
            "loss: 0.983412  [ 2368/ 3200]\n",
            "loss: 0.876014  [ 2384/ 3200]\n",
            "loss: 1.108925  [ 2400/ 3200]\n",
            "loss: 0.806307  [ 2416/ 3200]\n",
            "loss: 0.900737  [ 2432/ 3200]\n",
            "loss: 1.254373  [ 2448/ 3200]\n",
            "loss: 0.896389  [ 2464/ 3200]\n",
            "loss: 0.986022  [ 2480/ 3200]\n",
            "loss: 1.044389  [ 2496/ 3200]\n",
            "loss: 0.956355  [ 2512/ 3200]\n",
            "loss: 0.890151  [ 2528/ 3200]\n",
            "loss: 0.785656  [ 2544/ 3200]\n",
            "loss: 1.095422  [ 2560/ 3200]\n",
            "loss: 0.897848  [ 2576/ 3200]\n",
            "loss: 1.135834  [ 2592/ 3200]\n",
            "loss: 0.886571  [ 2608/ 3200]\n",
            "loss: 0.934168  [ 2624/ 3200]\n",
            "loss: 0.857273  [ 2640/ 3200]\n",
            "loss: 1.216058  [ 2656/ 3200]\n",
            "loss: 0.985802  [ 2672/ 3200]\n",
            "loss: 0.764961  [ 2688/ 3200]\n",
            "loss: 0.853893  [ 2704/ 3200]\n",
            "loss: 0.926688  [ 2720/ 3200]\n",
            "loss: 1.235309  [ 2736/ 3200]\n",
            "loss: 0.746826  [ 2752/ 3200]\n",
            "loss: 0.731938  [ 2768/ 3200]\n",
            "loss: 1.021113  [ 2784/ 3200]\n",
            "loss: 0.871661  [ 2800/ 3200]\n",
            "loss: 0.902870  [ 2816/ 3200]\n",
            "loss: 1.052251  [ 2832/ 3200]\n",
            "loss: 0.940920  [ 2848/ 3200]\n",
            "loss: 1.007596  [ 2864/ 3200]\n",
            "loss: 0.959918  [ 2880/ 3200]\n",
            "loss: 0.839652  [ 2896/ 3200]\n",
            "loss: 1.024452  [ 2912/ 3200]\n",
            "loss: 1.081098  [ 2928/ 3200]\n",
            "loss: 0.952302  [ 2944/ 3200]\n",
            "loss: 0.955371  [ 2960/ 3200]\n",
            "loss: 0.969340  [ 2976/ 3200]\n",
            "loss: 1.082486  [ 2992/ 3200]\n",
            "loss: 1.183330  [ 3008/ 3200]\n",
            "loss: 1.242730  [ 3024/ 3200]\n",
            "loss: 1.196921  [ 3040/ 3200]\n",
            "loss: 1.080084  [ 3056/ 3200]\n",
            "loss: 0.871487  [ 3072/ 3200]\n",
            "loss: 0.967028  [ 3088/ 3200]\n",
            "loss: 1.102185  [ 3104/ 3200]\n",
            "loss: 0.954849  [ 3120/ 3200]\n",
            "loss: 1.104495  [ 3136/ 3200]\n",
            "loss: 0.898760  [ 3152/ 3200]\n",
            "loss: 0.837493  [ 3168/ 3200]\n",
            "loss: 0.846818  [ 3184/ 3200]\n",
            "current epoch: 18\n",
            "\n",
            "loss: 0.888758  [    0/ 3200]\n",
            "loss: 0.818467  [   16/ 3200]\n",
            "loss: 1.064389  [   32/ 3200]\n",
            "loss: 0.765750  [   48/ 3200]\n",
            "loss: 1.304048  [   64/ 3200]\n",
            "loss: 1.013007  [   80/ 3200]\n",
            "loss: 1.151804  [   96/ 3200]\n",
            "loss: 0.935564  [  112/ 3200]\n",
            "loss: 1.082448  [  128/ 3200]\n",
            "loss: 0.841002  [  144/ 3200]\n",
            "loss: 1.150445  [  160/ 3200]\n",
            "loss: 0.982644  [  176/ 3200]\n",
            "loss: 1.142421  [  192/ 3200]\n",
            "loss: 0.946366  [  208/ 3200]\n",
            "loss: 1.119242  [  224/ 3200]\n",
            "loss: 1.020398  [  240/ 3200]\n",
            "loss: 0.796398  [  256/ 3200]\n",
            "loss: 1.004062  [  272/ 3200]\n",
            "loss: 0.940299  [  288/ 3200]\n",
            "loss: 1.439352  [  304/ 3200]\n",
            "loss: 0.868908  [  320/ 3200]\n",
            "loss: 1.187747  [  336/ 3200]\n",
            "loss: 0.939578  [  352/ 3200]\n",
            "loss: 0.798317  [  368/ 3200]\n",
            "loss: 1.023076  [  384/ 3200]\n",
            "loss: 0.947343  [  400/ 3200]\n",
            "loss: 1.324602  [  416/ 3200]\n",
            "loss: 0.790010  [  432/ 3200]\n",
            "loss: 0.632437  [  448/ 3200]\n",
            "loss: 1.111666  [  464/ 3200]\n",
            "loss: 0.966717  [  480/ 3200]\n",
            "loss: 0.962934  [  496/ 3200]\n",
            "loss: 1.199860  [  512/ 3200]\n",
            "loss: 0.928642  [  528/ 3200]\n",
            "loss: 1.154367  [  544/ 3200]\n",
            "loss: 1.410686  [  560/ 3200]\n",
            "loss: 1.027487  [  576/ 3200]\n",
            "loss: 0.964203  [  592/ 3200]\n",
            "loss: 0.900816  [  608/ 3200]\n",
            "loss: 0.934237  [  624/ 3200]\n",
            "loss: 0.887252  [  640/ 3200]\n",
            "loss: 1.029650  [  656/ 3200]\n",
            "loss: 1.145614  [  672/ 3200]\n",
            "loss: 0.990277  [  688/ 3200]\n",
            "loss: 0.877399  [  704/ 3200]\n",
            "loss: 0.862277  [  720/ 3200]\n",
            "loss: 1.216388  [  736/ 3200]\n",
            "loss: 1.062002  [  752/ 3200]\n",
            "loss: 0.823042  [  768/ 3200]\n",
            "loss: 0.835873  [  784/ 3200]\n",
            "loss: 1.143446  [  800/ 3200]\n",
            "loss: 1.221944  [  816/ 3200]\n",
            "loss: 1.048020  [  832/ 3200]\n",
            "loss: 0.952034  [  848/ 3200]\n",
            "loss: 0.903173  [  864/ 3200]\n",
            "loss: 0.952443  [  880/ 3200]\n",
            "loss: 1.176986  [  896/ 3200]\n",
            "loss: 0.986345  [  912/ 3200]\n",
            "loss: 0.945616  [  928/ 3200]\n",
            "loss: 0.906174  [  944/ 3200]\n",
            "loss: 1.079968  [  960/ 3200]\n",
            "loss: 0.919022  [  976/ 3200]\n",
            "loss: 0.925677  [  992/ 3200]\n",
            "loss: 1.070983  [ 1008/ 3200]\n",
            "loss: 0.966140  [ 1024/ 3200]\n",
            "loss: 1.048582  [ 1040/ 3200]\n",
            "loss: 1.027525  [ 1056/ 3200]\n",
            "loss: 0.963580  [ 1072/ 3200]\n",
            "loss: 1.075349  [ 1088/ 3200]\n",
            "loss: 0.802161  [ 1104/ 3200]\n",
            "loss: 0.970584  [ 1120/ 3200]\n",
            "loss: 0.971987  [ 1136/ 3200]\n",
            "loss: 1.076036  [ 1152/ 3200]\n",
            "loss: 1.224221  [ 1168/ 3200]\n",
            "loss: 0.976214  [ 1184/ 3200]\n",
            "loss: 0.955927  [ 1200/ 3200]\n",
            "loss: 1.355573  [ 1216/ 3200]\n",
            "loss: 0.878871  [ 1232/ 3200]\n",
            "loss: 0.807580  [ 1248/ 3200]\n",
            "loss: 0.877070  [ 1264/ 3200]\n",
            "loss: 0.914708  [ 1280/ 3200]\n",
            "loss: 1.241625  [ 1296/ 3200]\n",
            "loss: 1.258371  [ 1312/ 3200]\n",
            "loss: 1.122051  [ 1328/ 3200]\n",
            "loss: 1.072216  [ 1344/ 3200]\n",
            "loss: 1.252711  [ 1360/ 3200]\n",
            "loss: 0.761242  [ 1376/ 3200]\n",
            "loss: 0.924370  [ 1392/ 3200]\n",
            "loss: 1.061663  [ 1408/ 3200]\n",
            "loss: 0.965286  [ 1424/ 3200]\n",
            "loss: 1.062297  [ 1440/ 3200]\n",
            "loss: 0.920599  [ 1456/ 3200]\n",
            "loss: 0.834232  [ 1472/ 3200]\n",
            "loss: 0.972616  [ 1488/ 3200]\n",
            "loss: 1.086525  [ 1504/ 3200]\n",
            "loss: 1.163072  [ 1520/ 3200]\n",
            "loss: 0.907821  [ 1536/ 3200]\n",
            "loss: 0.780829  [ 1552/ 3200]\n",
            "loss: 0.920897  [ 1568/ 3200]\n",
            "loss: 0.928601  [ 1584/ 3200]\n",
            "loss: 0.768347  [ 1600/ 3200]\n",
            "loss: 0.999840  [ 1616/ 3200]\n",
            "loss: 1.022575  [ 1632/ 3200]\n",
            "loss: 1.066497  [ 1648/ 3200]\n",
            "loss: 1.302054  [ 1664/ 3200]\n",
            "loss: 1.230969  [ 1680/ 3200]\n",
            "loss: 0.934651  [ 1696/ 3200]\n",
            "loss: 1.032415  [ 1712/ 3200]\n",
            "loss: 0.982574  [ 1728/ 3200]\n",
            "loss: 1.051270  [ 1744/ 3200]\n",
            "loss: 1.303324  [ 1760/ 3200]\n",
            "loss: 0.998137  [ 1776/ 3200]\n",
            "loss: 1.066070  [ 1792/ 3200]\n",
            "loss: 1.130777  [ 1808/ 3200]\n",
            "loss: 0.879147  [ 1824/ 3200]\n",
            "loss: 0.926240  [ 1840/ 3200]\n",
            "loss: 1.088125  [ 1856/ 3200]\n",
            "loss: 0.924581  [ 1872/ 3200]\n",
            "loss: 1.228746  [ 1888/ 3200]\n",
            "loss: 0.901172  [ 1904/ 3200]\n",
            "loss: 0.794806  [ 1920/ 3200]\n",
            "loss: 1.000983  [ 1936/ 3200]\n",
            "loss: 1.020574  [ 1952/ 3200]\n",
            "loss: 0.891269  [ 1968/ 3200]\n",
            "loss: 0.989950  [ 1984/ 3200]\n",
            "loss: 1.042559  [ 2000/ 3200]\n",
            "loss: 0.984415  [ 2016/ 3200]\n",
            "loss: 0.970667  [ 2032/ 3200]\n",
            "loss: 0.902282  [ 2048/ 3200]\n",
            "loss: 1.040643  [ 2064/ 3200]\n",
            "loss: 0.913002  [ 2080/ 3200]\n",
            "loss: 0.796997  [ 2096/ 3200]\n",
            "loss: 1.031936  [ 2112/ 3200]\n",
            "loss: 1.027613  [ 2128/ 3200]\n",
            "loss: 0.894664  [ 2144/ 3200]\n",
            "loss: 0.896514  [ 2160/ 3200]\n",
            "loss: 1.133217  [ 2176/ 3200]\n",
            "loss: 0.980273  [ 2192/ 3200]\n",
            "loss: 0.962456  [ 2208/ 3200]\n",
            "loss: 0.766770  [ 2224/ 3200]\n",
            "loss: 1.027300  [ 2240/ 3200]\n",
            "loss: 1.214346  [ 2256/ 3200]\n",
            "loss: 0.951099  [ 2272/ 3200]\n",
            "loss: 0.969706  [ 2288/ 3200]\n",
            "loss: 0.936726  [ 2304/ 3200]\n",
            "loss: 0.857816  [ 2320/ 3200]\n",
            "loss: 0.840192  [ 2336/ 3200]\n",
            "loss: 1.043717  [ 2352/ 3200]\n",
            "loss: 1.064796  [ 2368/ 3200]\n",
            "loss: 0.924721  [ 2384/ 3200]\n",
            "loss: 0.861244  [ 2400/ 3200]\n",
            "loss: 0.963577  [ 2416/ 3200]\n",
            "loss: 1.192779  [ 2432/ 3200]\n",
            "loss: 1.005902  [ 2448/ 3200]\n",
            "loss: 1.083959  [ 2464/ 3200]\n",
            "loss: 1.070642  [ 2480/ 3200]\n",
            "loss: 0.950159  [ 2496/ 3200]\n",
            "loss: 0.842232  [ 2512/ 3200]\n",
            "loss: 0.945047  [ 2528/ 3200]\n",
            "loss: 0.920105  [ 2544/ 3200]\n",
            "loss: 0.961597  [ 2560/ 3200]\n",
            "loss: 1.234710  [ 2576/ 3200]\n",
            "loss: 0.840920  [ 2592/ 3200]\n",
            "loss: 0.821393  [ 2608/ 3200]\n",
            "loss: 0.749406  [ 2624/ 3200]\n",
            "loss: 0.710050  [ 2640/ 3200]\n",
            "loss: 0.999548  [ 2656/ 3200]\n",
            "loss: 1.101396  [ 2672/ 3200]\n",
            "loss: 1.421320  [ 2688/ 3200]\n",
            "loss: 1.040641  [ 2704/ 3200]\n",
            "loss: 1.160272  [ 2720/ 3200]\n",
            "loss: 0.995601  [ 2736/ 3200]\n",
            "loss: 1.076985  [ 2752/ 3200]\n",
            "loss: 1.093046  [ 2768/ 3200]\n",
            "loss: 0.803203  [ 2784/ 3200]\n",
            "loss: 0.872137  [ 2800/ 3200]\n",
            "loss: 0.800047  [ 2816/ 3200]\n",
            "loss: 1.019665  [ 2832/ 3200]\n",
            "loss: 1.109998  [ 2848/ 3200]\n",
            "loss: 1.033012  [ 2864/ 3200]\n",
            "loss: 1.017417  [ 2880/ 3200]\n",
            "loss: 0.758919  [ 2896/ 3200]\n",
            "loss: 1.072958  [ 2912/ 3200]\n",
            "loss: 1.044572  [ 2928/ 3200]\n",
            "loss: 0.897369  [ 2944/ 3200]\n",
            "loss: 1.096372  [ 2960/ 3200]\n",
            "loss: 0.848851  [ 2976/ 3200]\n",
            "loss: 0.962275  [ 2992/ 3200]\n",
            "loss: 0.844597  [ 3008/ 3200]\n",
            "loss: 1.046696  [ 3024/ 3200]\n",
            "loss: 1.261346  [ 3040/ 3200]\n",
            "loss: 0.880158  [ 3056/ 3200]\n",
            "loss: 1.079557  [ 3072/ 3200]\n",
            "loss: 1.012397  [ 3088/ 3200]\n",
            "loss: 0.847512  [ 3104/ 3200]\n",
            "loss: 1.044339  [ 3120/ 3200]\n",
            "loss: 0.838676  [ 3136/ 3200]\n",
            "loss: 0.865424  [ 3152/ 3200]\n",
            "loss: 1.062152  [ 3168/ 3200]\n",
            "loss: 1.007062  [ 3184/ 3200]\n",
            "current epoch: 19\n",
            "\n",
            "loss: 0.831629  [    0/ 3200]\n",
            "loss: 1.011758  [   16/ 3200]\n",
            "loss: 0.917154  [   32/ 3200]\n",
            "loss: 1.289960  [   48/ 3200]\n",
            "loss: 1.075747  [   64/ 3200]\n",
            "loss: 1.029727  [   80/ 3200]\n",
            "loss: 0.818933  [   96/ 3200]\n",
            "loss: 1.001424  [  112/ 3200]\n",
            "loss: 0.904731  [  128/ 3200]\n",
            "loss: 1.138754  [  144/ 3200]\n",
            "loss: 0.866307  [  160/ 3200]\n",
            "loss: 0.593237  [  176/ 3200]\n",
            "loss: 0.820521  [  192/ 3200]\n",
            "loss: 0.994320  [  208/ 3200]\n",
            "loss: 1.108661  [  224/ 3200]\n",
            "loss: 1.244238  [  240/ 3200]\n",
            "loss: 1.200768  [  256/ 3200]\n",
            "loss: 0.912814  [  272/ 3200]\n",
            "loss: 0.848468  [  288/ 3200]\n",
            "loss: 0.973522  [  304/ 3200]\n",
            "loss: 1.170353  [  320/ 3200]\n",
            "loss: 0.957459  [  336/ 3200]\n",
            "loss: 1.331526  [  352/ 3200]\n",
            "loss: 0.906480  [  368/ 3200]\n",
            "loss: 1.044080  [  384/ 3200]\n",
            "loss: 1.006791  [  400/ 3200]\n",
            "loss: 0.972625  [  416/ 3200]\n",
            "loss: 1.067047  [  432/ 3200]\n",
            "loss: 1.118717  [  448/ 3200]\n",
            "loss: 1.054418  [  464/ 3200]\n",
            "loss: 1.047524  [  480/ 3200]\n",
            "loss: 0.847879  [  496/ 3200]\n",
            "loss: 0.640577  [  512/ 3200]\n",
            "loss: 0.807205  [  528/ 3200]\n",
            "loss: 1.118662  [  544/ 3200]\n",
            "loss: 1.167422  [  560/ 3200]\n",
            "loss: 1.064772  [  576/ 3200]\n",
            "loss: 1.088412  [  592/ 3200]\n",
            "loss: 1.032736  [  608/ 3200]\n",
            "loss: 1.232592  [  624/ 3200]\n",
            "loss: 1.006138  [  640/ 3200]\n",
            "loss: 1.109368  [  656/ 3200]\n",
            "loss: 0.983269  [  672/ 3200]\n",
            "loss: 0.990140  [  688/ 3200]\n",
            "loss: 0.931601  [  704/ 3200]\n",
            "loss: 0.884395  [  720/ 3200]\n",
            "loss: 0.780989  [  736/ 3200]\n",
            "loss: 1.073831  [  752/ 3200]\n",
            "loss: 1.077865  [  768/ 3200]\n",
            "loss: 0.844639  [  784/ 3200]\n",
            "loss: 0.654065  [  800/ 3200]\n",
            "loss: 0.806687  [  816/ 3200]\n",
            "loss: 0.793935  [  832/ 3200]\n",
            "loss: 0.902158  [  848/ 3200]\n",
            "loss: 1.068997  [  864/ 3200]\n",
            "loss: 0.878192  [  880/ 3200]\n",
            "loss: 0.852971  [  896/ 3200]\n",
            "loss: 1.203415  [  912/ 3200]\n",
            "loss: 0.945511  [  928/ 3200]\n",
            "loss: 1.084750  [  944/ 3200]\n",
            "loss: 0.920016  [  960/ 3200]\n",
            "loss: 1.030277  [  976/ 3200]\n",
            "loss: 0.884469  [  992/ 3200]\n",
            "loss: 1.053694  [ 1008/ 3200]\n",
            "loss: 0.930162  [ 1024/ 3200]\n",
            "loss: 0.854614  [ 1040/ 3200]\n",
            "loss: 0.895898  [ 1056/ 3200]\n",
            "loss: 1.155013  [ 1072/ 3200]\n",
            "loss: 1.014992  [ 1088/ 3200]\n",
            "loss: 0.868246  [ 1104/ 3200]\n",
            "loss: 0.928611  [ 1120/ 3200]\n",
            "loss: 0.976798  [ 1136/ 3200]\n",
            "loss: 1.144228  [ 1152/ 3200]\n",
            "loss: 0.869533  [ 1168/ 3200]\n",
            "loss: 0.904474  [ 1184/ 3200]\n",
            "loss: 0.973298  [ 1200/ 3200]\n",
            "loss: 1.215422  [ 1216/ 3200]\n",
            "loss: 0.877710  [ 1232/ 3200]\n",
            "loss: 0.792139  [ 1248/ 3200]\n",
            "loss: 0.977173  [ 1264/ 3200]\n",
            "loss: 1.133676  [ 1280/ 3200]\n",
            "loss: 0.977440  [ 1296/ 3200]\n",
            "loss: 0.805087  [ 1312/ 3200]\n",
            "loss: 0.910393  [ 1328/ 3200]\n",
            "loss: 1.037276  [ 1344/ 3200]\n",
            "loss: 0.898024  [ 1360/ 3200]\n",
            "loss: 0.823536  [ 1376/ 3200]\n",
            "loss: 0.768444  [ 1392/ 3200]\n",
            "loss: 0.952454  [ 1408/ 3200]\n",
            "loss: 0.956093  [ 1424/ 3200]\n",
            "loss: 0.727303  [ 1440/ 3200]\n",
            "loss: 1.249741  [ 1456/ 3200]\n",
            "loss: 1.030697  [ 1472/ 3200]\n",
            "loss: 0.877172  [ 1488/ 3200]\n",
            "loss: 1.219530  [ 1504/ 3200]\n",
            "loss: 0.937442  [ 1520/ 3200]\n",
            "loss: 0.837922  [ 1536/ 3200]\n",
            "loss: 1.038315  [ 1552/ 3200]\n",
            "loss: 0.951520  [ 1568/ 3200]\n",
            "loss: 0.963732  [ 1584/ 3200]\n",
            "loss: 1.160850  [ 1600/ 3200]\n",
            "loss: 1.116928  [ 1616/ 3200]\n",
            "loss: 1.347090  [ 1632/ 3200]\n",
            "loss: 1.266443  [ 1648/ 3200]\n",
            "loss: 0.739674  [ 1664/ 3200]\n",
            "loss: 1.184064  [ 1680/ 3200]\n",
            "loss: 0.762865  [ 1696/ 3200]\n",
            "loss: 1.039158  [ 1712/ 3200]\n",
            "loss: 0.834871  [ 1728/ 3200]\n",
            "loss: 1.142426  [ 1744/ 3200]\n",
            "loss: 1.008869  [ 1760/ 3200]\n",
            "loss: 0.812172  [ 1776/ 3200]\n",
            "loss: 1.102539  [ 1792/ 3200]\n",
            "loss: 0.953199  [ 1808/ 3200]\n",
            "loss: 1.515452  [ 1824/ 3200]\n",
            "loss: 1.063682  [ 1840/ 3200]\n",
            "loss: 0.861094  [ 1856/ 3200]\n",
            "loss: 1.134279  [ 1872/ 3200]\n",
            "loss: 1.018757  [ 1888/ 3200]\n",
            "loss: 1.244496  [ 1904/ 3200]\n",
            "loss: 0.654693  [ 1920/ 3200]\n",
            "loss: 0.829880  [ 1936/ 3200]\n",
            "loss: 0.744059  [ 1952/ 3200]\n",
            "loss: 0.874119  [ 1968/ 3200]\n",
            "loss: 1.082611  [ 1984/ 3200]\n",
            "loss: 1.009561  [ 2000/ 3200]\n",
            "loss: 1.101337  [ 2016/ 3200]\n",
            "loss: 1.379357  [ 2032/ 3200]\n",
            "loss: 0.820842  [ 2048/ 3200]\n",
            "loss: 0.915539  [ 2064/ 3200]\n",
            "loss: 0.871969  [ 2080/ 3200]\n",
            "loss: 0.936313  [ 2096/ 3200]\n",
            "loss: 1.071259  [ 2112/ 3200]\n",
            "loss: 0.985770  [ 2128/ 3200]\n",
            "loss: 0.973613  [ 2144/ 3200]\n",
            "loss: 1.054956  [ 2160/ 3200]\n",
            "loss: 0.994239  [ 2176/ 3200]\n",
            "loss: 1.009157  [ 2192/ 3200]\n",
            "loss: 0.832039  [ 2208/ 3200]\n",
            "loss: 0.867410  [ 2224/ 3200]\n",
            "loss: 1.047674  [ 2240/ 3200]\n",
            "loss: 0.957465  [ 2256/ 3200]\n",
            "loss: 1.201943  [ 2272/ 3200]\n",
            "loss: 0.805041  [ 2288/ 3200]\n",
            "loss: 0.829997  [ 2304/ 3200]\n",
            "loss: 1.430383  [ 2320/ 3200]\n",
            "loss: 0.865395  [ 2336/ 3200]\n",
            "loss: 0.779297  [ 2352/ 3200]\n",
            "loss: 0.733658  [ 2368/ 3200]\n",
            "loss: 1.181136  [ 2384/ 3200]\n",
            "loss: 0.954435  [ 2400/ 3200]\n",
            "loss: 0.971164  [ 2416/ 3200]\n",
            "loss: 0.878020  [ 2432/ 3200]\n",
            "loss: 1.054878  [ 2448/ 3200]\n",
            "loss: 1.053724  [ 2464/ 3200]\n",
            "loss: 0.905507  [ 2480/ 3200]\n",
            "loss: 0.973073  [ 2496/ 3200]\n",
            "loss: 1.097680  [ 2512/ 3200]\n",
            "loss: 1.267151  [ 2528/ 3200]\n",
            "loss: 0.648163  [ 2544/ 3200]\n",
            "loss: 1.057990  [ 2560/ 3200]\n",
            "loss: 0.841744  [ 2576/ 3200]\n",
            "loss: 1.516121  [ 2592/ 3200]\n",
            "loss: 1.106023  [ 2608/ 3200]\n",
            "loss: 1.338406  [ 2624/ 3200]\n",
            "loss: 1.133658  [ 2640/ 3200]\n",
            "loss: 0.882068  [ 2656/ 3200]\n",
            "loss: 0.840791  [ 2672/ 3200]\n",
            "loss: 1.094130  [ 2688/ 3200]\n",
            "loss: 0.784234  [ 2704/ 3200]\n",
            "loss: 0.772837  [ 2720/ 3200]\n",
            "loss: 1.045901  [ 2736/ 3200]\n",
            "loss: 0.749802  [ 2752/ 3200]\n",
            "loss: 0.866127  [ 2768/ 3200]\n",
            "loss: 0.920775  [ 2784/ 3200]\n",
            "loss: 1.194860  [ 2800/ 3200]\n",
            "loss: 0.865539  [ 2816/ 3200]\n",
            "loss: 1.050315  [ 2832/ 3200]\n",
            "loss: 0.947560  [ 2848/ 3200]\n",
            "loss: 0.989469  [ 2864/ 3200]\n",
            "loss: 1.089195  [ 2880/ 3200]\n",
            "loss: 0.953683  [ 2896/ 3200]\n",
            "loss: 1.005872  [ 2912/ 3200]\n",
            "loss: 1.442185  [ 2928/ 3200]\n",
            "loss: 0.829554  [ 2944/ 3200]\n",
            "loss: 1.394327  [ 2960/ 3200]\n",
            "loss: 0.952316  [ 2976/ 3200]\n",
            "loss: 0.963891  [ 2992/ 3200]\n",
            "loss: 0.833763  [ 3008/ 3200]\n",
            "loss: 0.870815  [ 3024/ 3200]\n",
            "loss: 0.804881  [ 3040/ 3200]\n",
            "loss: 1.063677  [ 3056/ 3200]\n",
            "loss: 0.937673  [ 3072/ 3200]\n",
            "loss: 0.899383  [ 3088/ 3200]\n",
            "loss: 0.886792  [ 3104/ 3200]\n",
            "loss: 1.219771  [ 3120/ 3200]\n",
            "loss: 0.889575  [ 3136/ 3200]\n",
            "loss: 0.919094  [ 3152/ 3200]\n",
            "loss: 0.677778  [ 3168/ 3200]\n",
            "loss: 1.022265  [ 3184/ 3200]\n",
            "current epoch: 20\n",
            "\n",
            "loss: 1.122467  [    0/ 3200]\n",
            "loss: 1.024976  [   16/ 3200]\n",
            "loss: 1.013292  [   32/ 3200]\n",
            "loss: 0.862852  [   48/ 3200]\n",
            "loss: 1.007976  [   64/ 3200]\n",
            "loss: 1.148613  [   80/ 3200]\n",
            "loss: 0.909351  [   96/ 3200]\n",
            "loss: 0.881040  [  112/ 3200]\n",
            "loss: 1.472653  [  128/ 3200]\n",
            "loss: 1.030635  [  144/ 3200]\n",
            "loss: 0.999917  [  160/ 3200]\n",
            "loss: 1.497784  [  176/ 3200]\n",
            "loss: 1.116566  [  192/ 3200]\n",
            "loss: 0.916342  [  208/ 3200]\n",
            "loss: 0.909730  [  224/ 3200]\n",
            "loss: 0.834921  [  240/ 3200]\n",
            "loss: 1.067509  [  256/ 3200]\n",
            "loss: 0.902054  [  272/ 3200]\n",
            "loss: 1.274777  [  288/ 3200]\n",
            "loss: 1.027768  [  304/ 3200]\n",
            "loss: 0.983117  [  320/ 3200]\n",
            "loss: 0.991843  [  336/ 3200]\n",
            "loss: 0.868232  [  352/ 3200]\n",
            "loss: 0.951319  [  368/ 3200]\n",
            "loss: 0.838247  [  384/ 3200]\n",
            "loss: 0.853540  [  400/ 3200]\n",
            "loss: 1.164277  [  416/ 3200]\n",
            "loss: 0.799446  [  432/ 3200]\n",
            "loss: 0.685133  [  448/ 3200]\n",
            "loss: 1.100674  [  464/ 3200]\n",
            "loss: 0.945161  [  480/ 3200]\n",
            "loss: 0.938824  [  496/ 3200]\n",
            "loss: 1.034781  [  512/ 3200]\n",
            "loss: 1.120848  [  528/ 3200]\n",
            "loss: 1.093851  [  544/ 3200]\n",
            "loss: 0.992523  [  560/ 3200]\n",
            "loss: 0.992738  [  576/ 3200]\n",
            "loss: 1.103964  [  592/ 3200]\n",
            "loss: 0.755645  [  608/ 3200]\n",
            "loss: 0.814434  [  624/ 3200]\n",
            "loss: 1.022293  [  640/ 3200]\n",
            "loss: 1.235688  [  656/ 3200]\n",
            "loss: 0.722955  [  672/ 3200]\n",
            "loss: 1.093839  [  688/ 3200]\n",
            "loss: 0.824096  [  704/ 3200]\n",
            "loss: 0.930867  [  720/ 3200]\n",
            "loss: 0.699342  [  736/ 3200]\n",
            "loss: 0.822772  [  752/ 3200]\n",
            "loss: 0.769455  [  768/ 3200]\n",
            "loss: 1.200403  [  784/ 3200]\n",
            "loss: 1.157825  [  800/ 3200]\n",
            "loss: 1.098789  [  816/ 3200]\n",
            "loss: 1.093418  [  832/ 3200]\n",
            "loss: 0.954414  [  848/ 3200]\n",
            "loss: 0.934929  [  864/ 3200]\n",
            "loss: 1.274902  [  880/ 3200]\n",
            "loss: 0.827825  [  896/ 3200]\n",
            "loss: 0.705802  [  912/ 3200]\n",
            "loss: 0.921963  [  928/ 3200]\n",
            "loss: 0.770644  [  944/ 3200]\n",
            "loss: 0.840944  [  960/ 3200]\n",
            "loss: 0.972648  [  976/ 3200]\n",
            "loss: 0.917191  [  992/ 3200]\n",
            "loss: 0.751043  [ 1008/ 3200]\n",
            "loss: 0.973176  [ 1024/ 3200]\n",
            "loss: 1.005813  [ 1040/ 3200]\n",
            "loss: 1.050596  [ 1056/ 3200]\n",
            "loss: 0.913953  [ 1072/ 3200]\n",
            "loss: 1.047185  [ 1088/ 3200]\n",
            "loss: 1.152106  [ 1104/ 3200]\n",
            "loss: 0.880675  [ 1120/ 3200]\n",
            "loss: 0.808795  [ 1136/ 3200]\n",
            "loss: 0.672283  [ 1152/ 3200]\n",
            "loss: 0.965918  [ 1168/ 3200]\n",
            "loss: 1.020475  [ 1184/ 3200]\n",
            "loss: 1.078559  [ 1200/ 3200]\n",
            "loss: 0.984585  [ 1216/ 3200]\n",
            "loss: 0.905719  [ 1232/ 3200]\n",
            "loss: 0.825890  [ 1248/ 3200]\n",
            "loss: 1.103818  [ 1264/ 3200]\n",
            "loss: 1.397862  [ 1280/ 3200]\n",
            "loss: 0.680159  [ 1296/ 3200]\n",
            "loss: 1.238861  [ 1312/ 3200]\n",
            "loss: 1.126828  [ 1328/ 3200]\n",
            "loss: 0.889299  [ 1344/ 3200]\n",
            "loss: 0.738283  [ 1360/ 3200]\n",
            "loss: 0.969986  [ 1376/ 3200]\n",
            "loss: 1.283078  [ 1392/ 3200]\n",
            "loss: 0.989697  [ 1408/ 3200]\n",
            "loss: 0.843812  [ 1424/ 3200]\n",
            "loss: 1.104096  [ 1440/ 3200]\n",
            "loss: 0.809658  [ 1456/ 3200]\n",
            "loss: 1.137963  [ 1472/ 3200]\n",
            "loss: 1.056382  [ 1488/ 3200]\n",
            "loss: 0.821160  [ 1504/ 3200]\n",
            "loss: 0.972239  [ 1520/ 3200]\n",
            "loss: 0.886029  [ 1536/ 3200]\n",
            "loss: 1.079822  [ 1552/ 3200]\n",
            "loss: 1.052901  [ 1568/ 3200]\n",
            "loss: 1.074392  [ 1584/ 3200]\n",
            "loss: 1.042438  [ 1600/ 3200]\n",
            "loss: 1.142627  [ 1616/ 3200]\n",
            "loss: 0.977636  [ 1632/ 3200]\n",
            "loss: 1.270198  [ 1648/ 3200]\n",
            "loss: 0.830489  [ 1664/ 3200]\n",
            "loss: 0.897030  [ 1680/ 3200]\n",
            "loss: 0.897693  [ 1696/ 3200]\n",
            "loss: 1.239146  [ 1712/ 3200]\n",
            "loss: 0.947882  [ 1728/ 3200]\n",
            "loss: 0.828898  [ 1744/ 3200]\n",
            "loss: 1.020747  [ 1760/ 3200]\n",
            "loss: 1.064022  [ 1776/ 3200]\n",
            "loss: 0.921898  [ 1792/ 3200]\n",
            "loss: 0.962253  [ 1808/ 3200]\n",
            "loss: 1.181907  [ 1824/ 3200]\n",
            "loss: 0.863604  [ 1840/ 3200]\n",
            "loss: 0.836778  [ 1856/ 3200]\n",
            "loss: 0.717881  [ 1872/ 3200]\n",
            "loss: 0.909028  [ 1888/ 3200]\n",
            "loss: 1.069101  [ 1904/ 3200]\n",
            "loss: 1.037380  [ 1920/ 3200]\n",
            "loss: 0.932489  [ 1936/ 3200]\n",
            "loss: 1.115446  [ 1952/ 3200]\n",
            "loss: 0.880176  [ 1968/ 3200]\n",
            "loss: 0.876721  [ 1984/ 3200]\n",
            "loss: 0.790567  [ 2000/ 3200]\n",
            "loss: 1.001850  [ 2016/ 3200]\n",
            "loss: 0.881862  [ 2032/ 3200]\n",
            "loss: 0.933908  [ 2048/ 3200]\n",
            "loss: 1.028394  [ 2064/ 3200]\n",
            "loss: 0.773722  [ 2080/ 3200]\n",
            "loss: 0.980514  [ 2096/ 3200]\n",
            "loss: 0.925856  [ 2112/ 3200]\n",
            "loss: 1.009492  [ 2128/ 3200]\n",
            "loss: 1.018113  [ 2144/ 3200]\n",
            "loss: 0.746840  [ 2160/ 3200]\n",
            "loss: 1.036968  [ 2176/ 3200]\n",
            "loss: 1.112635  [ 2192/ 3200]\n",
            "loss: 0.728675  [ 2208/ 3200]\n",
            "loss: 0.943027  [ 2224/ 3200]\n",
            "loss: 1.406106  [ 2240/ 3200]\n",
            "loss: 1.102808  [ 2256/ 3200]\n",
            "loss: 1.008065  [ 2272/ 3200]\n",
            "loss: 0.992538  [ 2288/ 3200]\n",
            "loss: 0.788238  [ 2304/ 3200]\n",
            "loss: 1.101250  [ 2320/ 3200]\n",
            "loss: 0.928937  [ 2336/ 3200]\n",
            "loss: 1.066785  [ 2352/ 3200]\n",
            "loss: 1.238255  [ 2368/ 3200]\n",
            "loss: 0.841172  [ 2384/ 3200]\n",
            "loss: 0.798270  [ 2400/ 3200]\n",
            "loss: 0.955140  [ 2416/ 3200]\n",
            "loss: 0.869678  [ 2432/ 3200]\n",
            "loss: 1.196032  [ 2448/ 3200]\n",
            "loss: 0.562602  [ 2464/ 3200]\n",
            "loss: 0.844395  [ 2480/ 3200]\n",
            "loss: 1.081352  [ 2496/ 3200]\n",
            "loss: 0.898490  [ 2512/ 3200]\n",
            "loss: 1.034799  [ 2528/ 3200]\n",
            "loss: 1.225600  [ 2544/ 3200]\n",
            "loss: 1.075153  [ 2560/ 3200]\n",
            "loss: 0.924006  [ 2576/ 3200]\n",
            "loss: 1.086915  [ 2592/ 3200]\n",
            "loss: 0.897021  [ 2608/ 3200]\n",
            "loss: 1.343362  [ 2624/ 3200]\n",
            "loss: 1.065686  [ 2640/ 3200]\n",
            "loss: 0.936392  [ 2656/ 3200]\n",
            "loss: 0.949242  [ 2672/ 3200]\n",
            "loss: 1.017395  [ 2688/ 3200]\n",
            "loss: 1.091822  [ 2704/ 3200]\n",
            "loss: 0.796930  [ 2720/ 3200]\n",
            "loss: 0.838159  [ 2736/ 3200]\n",
            "loss: 1.078186  [ 2752/ 3200]\n",
            "loss: 0.818543  [ 2768/ 3200]\n",
            "loss: 1.103088  [ 2784/ 3200]\n",
            "loss: 1.012689  [ 2800/ 3200]\n",
            "loss: 0.978840  [ 2816/ 3200]\n",
            "loss: 0.870752  [ 2832/ 3200]\n",
            "loss: 1.051624  [ 2848/ 3200]\n",
            "loss: 0.757059  [ 2864/ 3200]\n",
            "loss: 0.882009  [ 2880/ 3200]\n",
            "loss: 0.796191  [ 2896/ 3200]\n",
            "loss: 0.787994  [ 2912/ 3200]\n",
            "loss: 1.387844  [ 2928/ 3200]\n",
            "loss: 0.943671  [ 2944/ 3200]\n",
            "loss: 0.858283  [ 2960/ 3200]\n",
            "loss: 1.006462  [ 2976/ 3200]\n",
            "loss: 0.916315  [ 2992/ 3200]\n",
            "loss: 0.949749  [ 3008/ 3200]\n",
            "loss: 0.964796  [ 3024/ 3200]\n",
            "loss: 0.614116  [ 3040/ 3200]\n",
            "loss: 1.158112  [ 3056/ 3200]\n",
            "loss: 1.070009  [ 3072/ 3200]\n",
            "loss: 1.046418  [ 3088/ 3200]\n",
            "loss: 0.880115  [ 3104/ 3200]\n",
            "loss: 0.660300  [ 3120/ 3200]\n",
            "loss: 0.962591  [ 3136/ 3200]\n",
            "loss: 0.996610  [ 3152/ 3200]\n",
            "loss: 0.989024  [ 3168/ 3200]\n",
            "loss: 1.217393  [ 3184/ 3200]\n",
            "current epoch: 21\n",
            "\n",
            "loss: 1.063406  [    0/ 3200]\n",
            "loss: 0.904471  [   16/ 3200]\n",
            "loss: 1.217201  [   32/ 3200]\n",
            "loss: 0.844788  [   48/ 3200]\n",
            "loss: 0.968997  [   64/ 3200]\n",
            "loss: 1.033514  [   80/ 3200]\n",
            "loss: 1.145519  [   96/ 3200]\n",
            "loss: 1.140458  [  112/ 3200]\n",
            "loss: 1.073100  [  128/ 3200]\n",
            "loss: 0.796645  [  144/ 3200]\n",
            "loss: 0.894219  [  160/ 3200]\n",
            "loss: 0.895209  [  176/ 3200]\n",
            "loss: 0.968291  [  192/ 3200]\n",
            "loss: 0.946388  [  208/ 3200]\n",
            "loss: 0.850230  [  224/ 3200]\n",
            "loss: 0.685253  [  240/ 3200]\n",
            "loss: 1.363568  [  256/ 3200]\n",
            "loss: 0.969268  [  272/ 3200]\n",
            "loss: 0.700674  [  288/ 3200]\n",
            "loss: 0.978311  [  304/ 3200]\n",
            "loss: 0.899115  [  320/ 3200]\n",
            "loss: 1.046475  [  336/ 3200]\n",
            "loss: 0.679762  [  352/ 3200]\n",
            "loss: 0.773564  [  368/ 3200]\n",
            "loss: 0.770832  [  384/ 3200]\n",
            "loss: 1.078694  [  400/ 3200]\n",
            "loss: 1.015549  [  416/ 3200]\n",
            "loss: 1.227721  [  432/ 3200]\n",
            "loss: 1.109444  [  448/ 3200]\n",
            "loss: 1.048318  [  464/ 3200]\n",
            "loss: 0.915890  [  480/ 3200]\n",
            "loss: 0.848498  [  496/ 3200]\n",
            "loss: 1.240635  [  512/ 3200]\n",
            "loss: 0.689089  [  528/ 3200]\n",
            "loss: 0.834859  [  544/ 3200]\n",
            "loss: 0.902284  [  560/ 3200]\n",
            "loss: 0.865116  [  576/ 3200]\n",
            "loss: 1.199742  [  592/ 3200]\n",
            "loss: 0.918321  [  608/ 3200]\n",
            "loss: 1.079814  [  624/ 3200]\n",
            "loss: 0.832969  [  640/ 3200]\n",
            "loss: 0.989787  [  656/ 3200]\n",
            "loss: 1.388059  [  672/ 3200]\n",
            "loss: 0.697710  [  688/ 3200]\n",
            "loss: 1.037627  [  704/ 3200]\n",
            "loss: 1.165960  [  720/ 3200]\n",
            "loss: 0.958755  [  736/ 3200]\n",
            "loss: 1.311852  [  752/ 3200]\n",
            "loss: 1.228786  [  768/ 3200]\n",
            "loss: 0.844134  [  784/ 3200]\n",
            "loss: 0.742894  [  800/ 3200]\n",
            "loss: 0.841578  [  816/ 3200]\n",
            "loss: 1.042569  [  832/ 3200]\n",
            "loss: 1.190674  [  848/ 3200]\n",
            "loss: 0.927866  [  864/ 3200]\n",
            "loss: 0.758148  [  880/ 3200]\n",
            "loss: 1.266641  [  896/ 3200]\n",
            "loss: 0.839671  [  912/ 3200]\n",
            "loss: 1.014757  [  928/ 3200]\n",
            "loss: 0.839557  [  944/ 3200]\n",
            "loss: 0.675046  [  960/ 3200]\n",
            "loss: 1.053412  [  976/ 3200]\n",
            "loss: 0.956991  [  992/ 3200]\n",
            "loss: 1.241202  [ 1008/ 3200]\n",
            "loss: 0.885239  [ 1024/ 3200]\n",
            "loss: 1.042811  [ 1040/ 3200]\n",
            "loss: 0.717625  [ 1056/ 3200]\n",
            "loss: 0.936293  [ 1072/ 3200]\n",
            "loss: 1.202705  [ 1088/ 3200]\n",
            "loss: 0.753026  [ 1104/ 3200]\n",
            "loss: 0.743980  [ 1120/ 3200]\n",
            "loss: 1.037791  [ 1136/ 3200]\n",
            "loss: 1.286041  [ 1152/ 3200]\n",
            "loss: 0.833576  [ 1168/ 3200]\n",
            "loss: 1.053447  [ 1184/ 3200]\n",
            "loss: 1.412963  [ 1200/ 3200]\n",
            "loss: 0.980033  [ 1216/ 3200]\n",
            "loss: 0.993940  [ 1232/ 3200]\n",
            "loss: 1.029331  [ 1248/ 3200]\n",
            "loss: 1.026123  [ 1264/ 3200]\n",
            "loss: 0.769629  [ 1280/ 3200]\n",
            "loss: 0.637703  [ 1296/ 3200]\n",
            "loss: 0.845488  [ 1312/ 3200]\n",
            "loss: 0.796945  [ 1328/ 3200]\n",
            "loss: 0.816079  [ 1344/ 3200]\n",
            "loss: 0.735217  [ 1360/ 3200]\n",
            "loss: 0.991858  [ 1376/ 3200]\n",
            "loss: 1.000232  [ 1392/ 3200]\n",
            "loss: 1.099861  [ 1408/ 3200]\n",
            "loss: 1.025493  [ 1424/ 3200]\n",
            "loss: 1.311108  [ 1440/ 3200]\n",
            "loss: 0.800400  [ 1456/ 3200]\n",
            "loss: 1.279574  [ 1472/ 3200]\n",
            "loss: 0.888459  [ 1488/ 3200]\n",
            "loss: 1.187342  [ 1504/ 3200]\n",
            "loss: 0.739686  [ 1520/ 3200]\n",
            "loss: 0.867826  [ 1536/ 3200]\n",
            "loss: 0.886473  [ 1552/ 3200]\n",
            "loss: 0.686231  [ 1568/ 3200]\n",
            "loss: 1.064934  [ 1584/ 3200]\n",
            "loss: 0.864583  [ 1600/ 3200]\n",
            "loss: 1.041581  [ 1616/ 3200]\n",
            "loss: 0.977242  [ 1632/ 3200]\n",
            "loss: 1.099640  [ 1648/ 3200]\n",
            "loss: 0.991369  [ 1664/ 3200]\n",
            "loss: 1.041181  [ 1680/ 3200]\n",
            "loss: 0.892886  [ 1696/ 3200]\n",
            "loss: 0.941771  [ 1712/ 3200]\n",
            "loss: 1.111873  [ 1728/ 3200]\n",
            "loss: 0.832399  [ 1744/ 3200]\n",
            "loss: 0.777026  [ 1760/ 3200]\n",
            "loss: 0.880822  [ 1776/ 3200]\n",
            "loss: 0.786084  [ 1792/ 3200]\n",
            "loss: 0.784353  [ 1808/ 3200]\n",
            "loss: 0.885649  [ 1824/ 3200]\n",
            "loss: 1.017937  [ 1840/ 3200]\n",
            "loss: 0.984260  [ 1856/ 3200]\n",
            "loss: 0.847332  [ 1872/ 3200]\n",
            "loss: 0.953942  [ 1888/ 3200]\n",
            "loss: 0.905814  [ 1904/ 3200]\n",
            "loss: 0.811669  [ 1920/ 3200]\n",
            "loss: 0.816865  [ 1936/ 3200]\n",
            "loss: 1.202603  [ 1952/ 3200]\n",
            "loss: 0.577799  [ 1968/ 3200]\n",
            "loss: 1.044540  [ 1984/ 3200]\n",
            "loss: 1.186594  [ 2000/ 3200]\n",
            "loss: 0.988477  [ 2016/ 3200]\n",
            "loss: 0.889216  [ 2032/ 3200]\n",
            "loss: 0.925143  [ 2048/ 3200]\n",
            "loss: 1.249207  [ 2064/ 3200]\n",
            "loss: 1.308378  [ 2080/ 3200]\n",
            "loss: 1.134815  [ 2096/ 3200]\n",
            "loss: 1.022903  [ 2112/ 3200]\n",
            "loss: 1.020859  [ 2128/ 3200]\n",
            "loss: 1.219749  [ 2144/ 3200]\n",
            "loss: 0.966744  [ 2160/ 3200]\n",
            "loss: 1.041921  [ 2176/ 3200]\n",
            "loss: 0.913525  [ 2192/ 3200]\n",
            "loss: 0.604290  [ 2208/ 3200]\n",
            "loss: 0.899432  [ 2224/ 3200]\n",
            "loss: 1.106912  [ 2240/ 3200]\n",
            "loss: 0.832003  [ 2256/ 3200]\n",
            "loss: 1.030801  [ 2272/ 3200]\n",
            "loss: 0.778434  [ 2288/ 3200]\n",
            "loss: 0.972069  [ 2304/ 3200]\n",
            "loss: 0.976795  [ 2320/ 3200]\n",
            "loss: 1.121865  [ 2336/ 3200]\n",
            "loss: 1.017813  [ 2352/ 3200]\n",
            "loss: 1.118375  [ 2368/ 3200]\n",
            "loss: 1.024553  [ 2384/ 3200]\n",
            "loss: 0.947844  [ 2400/ 3200]\n",
            "loss: 1.007181  [ 2416/ 3200]\n",
            "loss: 1.073276  [ 2432/ 3200]\n",
            "loss: 1.220226  [ 2448/ 3200]\n",
            "loss: 1.359513  [ 2464/ 3200]\n",
            "loss: 0.923178  [ 2480/ 3200]\n",
            "loss: 0.860957  [ 2496/ 3200]\n",
            "loss: 1.094275  [ 2512/ 3200]\n",
            "loss: 0.819234  [ 2528/ 3200]\n",
            "loss: 1.025200  [ 2544/ 3200]\n",
            "loss: 0.981897  [ 2560/ 3200]\n",
            "loss: 0.948801  [ 2576/ 3200]\n",
            "loss: 0.904484  [ 2592/ 3200]\n",
            "loss: 0.976046  [ 2608/ 3200]\n",
            "loss: 0.814388  [ 2624/ 3200]\n",
            "loss: 0.826240  [ 2640/ 3200]\n",
            "loss: 0.891442  [ 2656/ 3200]\n",
            "loss: 1.110736  [ 2672/ 3200]\n",
            "loss: 1.194941  [ 2688/ 3200]\n",
            "loss: 0.991476  [ 2704/ 3200]\n",
            "loss: 0.826431  [ 2720/ 3200]\n",
            "loss: 0.903980  [ 2736/ 3200]\n",
            "loss: 1.001118  [ 2752/ 3200]\n",
            "loss: 1.193655  [ 2768/ 3200]\n",
            "loss: 0.921443  [ 2784/ 3200]\n",
            "loss: 1.040110  [ 2800/ 3200]\n",
            "loss: 0.761704  [ 2816/ 3200]\n",
            "loss: 1.018101  [ 2832/ 3200]\n",
            "loss: 0.795314  [ 2848/ 3200]\n",
            "loss: 1.134172  [ 2864/ 3200]\n",
            "loss: 1.132626  [ 2880/ 3200]\n",
            "loss: 0.800602  [ 2896/ 3200]\n",
            "loss: 0.886021  [ 2912/ 3200]\n",
            "loss: 0.935645  [ 2928/ 3200]\n",
            "loss: 0.873114  [ 2944/ 3200]\n",
            "loss: 1.299603  [ 2960/ 3200]\n",
            "loss: 1.250335  [ 2976/ 3200]\n",
            "loss: 0.752034  [ 2992/ 3200]\n",
            "loss: 1.340270  [ 3008/ 3200]\n",
            "loss: 1.306066  [ 3024/ 3200]\n",
            "loss: 1.049550  [ 3040/ 3200]\n",
            "loss: 0.869812  [ 3056/ 3200]\n",
            "loss: 1.107132  [ 3072/ 3200]\n",
            "loss: 1.114431  [ 3088/ 3200]\n",
            "loss: 0.935151  [ 3104/ 3200]\n",
            "loss: 0.617957  [ 3120/ 3200]\n",
            "loss: 0.669663  [ 3136/ 3200]\n",
            "loss: 0.999583  [ 3152/ 3200]\n",
            "loss: 0.954830  [ 3168/ 3200]\n",
            "loss: 1.108319  [ 3184/ 3200]\n",
            "current epoch: 22\n",
            "\n",
            "loss: 1.162425  [    0/ 3200]\n",
            "loss: 0.919811  [   16/ 3200]\n",
            "loss: 0.851218  [   32/ 3200]\n",
            "loss: 1.053637  [   48/ 3200]\n",
            "loss: 1.126835  [   64/ 3200]\n",
            "loss: 1.071593  [   80/ 3200]\n",
            "loss: 1.074480  [   96/ 3200]\n",
            "loss: 0.637753  [  112/ 3200]\n",
            "loss: 1.034590  [  128/ 3200]\n",
            "loss: 0.769685  [  144/ 3200]\n",
            "loss: 0.968505  [  160/ 3200]\n",
            "loss: 1.037427  [  176/ 3200]\n",
            "loss: 1.049504  [  192/ 3200]\n",
            "loss: 0.886399  [  208/ 3200]\n",
            "loss: 0.697736  [  224/ 3200]\n",
            "loss: 0.975931  [  240/ 3200]\n",
            "loss: 0.792052  [  256/ 3200]\n",
            "loss: 0.750119  [  272/ 3200]\n",
            "loss: 0.857170  [  288/ 3200]\n",
            "loss: 0.830238  [  304/ 3200]\n",
            "loss: 0.963166  [  320/ 3200]\n",
            "loss: 1.156624  [  336/ 3200]\n",
            "loss: 0.788575  [  352/ 3200]\n",
            "loss: 1.007770  [  368/ 3200]\n",
            "loss: 0.844660  [  384/ 3200]\n",
            "loss: 0.778940  [  400/ 3200]\n",
            "loss: 0.838250  [  416/ 3200]\n",
            "loss: 0.939864  [  432/ 3200]\n",
            "loss: 1.052274  [  448/ 3200]\n",
            "loss: 1.010555  [  464/ 3200]\n",
            "loss: 1.298379  [  480/ 3200]\n",
            "loss: 1.010947  [  496/ 3200]\n",
            "loss: 1.104296  [  512/ 3200]\n",
            "loss: 1.188607  [  528/ 3200]\n",
            "loss: 0.757938  [  544/ 3200]\n",
            "loss: 0.994634  [  560/ 3200]\n",
            "loss: 0.752999  [  576/ 3200]\n",
            "loss: 1.322649  [  592/ 3200]\n",
            "loss: 0.915582  [  608/ 3200]\n",
            "loss: 0.850413  [  624/ 3200]\n",
            "loss: 0.999771  [  640/ 3200]\n",
            "loss: 0.963007  [  656/ 3200]\n",
            "loss: 0.883394  [  672/ 3200]\n",
            "loss: 0.948550  [  688/ 3200]\n",
            "loss: 0.916051  [  704/ 3200]\n",
            "loss: 0.753612  [  720/ 3200]\n",
            "loss: 1.085653  [  736/ 3200]\n",
            "loss: 1.299412  [  752/ 3200]\n",
            "loss: 1.066926  [  768/ 3200]\n",
            "loss: 1.186116  [  784/ 3200]\n",
            "loss: 0.995937  [  800/ 3200]\n",
            "loss: 0.990540  [  816/ 3200]\n",
            "loss: 0.993054  [  832/ 3200]\n",
            "loss: 0.923169  [  848/ 3200]\n",
            "loss: 1.013124  [  864/ 3200]\n",
            "loss: 0.634234  [  880/ 3200]\n",
            "loss: 0.949270  [  896/ 3200]\n",
            "loss: 1.159412  [  912/ 3200]\n",
            "loss: 1.182677  [  928/ 3200]\n",
            "loss: 0.913034  [  944/ 3200]\n",
            "loss: 1.151062  [  960/ 3200]\n",
            "loss: 1.021936  [  976/ 3200]\n",
            "loss: 0.762228  [  992/ 3200]\n",
            "loss: 1.098772  [ 1008/ 3200]\n",
            "loss: 0.954962  [ 1024/ 3200]\n",
            "loss: 0.895500  [ 1040/ 3200]\n",
            "loss: 1.014467  [ 1056/ 3200]\n",
            "loss: 0.772781  [ 1072/ 3200]\n",
            "loss: 1.356206  [ 1088/ 3200]\n",
            "loss: 1.344079  [ 1104/ 3200]\n",
            "loss: 1.022591  [ 1120/ 3200]\n",
            "loss: 0.803350  [ 1136/ 3200]\n",
            "loss: 0.993774  [ 1152/ 3200]\n",
            "loss: 1.013508  [ 1168/ 3200]\n",
            "loss: 0.902903  [ 1184/ 3200]\n",
            "loss: 0.998335  [ 1200/ 3200]\n",
            "loss: 0.986565  [ 1216/ 3200]\n",
            "loss: 0.895425  [ 1232/ 3200]\n",
            "loss: 0.699455  [ 1248/ 3200]\n",
            "loss: 0.958089  [ 1264/ 3200]\n",
            "loss: 1.085827  [ 1280/ 3200]\n",
            "loss: 1.157050  [ 1296/ 3200]\n",
            "loss: 0.885630  [ 1312/ 3200]\n",
            "loss: 1.147037  [ 1328/ 3200]\n",
            "loss: 0.956296  [ 1344/ 3200]\n",
            "loss: 0.929861  [ 1360/ 3200]\n",
            "loss: 0.969628  [ 1376/ 3200]\n",
            "loss: 0.834242  [ 1392/ 3200]\n",
            "loss: 0.887714  [ 1408/ 3200]\n",
            "loss: 0.777786  [ 1424/ 3200]\n",
            "loss: 0.922136  [ 1440/ 3200]\n",
            "loss: 1.105571  [ 1456/ 3200]\n",
            "loss: 0.948140  [ 1472/ 3200]\n",
            "loss: 0.991122  [ 1488/ 3200]\n",
            "loss: 0.949531  [ 1504/ 3200]\n",
            "loss: 1.027254  [ 1520/ 3200]\n",
            "loss: 0.960324  [ 1536/ 3200]\n",
            "loss: 0.854768  [ 1552/ 3200]\n",
            "loss: 0.999870  [ 1568/ 3200]\n",
            "loss: 0.925175  [ 1584/ 3200]\n",
            "loss: 1.004614  [ 1600/ 3200]\n",
            "loss: 1.019071  [ 1616/ 3200]\n",
            "loss: 0.910985  [ 1632/ 3200]\n",
            "loss: 1.254540  [ 1648/ 3200]\n",
            "loss: 0.758053  [ 1664/ 3200]\n",
            "loss: 0.816502  [ 1680/ 3200]\n",
            "loss: 0.888742  [ 1696/ 3200]\n",
            "loss: 1.243740  [ 1712/ 3200]\n",
            "loss: 0.898456  [ 1728/ 3200]\n",
            "loss: 0.919242  [ 1744/ 3200]\n",
            "loss: 0.717293  [ 1760/ 3200]\n",
            "loss: 0.845205  [ 1776/ 3200]\n",
            "loss: 0.644039  [ 1792/ 3200]\n",
            "loss: 0.779055  [ 1808/ 3200]\n",
            "loss: 0.897937  [ 1824/ 3200]\n",
            "loss: 0.752173  [ 1840/ 3200]\n",
            "loss: 0.884987  [ 1856/ 3200]\n",
            "loss: 1.334944  [ 1872/ 3200]\n",
            "loss: 1.053443  [ 1888/ 3200]\n",
            "loss: 1.021630  [ 1904/ 3200]\n",
            "loss: 0.919679  [ 1920/ 3200]\n",
            "loss: 0.985998  [ 1936/ 3200]\n",
            "loss: 1.294519  [ 1952/ 3200]\n",
            "loss: 0.929328  [ 1968/ 3200]\n",
            "loss: 1.034550  [ 1984/ 3200]\n",
            "loss: 0.891481  [ 2000/ 3200]\n",
            "loss: 0.706496  [ 2016/ 3200]\n",
            "loss: 0.854731  [ 2032/ 3200]\n",
            "loss: 0.825917  [ 2048/ 3200]\n",
            "loss: 0.939721  [ 2064/ 3200]\n",
            "loss: 0.893385  [ 2080/ 3200]\n",
            "loss: 0.778781  [ 2096/ 3200]\n",
            "loss: 0.701229  [ 2112/ 3200]\n",
            "loss: 0.874041  [ 2128/ 3200]\n",
            "loss: 1.177935  [ 2144/ 3200]\n",
            "loss: 0.849016  [ 2160/ 3200]\n",
            "loss: 1.026397  [ 2176/ 3200]\n",
            "loss: 1.058614  [ 2192/ 3200]\n",
            "loss: 0.996917  [ 2208/ 3200]\n",
            "loss: 0.863989  [ 2224/ 3200]\n",
            "loss: 0.859222  [ 2240/ 3200]\n",
            "loss: 1.056893  [ 2256/ 3200]\n",
            "loss: 1.128564  [ 2272/ 3200]\n",
            "loss: 1.181907  [ 2288/ 3200]\n",
            "loss: 0.828058  [ 2304/ 3200]\n",
            "loss: 0.895326  [ 2320/ 3200]\n",
            "loss: 0.760402  [ 2336/ 3200]\n",
            "loss: 0.789236  [ 2352/ 3200]\n",
            "loss: 0.914342  [ 2368/ 3200]\n",
            "loss: 0.833256  [ 2384/ 3200]\n",
            "loss: 0.948053  [ 2400/ 3200]\n",
            "loss: 0.933499  [ 2416/ 3200]\n",
            "loss: 0.745802  [ 2432/ 3200]\n",
            "loss: 0.900589  [ 2448/ 3200]\n",
            "loss: 0.814655  [ 2464/ 3200]\n",
            "loss: 1.155229  [ 2480/ 3200]\n",
            "loss: 1.065574  [ 2496/ 3200]\n",
            "loss: 0.945178  [ 2512/ 3200]\n",
            "loss: 0.800063  [ 2528/ 3200]\n",
            "loss: 0.815241  [ 2544/ 3200]\n",
            "loss: 0.978150  [ 2560/ 3200]\n",
            "loss: 1.201055  [ 2576/ 3200]\n",
            "loss: 0.924724  [ 2592/ 3200]\n",
            "loss: 1.116802  [ 2608/ 3200]\n",
            "loss: 1.114164  [ 2624/ 3200]\n",
            "loss: 0.890699  [ 2640/ 3200]\n",
            "loss: 1.123761  [ 2656/ 3200]\n",
            "loss: 1.423618  [ 2672/ 3200]\n",
            "loss: 0.948134  [ 2688/ 3200]\n",
            "loss: 1.006992  [ 2704/ 3200]\n",
            "loss: 0.955705  [ 2720/ 3200]\n",
            "loss: 1.237218  [ 2736/ 3200]\n",
            "loss: 0.904037  [ 2752/ 3200]\n",
            "loss: 0.833105  [ 2768/ 3200]\n",
            "loss: 0.718474  [ 2784/ 3200]\n",
            "loss: 1.112674  [ 2800/ 3200]\n",
            "loss: 0.732300  [ 2816/ 3200]\n",
            "loss: 1.109199  [ 2832/ 3200]\n",
            "loss: 0.984490  [ 2848/ 3200]\n",
            "loss: 1.151735  [ 2864/ 3200]\n",
            "loss: 1.024429  [ 2880/ 3200]\n",
            "loss: 1.198737  [ 2896/ 3200]\n",
            "loss: 1.149960  [ 2912/ 3200]\n",
            "loss: 1.125114  [ 2928/ 3200]\n",
            "loss: 0.814075  [ 2944/ 3200]\n",
            "loss: 1.017304  [ 2960/ 3200]\n",
            "loss: 0.912651  [ 2976/ 3200]\n",
            "loss: 0.809046  [ 2992/ 3200]\n",
            "loss: 0.798361  [ 3008/ 3200]\n",
            "loss: 1.173225  [ 3024/ 3200]\n",
            "loss: 0.973222  [ 3040/ 3200]\n",
            "loss: 1.194124  [ 3056/ 3200]\n",
            "loss: 0.987608  [ 3072/ 3200]\n",
            "loss: 0.908952  [ 3088/ 3200]\n",
            "loss: 1.156836  [ 3104/ 3200]\n",
            "loss: 0.638693  [ 3120/ 3200]\n",
            "loss: 0.766566  [ 3136/ 3200]\n",
            "loss: 1.051744  [ 3152/ 3200]\n",
            "loss: 1.328147  [ 3168/ 3200]\n",
            "loss: 0.788364  [ 3184/ 3200]\n",
            "current epoch: 23\n",
            "\n",
            "loss: 1.007228  [    0/ 3200]\n",
            "loss: 0.982901  [   16/ 3200]\n",
            "loss: 0.834019  [   32/ 3200]\n",
            "loss: 1.161216  [   48/ 3200]\n",
            "loss: 1.054209  [   64/ 3200]\n",
            "loss: 0.766944  [   80/ 3200]\n",
            "loss: 0.936222  [   96/ 3200]\n",
            "loss: 0.793284  [  112/ 3200]\n",
            "loss: 0.826554  [  128/ 3200]\n",
            "loss: 1.184817  [  144/ 3200]\n",
            "loss: 0.884096  [  160/ 3200]\n",
            "loss: 0.870775  [  176/ 3200]\n",
            "loss: 0.740747  [  192/ 3200]\n",
            "loss: 0.905565  [  208/ 3200]\n",
            "loss: 0.807927  [  224/ 3200]\n",
            "loss: 0.908235  [  240/ 3200]\n",
            "loss: 0.960399  [  256/ 3200]\n",
            "loss: 1.032606  [  272/ 3200]\n",
            "loss: 1.023508  [  288/ 3200]\n",
            "loss: 0.895225  [  304/ 3200]\n",
            "loss: 1.078705  [  320/ 3200]\n",
            "loss: 1.035514  [  336/ 3200]\n",
            "loss: 0.650547  [  352/ 3200]\n",
            "loss: 0.845219  [  368/ 3200]\n",
            "loss: 0.703553  [  384/ 3200]\n",
            "loss: 0.909056  [  400/ 3200]\n",
            "loss: 0.909277  [  416/ 3200]\n",
            "loss: 1.001723  [  432/ 3200]\n",
            "loss: 0.973249  [  448/ 3200]\n",
            "loss: 1.105737  [  464/ 3200]\n",
            "loss: 1.065105  [  480/ 3200]\n",
            "loss: 0.978559  [  496/ 3200]\n",
            "loss: 0.972263  [  512/ 3200]\n",
            "loss: 0.650388  [  528/ 3200]\n",
            "loss: 0.894126  [  544/ 3200]\n",
            "loss: 0.730870  [  560/ 3200]\n",
            "loss: 1.186458  [  576/ 3200]\n",
            "loss: 0.859593  [  592/ 3200]\n",
            "loss: 1.136357  [  608/ 3200]\n",
            "loss: 0.746785  [  624/ 3200]\n",
            "loss: 1.175090  [  640/ 3200]\n",
            "loss: 0.569484  [  656/ 3200]\n",
            "loss: 0.961408  [  672/ 3200]\n",
            "loss: 0.990659  [  688/ 3200]\n",
            "loss: 0.957948  [  704/ 3200]\n",
            "loss: 0.949348  [  720/ 3200]\n",
            "loss: 1.082058  [  736/ 3200]\n",
            "loss: 0.936821  [  752/ 3200]\n",
            "loss: 0.676866  [  768/ 3200]\n",
            "loss: 0.837010  [  784/ 3200]\n",
            "loss: 1.070699  [  800/ 3200]\n",
            "loss: 0.690497  [  816/ 3200]\n",
            "loss: 0.907420  [  832/ 3200]\n",
            "loss: 0.763389  [  848/ 3200]\n",
            "loss: 1.432736  [  864/ 3200]\n",
            "loss: 0.916521  [  880/ 3200]\n",
            "loss: 1.025957  [  896/ 3200]\n",
            "loss: 1.233662  [  912/ 3200]\n",
            "loss: 1.251009  [  928/ 3200]\n",
            "loss: 0.925727  [  944/ 3200]\n",
            "loss: 0.841276  [  960/ 3200]\n",
            "loss: 0.977218  [  976/ 3200]\n",
            "loss: 0.930138  [  992/ 3200]\n",
            "loss: 1.121628  [ 1008/ 3200]\n",
            "loss: 0.906663  [ 1024/ 3200]\n",
            "loss: 1.102237  [ 1040/ 3200]\n",
            "loss: 1.014763  [ 1056/ 3200]\n",
            "loss: 0.988871  [ 1072/ 3200]\n",
            "loss: 0.829347  [ 1088/ 3200]\n",
            "loss: 1.176107  [ 1104/ 3200]\n",
            "loss: 0.997295  [ 1120/ 3200]\n",
            "loss: 1.211612  [ 1136/ 3200]\n",
            "loss: 1.007932  [ 1152/ 3200]\n",
            "loss: 0.781091  [ 1168/ 3200]\n",
            "loss: 0.940538  [ 1184/ 3200]\n",
            "loss: 1.080186  [ 1200/ 3200]\n",
            "loss: 0.873443  [ 1216/ 3200]\n",
            "loss: 1.008343  [ 1232/ 3200]\n",
            "loss: 1.191552  [ 1248/ 3200]\n",
            "loss: 0.867527  [ 1264/ 3200]\n",
            "loss: 1.051553  [ 1280/ 3200]\n",
            "loss: 1.142499  [ 1296/ 3200]\n",
            "loss: 0.591287  [ 1312/ 3200]\n",
            "loss: 1.230308  [ 1328/ 3200]\n",
            "loss: 0.993555  [ 1344/ 3200]\n",
            "loss: 0.978500  [ 1360/ 3200]\n",
            "loss: 0.829868  [ 1376/ 3200]\n",
            "loss: 0.932441  [ 1392/ 3200]\n",
            "loss: 0.903725  [ 1408/ 3200]\n",
            "loss: 0.776722  [ 1424/ 3200]\n",
            "loss: 0.965877  [ 1440/ 3200]\n",
            "loss: 0.801563  [ 1456/ 3200]\n",
            "loss: 0.819487  [ 1472/ 3200]\n",
            "loss: 1.279738  [ 1488/ 3200]\n",
            "loss: 1.184934  [ 1504/ 3200]\n",
            "loss: 0.788955  [ 1520/ 3200]\n",
            "loss: 1.087508  [ 1536/ 3200]\n",
            "loss: 0.916042  [ 1552/ 3200]\n",
            "loss: 0.820516  [ 1568/ 3200]\n",
            "loss: 0.983742  [ 1584/ 3200]\n",
            "loss: 0.855068  [ 1600/ 3200]\n",
            "loss: 0.712928  [ 1616/ 3200]\n",
            "loss: 0.826457  [ 1632/ 3200]\n",
            "loss: 0.958212  [ 1648/ 3200]\n",
            "loss: 1.132602  [ 1664/ 3200]\n",
            "loss: 0.895851  [ 1680/ 3200]\n",
            "loss: 0.967322  [ 1696/ 3200]\n",
            "loss: 0.910338  [ 1712/ 3200]\n",
            "loss: 0.939648  [ 1728/ 3200]\n",
            "loss: 0.795371  [ 1744/ 3200]\n",
            "loss: 0.909173  [ 1760/ 3200]\n",
            "loss: 1.246360  [ 1776/ 3200]\n",
            "loss: 0.897465  [ 1792/ 3200]\n",
            "loss: 0.825041  [ 1808/ 3200]\n",
            "loss: 0.897294  [ 1824/ 3200]\n",
            "loss: 1.010069  [ 1840/ 3200]\n",
            "loss: 0.981872  [ 1856/ 3200]\n",
            "loss: 0.825939  [ 1872/ 3200]\n",
            "loss: 0.808029  [ 1888/ 3200]\n",
            "loss: 1.271446  [ 1904/ 3200]\n",
            "loss: 0.880936  [ 1920/ 3200]\n",
            "loss: 0.925775  [ 1936/ 3200]\n",
            "loss: 1.285222  [ 1952/ 3200]\n",
            "loss: 1.022295  [ 1968/ 3200]\n",
            "loss: 0.846732  [ 1984/ 3200]\n",
            "loss: 0.824240  [ 2000/ 3200]\n",
            "loss: 1.188869  [ 2016/ 3200]\n",
            "loss: 0.830471  [ 2032/ 3200]\n",
            "loss: 0.679570  [ 2048/ 3200]\n",
            "loss: 1.486092  [ 2064/ 3200]\n",
            "loss: 0.905984  [ 2080/ 3200]\n",
            "loss: 0.954178  [ 2096/ 3200]\n",
            "loss: 0.865801  [ 2112/ 3200]\n",
            "loss: 1.064594  [ 2128/ 3200]\n",
            "loss: 1.612984  [ 2144/ 3200]\n",
            "loss: 1.070820  [ 2160/ 3200]\n",
            "loss: 0.608811  [ 2176/ 3200]\n",
            "loss: 0.987384  [ 2192/ 3200]\n",
            "loss: 1.036058  [ 2208/ 3200]\n",
            "loss: 1.259460  [ 2224/ 3200]\n",
            "loss: 1.227400  [ 2240/ 3200]\n",
            "loss: 0.848183  [ 2256/ 3200]\n",
            "loss: 0.693096  [ 2272/ 3200]\n",
            "loss: 0.826311  [ 2288/ 3200]\n",
            "loss: 1.062099  [ 2304/ 3200]\n",
            "loss: 0.875803  [ 2320/ 3200]\n",
            "loss: 0.714632  [ 2336/ 3200]\n",
            "loss: 0.655252  [ 2352/ 3200]\n",
            "loss: 0.875014  [ 2368/ 3200]\n",
            "loss: 0.920640  [ 2384/ 3200]\n",
            "loss: 0.957962  [ 2400/ 3200]\n",
            "loss: 1.075091  [ 2416/ 3200]\n",
            "loss: 1.171165  [ 2432/ 3200]\n",
            "loss: 1.026991  [ 2448/ 3200]\n",
            "loss: 1.018122  [ 2464/ 3200]\n",
            "loss: 0.896994  [ 2480/ 3200]\n",
            "loss: 0.708412  [ 2496/ 3200]\n",
            "loss: 0.840133  [ 2512/ 3200]\n",
            "loss: 0.915969  [ 2528/ 3200]\n",
            "loss: 0.882201  [ 2544/ 3200]\n",
            "loss: 0.844099  [ 2560/ 3200]\n",
            "loss: 0.804734  [ 2576/ 3200]\n",
            "loss: 0.912673  [ 2592/ 3200]\n",
            "loss: 1.103362  [ 2608/ 3200]\n",
            "loss: 1.006245  [ 2624/ 3200]\n",
            "loss: 1.320679  [ 2640/ 3200]\n",
            "loss: 1.181784  [ 2656/ 3200]\n",
            "loss: 1.285733  [ 2672/ 3200]\n",
            "loss: 1.089642  [ 2688/ 3200]\n",
            "loss: 0.760390  [ 2704/ 3200]\n",
            "loss: 0.918968  [ 2720/ 3200]\n",
            "loss: 0.806962  [ 2736/ 3200]\n",
            "loss: 1.112542  [ 2752/ 3200]\n",
            "loss: 0.741370  [ 2768/ 3200]\n",
            "loss: 1.044998  [ 2784/ 3200]\n",
            "loss: 1.108502  [ 2800/ 3200]\n",
            "loss: 0.813217  [ 2816/ 3200]\n",
            "loss: 0.582493  [ 2832/ 3200]\n",
            "loss: 1.090992  [ 2848/ 3200]\n",
            "loss: 1.004012  [ 2864/ 3200]\n",
            "loss: 0.825755  [ 2880/ 3200]\n",
            "loss: 0.934551  [ 2896/ 3200]\n",
            "loss: 0.922463  [ 2912/ 3200]\n",
            "loss: 0.910209  [ 2928/ 3200]\n",
            "loss: 1.057452  [ 2944/ 3200]\n",
            "loss: 1.014647  [ 2960/ 3200]\n",
            "loss: 1.097819  [ 2976/ 3200]\n",
            "loss: 1.121180  [ 2992/ 3200]\n",
            "loss: 0.989717  [ 3008/ 3200]\n",
            "loss: 1.099473  [ 3024/ 3200]\n",
            "loss: 0.927544  [ 3040/ 3200]\n",
            "loss: 1.038127  [ 3056/ 3200]\n",
            "loss: 0.939615  [ 3072/ 3200]\n",
            "loss: 1.109863  [ 3088/ 3200]\n",
            "loss: 1.063603  [ 3104/ 3200]\n",
            "loss: 0.748496  [ 3120/ 3200]\n",
            "loss: 1.031704  [ 3136/ 3200]\n",
            "loss: 1.070775  [ 3152/ 3200]\n",
            "loss: 0.734924  [ 3168/ 3200]\n",
            "loss: 0.797747  [ 3184/ 3200]\n",
            "current epoch: 24\n",
            "\n",
            "loss: 0.745011  [    0/ 3200]\n",
            "loss: 1.216583  [   16/ 3200]\n",
            "loss: 0.894948  [   32/ 3200]\n",
            "loss: 0.713569  [   48/ 3200]\n",
            "loss: 0.848797  [   64/ 3200]\n",
            "loss: 0.721291  [   80/ 3200]\n",
            "loss: 1.259526  [   96/ 3200]\n",
            "loss: 0.751033  [  112/ 3200]\n",
            "loss: 1.177124  [  128/ 3200]\n",
            "loss: 1.129251  [  144/ 3200]\n",
            "loss: 1.143254  [  160/ 3200]\n",
            "loss: 1.037900  [  176/ 3200]\n",
            "loss: 1.191460  [  192/ 3200]\n",
            "loss: 1.018905  [  208/ 3200]\n",
            "loss: 0.969136  [  224/ 3200]\n",
            "loss: 0.937154  [  240/ 3200]\n",
            "loss: 0.641858  [  256/ 3200]\n",
            "loss: 1.067975  [  272/ 3200]\n",
            "loss: 0.578130  [  288/ 3200]\n",
            "loss: 0.731774  [  304/ 3200]\n",
            "loss: 1.199851  [  320/ 3200]\n",
            "loss: 1.005918  [  336/ 3200]\n",
            "loss: 1.104337  [  352/ 3200]\n",
            "loss: 1.249046  [  368/ 3200]\n",
            "loss: 0.952905  [  384/ 3200]\n",
            "loss: 0.866133  [  400/ 3200]\n",
            "loss: 1.050878  [  416/ 3200]\n",
            "loss: 1.079909  [  432/ 3200]\n",
            "loss: 0.935798  [  448/ 3200]\n",
            "loss: 0.996058  [  464/ 3200]\n",
            "loss: 0.754005  [  480/ 3200]\n",
            "loss: 0.899141  [  496/ 3200]\n",
            "loss: 1.102802  [  512/ 3200]\n",
            "loss: 1.024770  [  528/ 3200]\n",
            "loss: 0.831463  [  544/ 3200]\n",
            "loss: 0.963885  [  560/ 3200]\n",
            "loss: 0.743213  [  576/ 3200]\n",
            "loss: 1.071307  [  592/ 3200]\n",
            "loss: 1.494247  [  608/ 3200]\n",
            "loss: 0.921450  [  624/ 3200]\n",
            "loss: 0.889319  [  640/ 3200]\n",
            "loss: 0.956142  [  656/ 3200]\n",
            "loss: 0.717218  [  672/ 3200]\n",
            "loss: 0.630504  [  688/ 3200]\n",
            "loss: 0.734770  [  704/ 3200]\n",
            "loss: 0.974699  [  720/ 3200]\n",
            "loss: 0.866856  [  736/ 3200]\n",
            "loss: 1.455263  [  752/ 3200]\n",
            "loss: 0.854441  [  768/ 3200]\n",
            "loss: 1.177183  [  784/ 3200]\n",
            "loss: 0.900877  [  800/ 3200]\n",
            "loss: 0.803405  [  816/ 3200]\n",
            "loss: 1.193403  [  832/ 3200]\n",
            "loss: 1.002082  [  848/ 3200]\n",
            "loss: 1.080398  [  864/ 3200]\n",
            "loss: 1.119262  [  880/ 3200]\n",
            "loss: 0.850276  [  896/ 3200]\n",
            "loss: 1.101816  [  912/ 3200]\n",
            "loss: 0.825859  [  928/ 3200]\n",
            "loss: 0.813075  [  944/ 3200]\n",
            "loss: 0.837909  [  960/ 3200]\n",
            "loss: 0.996983  [  976/ 3200]\n",
            "loss: 1.026452  [  992/ 3200]\n",
            "loss: 0.929208  [ 1008/ 3200]\n",
            "loss: 0.813296  [ 1024/ 3200]\n",
            "loss: 0.986260  [ 1040/ 3200]\n",
            "loss: 0.868607  [ 1056/ 3200]\n",
            "loss: 1.061534  [ 1072/ 3200]\n",
            "loss: 0.837642  [ 1088/ 3200]\n",
            "loss: 0.948350  [ 1104/ 3200]\n",
            "loss: 1.065435  [ 1120/ 3200]\n",
            "loss: 1.039991  [ 1136/ 3200]\n",
            "loss: 0.943136  [ 1152/ 3200]\n",
            "loss: 1.132857  [ 1168/ 3200]\n",
            "loss: 0.949510  [ 1184/ 3200]\n",
            "loss: 0.762107  [ 1200/ 3200]\n",
            "loss: 1.102738  [ 1216/ 3200]\n",
            "loss: 0.630633  [ 1232/ 3200]\n",
            "loss: 0.782773  [ 1248/ 3200]\n",
            "loss: 1.067114  [ 1264/ 3200]\n",
            "loss: 0.712543  [ 1280/ 3200]\n",
            "loss: 1.004528  [ 1296/ 3200]\n",
            "loss: 1.046063  [ 1312/ 3200]\n",
            "loss: 1.009334  [ 1328/ 3200]\n",
            "loss: 1.194119  [ 1344/ 3200]\n",
            "loss: 0.996802  [ 1360/ 3200]\n",
            "loss: 0.818017  [ 1376/ 3200]\n",
            "loss: 1.395624  [ 1392/ 3200]\n",
            "loss: 1.095865  [ 1408/ 3200]\n",
            "loss: 0.645435  [ 1424/ 3200]\n",
            "loss: 1.023015  [ 1440/ 3200]\n",
            "loss: 0.812311  [ 1456/ 3200]\n",
            "loss: 1.151216  [ 1472/ 3200]\n",
            "loss: 1.112937  [ 1488/ 3200]\n",
            "loss: 0.968333  [ 1504/ 3200]\n",
            "loss: 1.147358  [ 1520/ 3200]\n",
            "loss: 0.957963  [ 1536/ 3200]\n",
            "loss: 1.022149  [ 1552/ 3200]\n",
            "loss: 1.037061  [ 1568/ 3200]\n",
            "loss: 0.726657  [ 1584/ 3200]\n",
            "loss: 1.041757  [ 1600/ 3200]\n",
            "loss: 1.130107  [ 1616/ 3200]\n",
            "loss: 0.920087  [ 1632/ 3200]\n",
            "loss: 0.783285  [ 1648/ 3200]\n",
            "loss: 0.929143  [ 1664/ 3200]\n",
            "loss: 1.064175  [ 1680/ 3200]\n",
            "loss: 0.694334  [ 1696/ 3200]\n",
            "loss: 1.020010  [ 1712/ 3200]\n",
            "loss: 0.906236  [ 1728/ 3200]\n",
            "loss: 1.008442  [ 1744/ 3200]\n",
            "loss: 1.048979  [ 1760/ 3200]\n",
            "loss: 0.938034  [ 1776/ 3200]\n",
            "loss: 0.894351  [ 1792/ 3200]\n",
            "loss: 1.018047  [ 1808/ 3200]\n",
            "loss: 0.753062  [ 1824/ 3200]\n",
            "loss: 0.962812  [ 1840/ 3200]\n",
            "loss: 1.128033  [ 1856/ 3200]\n",
            "loss: 0.798921  [ 1872/ 3200]\n",
            "loss: 0.823705  [ 1888/ 3200]\n",
            "loss: 1.001932  [ 1904/ 3200]\n",
            "loss: 1.171011  [ 1920/ 3200]\n",
            "loss: 1.059884  [ 1936/ 3200]\n",
            "loss: 1.069722  [ 1952/ 3200]\n",
            "loss: 1.396650  [ 1968/ 3200]\n",
            "loss: 0.999688  [ 1984/ 3200]\n",
            "loss: 0.838219  [ 2000/ 3200]\n",
            "loss: 0.903285  [ 2016/ 3200]\n",
            "loss: 0.902894  [ 2032/ 3200]\n",
            "loss: 1.192285  [ 2048/ 3200]\n",
            "loss: 0.958629  [ 2064/ 3200]\n",
            "loss: 1.032193  [ 2080/ 3200]\n",
            "loss: 1.133476  [ 2096/ 3200]\n",
            "loss: 0.988256  [ 2112/ 3200]\n",
            "loss: 0.824089  [ 2128/ 3200]\n",
            "loss: 0.824499  [ 2144/ 3200]\n",
            "loss: 0.867500  [ 2160/ 3200]\n",
            "loss: 1.004184  [ 2176/ 3200]\n",
            "loss: 0.720252  [ 2192/ 3200]\n",
            "loss: 0.879267  [ 2208/ 3200]\n",
            "loss: 1.000999  [ 2224/ 3200]\n",
            "loss: 0.747418  [ 2240/ 3200]\n",
            "loss: 0.846690  [ 2256/ 3200]\n",
            "loss: 0.876548  [ 2272/ 3200]\n",
            "loss: 0.776959  [ 2288/ 3200]\n",
            "loss: 0.776004  [ 2304/ 3200]\n",
            "loss: 0.771475  [ 2320/ 3200]\n",
            "loss: 1.049945  [ 2336/ 3200]\n",
            "loss: 0.653679  [ 2352/ 3200]\n",
            "loss: 1.102709  [ 2368/ 3200]\n",
            "loss: 1.200874  [ 2384/ 3200]\n",
            "loss: 0.783242  [ 2400/ 3200]\n",
            "loss: 1.038852  [ 2416/ 3200]\n",
            "loss: 0.785452  [ 2432/ 3200]\n",
            "loss: 1.088921  [ 2448/ 3200]\n",
            "loss: 1.192838  [ 2464/ 3200]\n",
            "loss: 0.898196  [ 2480/ 3200]\n",
            "loss: 1.113040  [ 2496/ 3200]\n",
            "loss: 0.533257  [ 2512/ 3200]\n",
            "loss: 0.945199  [ 2528/ 3200]\n",
            "loss: 1.135868  [ 2544/ 3200]\n",
            "loss: 0.896178  [ 2560/ 3200]\n",
            "loss: 1.055649  [ 2576/ 3200]\n",
            "loss: 0.850863  [ 2592/ 3200]\n",
            "loss: 1.181255  [ 2608/ 3200]\n",
            "loss: 1.300514  [ 2624/ 3200]\n",
            "loss: 1.136091  [ 2640/ 3200]\n",
            "loss: 0.813932  [ 2656/ 3200]\n",
            "loss: 0.797895  [ 2672/ 3200]\n",
            "loss: 0.977016  [ 2688/ 3200]\n",
            "loss: 0.694186  [ 2704/ 3200]\n",
            "loss: 0.991225  [ 2720/ 3200]\n",
            "loss: 1.050766  [ 2736/ 3200]\n",
            "loss: 1.003971  [ 2752/ 3200]\n",
            "loss: 0.989023  [ 2768/ 3200]\n",
            "loss: 0.958584  [ 2784/ 3200]\n",
            "loss: 0.917596  [ 2800/ 3200]\n",
            "loss: 0.988871  [ 2816/ 3200]\n",
            "loss: 1.246140  [ 2832/ 3200]\n",
            "loss: 0.894139  [ 2848/ 3200]\n",
            "loss: 0.964101  [ 2864/ 3200]\n",
            "loss: 1.035021  [ 2880/ 3200]\n",
            "loss: 1.211445  [ 2896/ 3200]\n",
            "loss: 0.823751  [ 2912/ 3200]\n",
            "loss: 0.943485  [ 2928/ 3200]\n",
            "loss: 0.784811  [ 2944/ 3200]\n",
            "loss: 0.891794  [ 2960/ 3200]\n",
            "loss: 1.132045  [ 2976/ 3200]\n",
            "loss: 1.031283  [ 2992/ 3200]\n",
            "loss: 0.764304  [ 3008/ 3200]\n",
            "loss: 0.850695  [ 3024/ 3200]\n",
            "loss: 0.962497  [ 3040/ 3200]\n",
            "loss: 0.670266  [ 3056/ 3200]\n",
            "loss: 0.919906  [ 3072/ 3200]\n",
            "loss: 1.162560  [ 3088/ 3200]\n",
            "loss: 1.022058  [ 3104/ 3200]\n",
            "loss: 0.862349  [ 3120/ 3200]\n",
            "loss: 1.283628  [ 3136/ 3200]\n",
            "loss: 0.814659  [ 3152/ 3200]\n",
            "loss: 0.906577  [ 3168/ 3200]\n",
            "loss: 0.746156  [ 3184/ 3200]\n",
            "current epoch: 25\n",
            "\n",
            "loss: 0.894721  [    0/ 3200]\n",
            "loss: 0.935384  [   16/ 3200]\n",
            "loss: 0.775616  [   32/ 3200]\n",
            "loss: 0.915658  [   48/ 3200]\n",
            "loss: 1.158365  [   64/ 3200]\n",
            "loss: 0.917862  [   80/ 3200]\n",
            "loss: 1.004686  [   96/ 3200]\n",
            "loss: 0.857506  [  112/ 3200]\n",
            "loss: 0.637385  [  128/ 3200]\n",
            "loss: 0.729134  [  144/ 3200]\n",
            "loss: 0.947044  [  160/ 3200]\n",
            "loss: 0.924524  [  176/ 3200]\n",
            "loss: 1.208303  [  192/ 3200]\n",
            "loss: 1.029301  [  208/ 3200]\n",
            "loss: 0.947684  [  224/ 3200]\n",
            "loss: 0.993992  [  240/ 3200]\n",
            "loss: 0.978024  [  256/ 3200]\n",
            "loss: 0.670837  [  272/ 3200]\n",
            "loss: 0.696241  [  288/ 3200]\n",
            "loss: 0.955674  [  304/ 3200]\n",
            "loss: 1.146194  [  320/ 3200]\n",
            "loss: 0.897293  [  336/ 3200]\n",
            "loss: 0.826517  [  352/ 3200]\n",
            "loss: 0.709180  [  368/ 3200]\n",
            "loss: 1.212177  [  384/ 3200]\n",
            "loss: 0.755271  [  400/ 3200]\n",
            "loss: 0.875769  [  416/ 3200]\n",
            "loss: 1.216574  [  432/ 3200]\n",
            "loss: 0.755067  [  448/ 3200]\n",
            "loss: 0.798440  [  464/ 3200]\n",
            "loss: 0.937738  [  480/ 3200]\n",
            "loss: 1.057259  [  496/ 3200]\n",
            "loss: 1.403945  [  512/ 3200]\n",
            "loss: 0.998745  [  528/ 3200]\n",
            "loss: 0.988003  [  544/ 3200]\n",
            "loss: 1.177808  [  560/ 3200]\n",
            "loss: 1.100855  [  576/ 3200]\n",
            "loss: 0.811886  [  592/ 3200]\n",
            "loss: 0.973794  [  608/ 3200]\n",
            "loss: 1.091514  [  624/ 3200]\n",
            "loss: 1.182647  [  640/ 3200]\n",
            "loss: 0.754287  [  656/ 3200]\n",
            "loss: 1.165280  [  672/ 3200]\n",
            "loss: 0.871174  [  688/ 3200]\n",
            "loss: 0.834105  [  704/ 3200]\n",
            "loss: 1.109859  [  720/ 3200]\n",
            "loss: 1.027603  [  736/ 3200]\n",
            "loss: 1.001345  [  752/ 3200]\n",
            "loss: 0.674313  [  768/ 3200]\n",
            "loss: 0.995713  [  784/ 3200]\n",
            "loss: 0.953213  [  800/ 3200]\n",
            "loss: 0.831426  [  816/ 3200]\n",
            "loss: 0.849923  [  832/ 3200]\n",
            "loss: 1.106306  [  848/ 3200]\n",
            "loss: 1.039692  [  864/ 3200]\n",
            "loss: 0.849175  [  880/ 3200]\n",
            "loss: 0.818316  [  896/ 3200]\n",
            "loss: 0.782328  [  912/ 3200]\n",
            "loss: 1.103907  [  928/ 3200]\n",
            "loss: 0.810915  [  944/ 3200]\n",
            "loss: 1.217687  [  960/ 3200]\n",
            "loss: 1.112885  [  976/ 3200]\n",
            "loss: 0.752344  [  992/ 3200]\n",
            "loss: 0.782906  [ 1008/ 3200]\n",
            "loss: 0.615348  [ 1024/ 3200]\n",
            "loss: 0.948766  [ 1040/ 3200]\n",
            "loss: 0.948074  [ 1056/ 3200]\n",
            "loss: 1.057498  [ 1072/ 3200]\n",
            "loss: 0.645360  [ 1088/ 3200]\n",
            "loss: 1.094002  [ 1104/ 3200]\n",
            "loss: 1.173123  [ 1120/ 3200]\n",
            "loss: 0.895705  [ 1136/ 3200]\n",
            "loss: 1.221874  [ 1152/ 3200]\n",
            "loss: 1.085623  [ 1168/ 3200]\n",
            "loss: 1.207427  [ 1184/ 3200]\n",
            "loss: 0.741808  [ 1200/ 3200]\n",
            "loss: 0.556083  [ 1216/ 3200]\n",
            "loss: 1.178860  [ 1232/ 3200]\n",
            "loss: 0.783700  [ 1248/ 3200]\n",
            "loss: 0.840900  [ 1264/ 3200]\n",
            "loss: 0.849116  [ 1280/ 3200]\n",
            "loss: 1.013028  [ 1296/ 3200]\n",
            "loss: 1.273339  [ 1312/ 3200]\n",
            "loss: 0.936296  [ 1328/ 3200]\n",
            "loss: 0.778128  [ 1344/ 3200]\n",
            "loss: 1.227567  [ 1360/ 3200]\n",
            "loss: 0.919037  [ 1376/ 3200]\n",
            "loss: 0.858179  [ 1392/ 3200]\n",
            "loss: 0.796033  [ 1408/ 3200]\n",
            "loss: 1.004097  [ 1424/ 3200]\n",
            "loss: 1.201955  [ 1440/ 3200]\n",
            "loss: 1.081101  [ 1456/ 3200]\n",
            "loss: 1.268957  [ 1472/ 3200]\n",
            "loss: 0.805829  [ 1488/ 3200]\n",
            "loss: 0.973203  [ 1504/ 3200]\n",
            "loss: 0.858926  [ 1520/ 3200]\n",
            "loss: 0.804851  [ 1536/ 3200]\n",
            "loss: 0.930274  [ 1552/ 3200]\n",
            "loss: 0.713697  [ 1568/ 3200]\n",
            "loss: 1.006080  [ 1584/ 3200]\n",
            "loss: 0.801656  [ 1600/ 3200]\n",
            "loss: 0.920855  [ 1616/ 3200]\n",
            "loss: 1.150234  [ 1632/ 3200]\n",
            "loss: 0.904491  [ 1648/ 3200]\n",
            "loss: 1.026690  [ 1664/ 3200]\n",
            "loss: 0.824410  [ 1680/ 3200]\n",
            "loss: 0.808485  [ 1696/ 3200]\n",
            "loss: 1.009545  [ 1712/ 3200]\n",
            "loss: 1.039311  [ 1728/ 3200]\n",
            "loss: 0.935701  [ 1744/ 3200]\n",
            "loss: 0.767057  [ 1760/ 3200]\n",
            "loss: 0.982790  [ 1776/ 3200]\n",
            "loss: 0.889628  [ 1792/ 3200]\n",
            "loss: 1.046246  [ 1808/ 3200]\n",
            "loss: 0.694870  [ 1824/ 3200]\n",
            "loss: 0.939601  [ 1840/ 3200]\n",
            "loss: 1.186823  [ 1856/ 3200]\n",
            "loss: 0.837590  [ 1872/ 3200]\n",
            "loss: 1.273317  [ 1888/ 3200]\n",
            "loss: 0.994263  [ 1904/ 3200]\n",
            "loss: 1.010456  [ 1920/ 3200]\n",
            "loss: 0.838476  [ 1936/ 3200]\n",
            "loss: 1.090869  [ 1952/ 3200]\n",
            "loss: 1.028393  [ 1968/ 3200]\n",
            "loss: 0.831562  [ 1984/ 3200]\n",
            "loss: 0.879258  [ 2000/ 3200]\n",
            "loss: 0.611678  [ 2016/ 3200]\n",
            "loss: 0.792675  [ 2032/ 3200]\n",
            "loss: 1.004915  [ 2048/ 3200]\n",
            "loss: 0.764143  [ 2064/ 3200]\n",
            "loss: 0.831394  [ 2080/ 3200]\n",
            "loss: 0.853140  [ 2096/ 3200]\n",
            "loss: 0.936661  [ 2112/ 3200]\n",
            "loss: 0.874878  [ 2128/ 3200]\n",
            "loss: 0.907155  [ 2144/ 3200]\n",
            "loss: 0.581846  [ 2160/ 3200]\n",
            "loss: 1.038360  [ 2176/ 3200]\n",
            "loss: 0.970597  [ 2192/ 3200]\n",
            "loss: 1.308899  [ 2208/ 3200]\n",
            "loss: 1.300671  [ 2224/ 3200]\n",
            "loss: 0.889133  [ 2240/ 3200]\n",
            "loss: 1.225492  [ 2256/ 3200]\n",
            "loss: 0.885005  [ 2272/ 3200]\n",
            "loss: 0.959068  [ 2288/ 3200]\n",
            "loss: 0.987574  [ 2304/ 3200]\n",
            "loss: 1.175701  [ 2320/ 3200]\n",
            "loss: 0.891460  [ 2336/ 3200]\n",
            "loss: 0.714951  [ 2352/ 3200]\n",
            "loss: 0.890364  [ 2368/ 3200]\n",
            "loss: 0.894328  [ 2384/ 3200]\n",
            "loss: 0.731940  [ 2400/ 3200]\n",
            "loss: 0.627590  [ 2416/ 3200]\n",
            "loss: 0.963505  [ 2432/ 3200]\n",
            "loss: 0.788925  [ 2448/ 3200]\n",
            "loss: 1.050860  [ 2464/ 3200]\n",
            "loss: 1.030492  [ 2480/ 3200]\n",
            "loss: 0.896256  [ 2496/ 3200]\n",
            "loss: 1.455901  [ 2512/ 3200]\n",
            "loss: 1.364875  [ 2528/ 3200]\n",
            "loss: 0.814837  [ 2544/ 3200]\n",
            "loss: 1.881729  [ 2560/ 3200]\n",
            "loss: 1.258506  [ 2576/ 3200]\n",
            "loss: 0.880318  [ 2592/ 3200]\n",
            "loss: 0.656846  [ 2608/ 3200]\n",
            "loss: 1.108749  [ 2624/ 3200]\n",
            "loss: 1.154572  [ 2640/ 3200]\n",
            "loss: 0.935302  [ 2656/ 3200]\n",
            "loss: 0.909378  [ 2672/ 3200]\n",
            "loss: 1.140894  [ 2688/ 3200]\n",
            "loss: 0.954211  [ 2704/ 3200]\n",
            "loss: 0.744122  [ 2720/ 3200]\n",
            "loss: 0.855172  [ 2736/ 3200]\n",
            "loss: 0.727980  [ 2752/ 3200]\n",
            "loss: 0.989127  [ 2768/ 3200]\n",
            "loss: 1.024809  [ 2784/ 3200]\n",
            "loss: 0.930355  [ 2800/ 3200]\n",
            "loss: 1.049342  [ 2816/ 3200]\n",
            "loss: 0.604079  [ 2832/ 3200]\n",
            "loss: 0.800512  [ 2848/ 3200]\n",
            "loss: 1.127198  [ 2864/ 3200]\n",
            "loss: 0.770283  [ 2880/ 3200]\n",
            "loss: 1.097927  [ 2896/ 3200]\n",
            "loss: 1.145563  [ 2912/ 3200]\n",
            "loss: 1.282084  [ 2928/ 3200]\n",
            "loss: 0.846714  [ 2944/ 3200]\n",
            "loss: 0.772720  [ 2960/ 3200]\n",
            "loss: 1.128830  [ 2976/ 3200]\n",
            "loss: 0.656694  [ 2992/ 3200]\n",
            "loss: 0.894550  [ 3008/ 3200]\n",
            "loss: 0.689031  [ 3024/ 3200]\n",
            "loss: 1.345532  [ 3040/ 3200]\n",
            "loss: 0.707418  [ 3056/ 3200]\n",
            "loss: 1.377178  [ 3072/ 3200]\n",
            "loss: 1.104943  [ 3088/ 3200]\n",
            "loss: 1.088712  [ 3104/ 3200]\n",
            "loss: 0.800657  [ 3120/ 3200]\n",
            "loss: 1.055444  [ 3136/ 3200]\n",
            "loss: 0.914109  [ 3152/ 3200]\n",
            "loss: 0.825922  [ 3168/ 3200]\n",
            "loss: 0.620060  [ 3184/ 3200]\n",
            "current epoch: 26\n",
            "\n",
            "loss: 1.025662  [    0/ 3200]\n",
            "loss: 0.932930  [   16/ 3200]\n",
            "loss: 0.986460  [   32/ 3200]\n",
            "loss: 1.375056  [   48/ 3200]\n",
            "loss: 0.708427  [   64/ 3200]\n",
            "loss: 1.044111  [   80/ 3200]\n",
            "loss: 1.153585  [   96/ 3200]\n",
            "loss: 0.851475  [  112/ 3200]\n",
            "loss: 1.108204  [  128/ 3200]\n",
            "loss: 0.996516  [  144/ 3200]\n",
            "loss: 0.927183  [  160/ 3200]\n",
            "loss: 1.241779  [  176/ 3200]\n",
            "loss: 0.783630  [  192/ 3200]\n",
            "loss: 1.194794  [  208/ 3200]\n",
            "loss: 1.231204  [  224/ 3200]\n",
            "loss: 0.821973  [  240/ 3200]\n",
            "loss: 0.804857  [  256/ 3200]\n",
            "loss: 1.145365  [  272/ 3200]\n",
            "loss: 0.993951  [  288/ 3200]\n",
            "loss: 1.004856  [  304/ 3200]\n",
            "loss: 0.830049  [  320/ 3200]\n",
            "loss: 0.802447  [  336/ 3200]\n",
            "loss: 0.753070  [  352/ 3200]\n",
            "loss: 0.704976  [  368/ 3200]\n",
            "loss: 0.940814  [  384/ 3200]\n",
            "loss: 0.854648  [  400/ 3200]\n",
            "loss: 1.094427  [  416/ 3200]\n",
            "loss: 1.232013  [  432/ 3200]\n",
            "loss: 0.660163  [  448/ 3200]\n",
            "loss: 0.942528  [  464/ 3200]\n",
            "loss: 0.836294  [  480/ 3200]\n",
            "loss: 1.022668  [  496/ 3200]\n",
            "loss: 1.050140  [  512/ 3200]\n",
            "loss: 0.969929  [  528/ 3200]\n",
            "loss: 0.978158  [  544/ 3200]\n",
            "loss: 1.170135  [  560/ 3200]\n",
            "loss: 0.794602  [  576/ 3200]\n",
            "loss: 0.802279  [  592/ 3200]\n",
            "loss: 0.922233  [  608/ 3200]\n",
            "loss: 0.937426  [  624/ 3200]\n",
            "loss: 1.004352  [  640/ 3200]\n",
            "loss: 0.856267  [  656/ 3200]\n",
            "loss: 0.973961  [  672/ 3200]\n",
            "loss: 0.833738  [  688/ 3200]\n",
            "loss: 0.980269  [  704/ 3200]\n",
            "loss: 1.044287  [  720/ 3200]\n",
            "loss: 0.949978  [  736/ 3200]\n",
            "loss: 0.984607  [  752/ 3200]\n",
            "loss: 0.805596  [  768/ 3200]\n",
            "loss: 0.942874  [  784/ 3200]\n",
            "loss: 1.230159  [  800/ 3200]\n",
            "loss: 1.000217  [  816/ 3200]\n",
            "loss: 0.703607  [  832/ 3200]\n",
            "loss: 0.583180  [  848/ 3200]\n",
            "loss: 1.385023  [  864/ 3200]\n",
            "loss: 1.249325  [  880/ 3200]\n",
            "loss: 1.015991  [  896/ 3200]\n",
            "loss: 0.680508  [  912/ 3200]\n",
            "loss: 0.747247  [  928/ 3200]\n",
            "loss: 0.856614  [  944/ 3200]\n",
            "loss: 0.806391  [  960/ 3200]\n",
            "loss: 0.972272  [  976/ 3200]\n",
            "loss: 0.652576  [  992/ 3200]\n",
            "loss: 0.831342  [ 1008/ 3200]\n",
            "loss: 0.803031  [ 1024/ 3200]\n",
            "loss: 1.281085  [ 1040/ 3200]\n",
            "loss: 1.035682  [ 1056/ 3200]\n",
            "loss: 0.718941  [ 1072/ 3200]\n",
            "loss: 0.916673  [ 1088/ 3200]\n",
            "loss: 1.206066  [ 1104/ 3200]\n",
            "loss: 0.806048  [ 1120/ 3200]\n",
            "loss: 0.872658  [ 1136/ 3200]\n",
            "loss: 0.904578  [ 1152/ 3200]\n",
            "loss: 0.851555  [ 1168/ 3200]\n",
            "loss: 0.692748  [ 1184/ 3200]\n",
            "loss: 1.237726  [ 1200/ 3200]\n",
            "loss: 0.801013  [ 1216/ 3200]\n",
            "loss: 1.095131  [ 1232/ 3200]\n",
            "loss: 0.785647  [ 1248/ 3200]\n",
            "loss: 0.931336  [ 1264/ 3200]\n",
            "loss: 1.508419  [ 1280/ 3200]\n",
            "loss: 1.180869  [ 1296/ 3200]\n",
            "loss: 1.003616  [ 1312/ 3200]\n",
            "loss: 0.834433  [ 1328/ 3200]\n",
            "loss: 0.618092  [ 1344/ 3200]\n",
            "loss: 0.622583  [ 1360/ 3200]\n",
            "loss: 0.686650  [ 1376/ 3200]\n",
            "loss: 0.991186  [ 1392/ 3200]\n",
            "loss: 0.680098  [ 1408/ 3200]\n",
            "loss: 0.875258  [ 1424/ 3200]\n",
            "loss: 0.825767  [ 1440/ 3200]\n",
            "loss: 1.063158  [ 1456/ 3200]\n",
            "loss: 0.827918  [ 1472/ 3200]\n",
            "loss: 0.803055  [ 1488/ 3200]\n",
            "loss: 1.259983  [ 1504/ 3200]\n",
            "loss: 0.719401  [ 1520/ 3200]\n",
            "loss: 1.011120  [ 1536/ 3200]\n",
            "loss: 0.839807  [ 1552/ 3200]\n",
            "loss: 0.975342  [ 1568/ 3200]\n",
            "loss: 1.008812  [ 1584/ 3200]\n",
            "loss: 0.899885  [ 1600/ 3200]\n",
            "loss: 1.024759  [ 1616/ 3200]\n",
            "loss: 0.840175  [ 1632/ 3200]\n",
            "loss: 0.965695  [ 1648/ 3200]\n",
            "loss: 0.887280  [ 1664/ 3200]\n",
            "loss: 0.849327  [ 1680/ 3200]\n",
            "loss: 0.586928  [ 1696/ 3200]\n",
            "loss: 0.923587  [ 1712/ 3200]\n",
            "loss: 0.814473  [ 1728/ 3200]\n",
            "loss: 0.890090  [ 1744/ 3200]\n",
            "loss: 1.251156  [ 1760/ 3200]\n",
            "loss: 1.200560  [ 1776/ 3200]\n",
            "loss: 1.373642  [ 1792/ 3200]\n",
            "loss: 1.031507  [ 1808/ 3200]\n",
            "loss: 0.843864  [ 1824/ 3200]\n",
            "loss: 1.143257  [ 1840/ 3200]\n",
            "loss: 0.884964  [ 1856/ 3200]\n",
            "loss: 0.862297  [ 1872/ 3200]\n",
            "loss: 1.122862  [ 1888/ 3200]\n",
            "loss: 0.887523  [ 1904/ 3200]\n",
            "loss: 0.770860  [ 1920/ 3200]\n",
            "loss: 1.024011  [ 1936/ 3200]\n",
            "loss: 1.139857  [ 1952/ 3200]\n",
            "loss: 1.214597  [ 1968/ 3200]\n",
            "loss: 1.282582  [ 1984/ 3200]\n",
            "loss: 0.906216  [ 2000/ 3200]\n",
            "loss: 0.831613  [ 2016/ 3200]\n",
            "loss: 0.854706  [ 2032/ 3200]\n",
            "loss: 0.962049  [ 2048/ 3200]\n",
            "loss: 0.819069  [ 2064/ 3200]\n",
            "loss: 1.116215  [ 2080/ 3200]\n",
            "loss: 0.776013  [ 2096/ 3200]\n",
            "loss: 0.941920  [ 2112/ 3200]\n",
            "loss: 0.803083  [ 2128/ 3200]\n",
            "loss: 0.958556  [ 2144/ 3200]\n",
            "loss: 0.890549  [ 2160/ 3200]\n",
            "loss: 0.987695  [ 2176/ 3200]\n",
            "loss: 1.127750  [ 2192/ 3200]\n",
            "loss: 0.987623  [ 2208/ 3200]\n",
            "loss: 0.877037  [ 2224/ 3200]\n",
            "loss: 1.240049  [ 2240/ 3200]\n",
            "loss: 0.836912  [ 2256/ 3200]\n",
            "loss: 0.916897  [ 2272/ 3200]\n",
            "loss: 1.010137  [ 2288/ 3200]\n",
            "loss: 1.076142  [ 2304/ 3200]\n",
            "loss: 0.750449  [ 2320/ 3200]\n",
            "loss: 1.000667  [ 2336/ 3200]\n",
            "loss: 1.137027  [ 2352/ 3200]\n",
            "loss: 1.210473  [ 2368/ 3200]\n",
            "loss: 1.012567  [ 2384/ 3200]\n",
            "loss: 1.086104  [ 2400/ 3200]\n",
            "loss: 0.934919  [ 2416/ 3200]\n",
            "loss: 0.947763  [ 2432/ 3200]\n",
            "loss: 0.813367  [ 2448/ 3200]\n",
            "loss: 0.637777  [ 2464/ 3200]\n",
            "loss: 0.552502  [ 2480/ 3200]\n",
            "loss: 1.194229  [ 2496/ 3200]\n",
            "loss: 1.067425  [ 2512/ 3200]\n",
            "loss: 0.798052  [ 2528/ 3200]\n",
            "loss: 0.951548  [ 2544/ 3200]\n",
            "loss: 0.653823  [ 2560/ 3200]\n",
            "loss: 1.097677  [ 2576/ 3200]\n",
            "loss: 0.856260  [ 2592/ 3200]\n",
            "loss: 1.028419  [ 2608/ 3200]\n",
            "loss: 0.648251  [ 2624/ 3200]\n",
            "loss: 0.953194  [ 2640/ 3200]\n",
            "loss: 0.830636  [ 2656/ 3200]\n",
            "loss: 1.130167  [ 2672/ 3200]\n",
            "loss: 1.032876  [ 2688/ 3200]\n",
            "loss: 0.567254  [ 2704/ 3200]\n",
            "loss: 1.021509  [ 2720/ 3200]\n",
            "loss: 0.881310  [ 2736/ 3200]\n",
            "loss: 1.039036  [ 2752/ 3200]\n",
            "loss: 0.967952  [ 2768/ 3200]\n",
            "loss: 0.977382  [ 2784/ 3200]\n",
            "loss: 0.689136  [ 2800/ 3200]\n",
            "loss: 0.743415  [ 2816/ 3200]\n",
            "loss: 1.070113  [ 2832/ 3200]\n",
            "loss: 1.201062  [ 2848/ 3200]\n",
            "loss: 1.095286  [ 2864/ 3200]\n",
            "loss: 1.063270  [ 2880/ 3200]\n",
            "loss: 0.949592  [ 2896/ 3200]\n",
            "loss: 1.008315  [ 2912/ 3200]\n",
            "loss: 0.934072  [ 2928/ 3200]\n",
            "loss: 0.808381  [ 2944/ 3200]\n",
            "loss: 1.153242  [ 2960/ 3200]\n",
            "loss: 0.928739  [ 2976/ 3200]\n",
            "loss: 0.767503  [ 2992/ 3200]\n",
            "loss: 1.005864  [ 3008/ 3200]\n",
            "loss: 0.942533  [ 3024/ 3200]\n",
            "loss: 0.862052  [ 3040/ 3200]\n",
            "loss: 0.910189  [ 3056/ 3200]\n",
            "loss: 1.251372  [ 3072/ 3200]\n",
            "loss: 1.240322  [ 3088/ 3200]\n",
            "loss: 1.127475  [ 3104/ 3200]\n",
            "loss: 0.822863  [ 3120/ 3200]\n",
            "loss: 0.848357  [ 3136/ 3200]\n",
            "loss: 1.197307  [ 3152/ 3200]\n",
            "loss: 0.985623  [ 3168/ 3200]\n",
            "loss: 1.159978  [ 3184/ 3200]\n",
            "current epoch: 27\n",
            "\n",
            "loss: 0.781029  [    0/ 3200]\n",
            "loss: 0.982763  [   16/ 3200]\n",
            "loss: 0.727978  [   32/ 3200]\n",
            "loss: 0.880046  [   48/ 3200]\n",
            "loss: 0.607300  [   64/ 3200]\n",
            "loss: 0.935175  [   80/ 3200]\n",
            "loss: 1.188596  [   96/ 3200]\n",
            "loss: 0.846157  [  112/ 3200]\n",
            "loss: 0.847397  [  128/ 3200]\n",
            "loss: 0.920672  [  144/ 3200]\n",
            "loss: 1.047362  [  160/ 3200]\n",
            "loss: 0.949458  [  176/ 3200]\n",
            "loss: 0.997434  [  192/ 3200]\n",
            "loss: 0.723621  [  208/ 3200]\n",
            "loss: 0.974084  [  224/ 3200]\n",
            "loss: 0.757186  [  240/ 3200]\n",
            "loss: 1.280826  [  256/ 3200]\n",
            "loss: 0.887301  [  272/ 3200]\n",
            "loss: 0.684752  [  288/ 3200]\n",
            "loss: 0.933131  [  304/ 3200]\n",
            "loss: 0.912229  [  320/ 3200]\n",
            "loss: 0.784650  [  336/ 3200]\n",
            "loss: 0.969788  [  352/ 3200]\n",
            "loss: 0.840500  [  368/ 3200]\n",
            "loss: 0.992068  [  384/ 3200]\n",
            "loss: 0.649959  [  400/ 3200]\n",
            "loss: 0.714159  [  416/ 3200]\n",
            "loss: 1.076041  [  432/ 3200]\n",
            "loss: 0.779899  [  448/ 3200]\n",
            "loss: 0.809502  [  464/ 3200]\n",
            "loss: 0.963935  [  480/ 3200]\n",
            "loss: 0.900148  [  496/ 3200]\n",
            "loss: 0.698475  [  512/ 3200]\n",
            "loss: 0.822808  [  528/ 3200]\n",
            "loss: 0.895478  [  544/ 3200]\n",
            "loss: 0.900107  [  560/ 3200]\n",
            "loss: 1.148732  [  576/ 3200]\n",
            "loss: 0.896283  [  592/ 3200]\n",
            "loss: 1.173755  [  608/ 3200]\n",
            "loss: 0.939061  [  624/ 3200]\n",
            "loss: 0.879607  [  640/ 3200]\n",
            "loss: 1.109526  [  656/ 3200]\n",
            "loss: 0.953869  [  672/ 3200]\n",
            "loss: 0.871589  [  688/ 3200]\n",
            "loss: 0.933304  [  704/ 3200]\n",
            "loss: 0.739195  [  720/ 3200]\n",
            "loss: 0.989158  [  736/ 3200]\n",
            "loss: 1.009040  [  752/ 3200]\n",
            "loss: 1.117443  [  768/ 3200]\n",
            "loss: 0.849386  [  784/ 3200]\n",
            "loss: 0.995843  [  800/ 3200]\n",
            "loss: 0.959499  [  816/ 3200]\n",
            "loss: 1.156195  [  832/ 3200]\n",
            "loss: 0.934271  [  848/ 3200]\n",
            "loss: 0.857343  [  864/ 3200]\n",
            "loss: 0.749912  [  880/ 3200]\n",
            "loss: 1.062135  [  896/ 3200]\n",
            "loss: 0.992334  [  912/ 3200]\n",
            "loss: 0.821763  [  928/ 3200]\n",
            "loss: 0.647257  [  944/ 3200]\n",
            "loss: 0.971200  [  960/ 3200]\n",
            "loss: 1.279136  [  976/ 3200]\n",
            "loss: 1.236392  [  992/ 3200]\n",
            "loss: 0.947474  [ 1008/ 3200]\n",
            "loss: 0.836301  [ 1024/ 3200]\n",
            "loss: 1.110825  [ 1040/ 3200]\n",
            "loss: 0.758099  [ 1056/ 3200]\n",
            "loss: 0.852080  [ 1072/ 3200]\n",
            "loss: 0.922267  [ 1088/ 3200]\n",
            "loss: 1.030255  [ 1104/ 3200]\n",
            "loss: 0.968759  [ 1120/ 3200]\n",
            "loss: 1.198561  [ 1136/ 3200]\n",
            "loss: 1.165803  [ 1152/ 3200]\n",
            "loss: 1.213370  [ 1168/ 3200]\n",
            "loss: 1.085413  [ 1184/ 3200]\n",
            "loss: 1.435956  [ 1200/ 3200]\n",
            "loss: 1.169489  [ 1216/ 3200]\n",
            "loss: 0.782374  [ 1232/ 3200]\n",
            "loss: 1.011952  [ 1248/ 3200]\n",
            "loss: 1.067646  [ 1264/ 3200]\n",
            "loss: 1.126640  [ 1280/ 3200]\n",
            "loss: 0.656854  [ 1296/ 3200]\n",
            "loss: 1.033103  [ 1312/ 3200]\n",
            "loss: 0.936250  [ 1328/ 3200]\n",
            "loss: 0.831449  [ 1344/ 3200]\n",
            "loss: 0.750392  [ 1360/ 3200]\n",
            "loss: 1.106501  [ 1376/ 3200]\n",
            "loss: 1.067723  [ 1392/ 3200]\n",
            "loss: 0.667948  [ 1408/ 3200]\n",
            "loss: 0.790275  [ 1424/ 3200]\n",
            "loss: 0.921216  [ 1440/ 3200]\n",
            "loss: 0.682891  [ 1456/ 3200]\n",
            "loss: 0.822488  [ 1472/ 3200]\n",
            "loss: 0.911801  [ 1488/ 3200]\n",
            "loss: 1.046894  [ 1504/ 3200]\n",
            "loss: 0.981766  [ 1520/ 3200]\n",
            "loss: 1.016955  [ 1536/ 3200]\n",
            "loss: 0.597735  [ 1552/ 3200]\n",
            "loss: 1.245114  [ 1568/ 3200]\n",
            "loss: 0.766373  [ 1584/ 3200]\n",
            "loss: 1.138851  [ 1600/ 3200]\n",
            "loss: 0.963351  [ 1616/ 3200]\n",
            "loss: 1.404856  [ 1632/ 3200]\n",
            "loss: 1.115190  [ 1648/ 3200]\n",
            "loss: 0.742735  [ 1664/ 3200]\n",
            "loss: 1.404106  [ 1680/ 3200]\n",
            "loss: 0.662995  [ 1696/ 3200]\n",
            "loss: 1.201057  [ 1712/ 3200]\n",
            "loss: 0.670061  [ 1728/ 3200]\n",
            "loss: 0.772846  [ 1744/ 3200]\n",
            "loss: 1.140832  [ 1760/ 3200]\n",
            "loss: 0.889726  [ 1776/ 3200]\n",
            "loss: 0.907677  [ 1792/ 3200]\n",
            "loss: 0.987433  [ 1808/ 3200]\n",
            "loss: 0.958879  [ 1824/ 3200]\n",
            "loss: 0.937660  [ 1840/ 3200]\n",
            "loss: 0.864669  [ 1856/ 3200]\n",
            "loss: 0.692700  [ 1872/ 3200]\n",
            "loss: 0.892478  [ 1888/ 3200]\n",
            "loss: 0.610398  [ 1904/ 3200]\n",
            "loss: 1.029662  [ 1920/ 3200]\n",
            "loss: 0.807112  [ 1936/ 3200]\n",
            "loss: 1.031271  [ 1952/ 3200]\n",
            "loss: 0.918408  [ 1968/ 3200]\n",
            "loss: 0.890167  [ 1984/ 3200]\n",
            "loss: 1.155751  [ 2000/ 3200]\n",
            "loss: 0.820301  [ 2016/ 3200]\n",
            "loss: 0.982755  [ 2032/ 3200]\n",
            "loss: 1.032115  [ 2048/ 3200]\n",
            "loss: 0.844214  [ 2064/ 3200]\n",
            "loss: 1.409435  [ 2080/ 3200]\n",
            "loss: 1.096666  [ 2096/ 3200]\n",
            "loss: 0.881574  [ 2112/ 3200]\n",
            "loss: 1.087284  [ 2128/ 3200]\n",
            "loss: 0.890484  [ 2144/ 3200]\n",
            "loss: 0.964440  [ 2160/ 3200]\n",
            "loss: 1.092212  [ 2176/ 3200]\n",
            "loss: 0.948843  [ 2192/ 3200]\n",
            "loss: 1.143167  [ 2208/ 3200]\n",
            "loss: 0.897618  [ 2224/ 3200]\n",
            "loss: 1.377095  [ 2240/ 3200]\n",
            "loss: 1.387381  [ 2256/ 3200]\n",
            "loss: 0.766213  [ 2272/ 3200]\n",
            "loss: 0.786658  [ 2288/ 3200]\n",
            "loss: 0.777175  [ 2304/ 3200]\n",
            "loss: 0.684610  [ 2320/ 3200]\n",
            "loss: 1.040235  [ 2336/ 3200]\n",
            "loss: 0.702616  [ 2352/ 3200]\n",
            "loss: 0.828548  [ 2368/ 3200]\n",
            "loss: 1.301953  [ 2384/ 3200]\n",
            "loss: 1.008032  [ 2400/ 3200]\n",
            "loss: 1.040392  [ 2416/ 3200]\n",
            "loss: 1.083931  [ 2432/ 3200]\n",
            "loss: 1.098603  [ 2448/ 3200]\n",
            "loss: 0.984588  [ 2464/ 3200]\n",
            "loss: 0.780079  [ 2480/ 3200]\n",
            "loss: 0.983753  [ 2496/ 3200]\n",
            "loss: 1.314560  [ 2512/ 3200]\n",
            "loss: 1.029813  [ 2528/ 3200]\n",
            "loss: 0.887117  [ 2544/ 3200]\n",
            "loss: 0.915041  [ 2560/ 3200]\n",
            "loss: 1.047985  [ 2576/ 3200]\n",
            "loss: 0.957480  [ 2592/ 3200]\n",
            "loss: 0.940357  [ 2608/ 3200]\n",
            "loss: 0.979380  [ 2624/ 3200]\n",
            "loss: 0.749316  [ 2640/ 3200]\n",
            "loss: 1.016413  [ 2656/ 3200]\n",
            "loss: 1.094164  [ 2672/ 3200]\n",
            "loss: 0.928524  [ 2688/ 3200]\n",
            "loss: 0.939944  [ 2704/ 3200]\n",
            "loss: 1.176520  [ 2720/ 3200]\n",
            "loss: 1.099581  [ 2736/ 3200]\n",
            "loss: 0.827982  [ 2752/ 3200]\n",
            "loss: 1.115674  [ 2768/ 3200]\n",
            "loss: 0.922514  [ 2784/ 3200]\n",
            "loss: 0.828619  [ 2800/ 3200]\n",
            "loss: 0.931654  [ 2816/ 3200]\n",
            "loss: 1.055310  [ 2832/ 3200]\n",
            "loss: 0.915302  [ 2848/ 3200]\n",
            "loss: 1.026017  [ 2864/ 3200]\n",
            "loss: 0.884718  [ 2880/ 3200]\n",
            "loss: 0.964940  [ 2896/ 3200]\n",
            "loss: 0.921445  [ 2912/ 3200]\n",
            "loss: 1.080500  [ 2928/ 3200]\n",
            "loss: 0.781838  [ 2944/ 3200]\n",
            "loss: 0.825027  [ 2960/ 3200]\n",
            "loss: 0.984222  [ 2976/ 3200]\n",
            "loss: 0.681315  [ 2992/ 3200]\n",
            "loss: 0.538999  [ 3008/ 3200]\n",
            "loss: 0.863940  [ 3024/ 3200]\n",
            "loss: 0.831362  [ 3040/ 3200]\n",
            "loss: 1.057202  [ 3056/ 3200]\n",
            "loss: 0.772698  [ 3072/ 3200]\n",
            "loss: 1.102010  [ 3088/ 3200]\n",
            "loss: 0.918781  [ 3104/ 3200]\n",
            "loss: 0.850085  [ 3120/ 3200]\n",
            "loss: 0.928237  [ 3136/ 3200]\n",
            "loss: 1.012638  [ 3152/ 3200]\n",
            "loss: 1.269954  [ 3168/ 3200]\n",
            "loss: 0.933794  [ 3184/ 3200]\n",
            "current epoch: 28\n",
            "\n",
            "loss: 0.689685  [    0/ 3200]\n",
            "loss: 0.988427  [   16/ 3200]\n",
            "loss: 1.253052  [   32/ 3200]\n",
            "loss: 0.693462  [   48/ 3200]\n",
            "loss: 0.989763  [   64/ 3200]\n",
            "loss: 1.041197  [   80/ 3200]\n",
            "loss: 0.932402  [   96/ 3200]\n",
            "loss: 0.819854  [  112/ 3200]\n",
            "loss: 1.059489  [  128/ 3200]\n",
            "loss: 1.138552  [  144/ 3200]\n",
            "loss: 1.419469  [  160/ 3200]\n",
            "loss: 1.037239  [  176/ 3200]\n",
            "loss: 0.786283  [  192/ 3200]\n",
            "loss: 0.968948  [  208/ 3200]\n",
            "loss: 1.076070  [  224/ 3200]\n",
            "loss: 0.732641  [  240/ 3200]\n",
            "loss: 0.898657  [  256/ 3200]\n",
            "loss: 0.984299  [  272/ 3200]\n",
            "loss: 0.922585  [  288/ 3200]\n",
            "loss: 1.013113  [  304/ 3200]\n",
            "loss: 0.675401  [  320/ 3200]\n",
            "loss: 0.804582  [  336/ 3200]\n",
            "loss: 0.903463  [  352/ 3200]\n",
            "loss: 0.996569  [  368/ 3200]\n",
            "loss: 0.902437  [  384/ 3200]\n",
            "loss: 1.006920  [  400/ 3200]\n",
            "loss: 0.893334  [  416/ 3200]\n",
            "loss: 0.921736  [  432/ 3200]\n",
            "loss: 0.638526  [  448/ 3200]\n",
            "loss: 1.249625  [  464/ 3200]\n",
            "loss: 1.122405  [  480/ 3200]\n",
            "loss: 1.088769  [  496/ 3200]\n",
            "loss: 0.986157  [  512/ 3200]\n",
            "loss: 1.034989  [  528/ 3200]\n",
            "loss: 0.897780  [  544/ 3200]\n",
            "loss: 1.235916  [  560/ 3200]\n",
            "loss: 0.524697  [  576/ 3200]\n",
            "loss: 0.623593  [  592/ 3200]\n",
            "loss: 1.030857  [  608/ 3200]\n",
            "loss: 0.727709  [  624/ 3200]\n",
            "loss: 0.745496  [  640/ 3200]\n",
            "loss: 1.060880  [  656/ 3200]\n",
            "loss: 0.971514  [  672/ 3200]\n",
            "loss: 1.000335  [  688/ 3200]\n",
            "loss: 1.124984  [  704/ 3200]\n",
            "loss: 1.148050  [  720/ 3200]\n",
            "loss: 0.824383  [  736/ 3200]\n",
            "loss: 1.204560  [  752/ 3200]\n",
            "loss: 0.917890  [  768/ 3200]\n",
            "loss: 0.811314  [  784/ 3200]\n",
            "loss: 0.940750  [  800/ 3200]\n",
            "loss: 0.931939  [  816/ 3200]\n",
            "loss: 0.955672  [  832/ 3200]\n",
            "loss: 1.016924  [  848/ 3200]\n",
            "loss: 1.039214  [  864/ 3200]\n",
            "loss: 1.111103  [  880/ 3200]\n",
            "loss: 0.695609  [  896/ 3200]\n",
            "loss: 0.915255  [  912/ 3200]\n",
            "loss: 0.776267  [  928/ 3200]\n",
            "loss: 0.872038  [  944/ 3200]\n",
            "loss: 1.162147  [  960/ 3200]\n",
            "loss: 0.652405  [  976/ 3200]\n",
            "loss: 1.155976  [  992/ 3200]\n",
            "loss: 0.974832  [ 1008/ 3200]\n",
            "loss: 0.854514  [ 1024/ 3200]\n",
            "loss: 1.136884  [ 1040/ 3200]\n",
            "loss: 0.882782  [ 1056/ 3200]\n",
            "loss: 0.664270  [ 1072/ 3200]\n",
            "loss: 0.819164  [ 1088/ 3200]\n",
            "loss: 0.926831  [ 1104/ 3200]\n",
            "loss: 0.779739  [ 1120/ 3200]\n",
            "loss: 0.642658  [ 1136/ 3200]\n",
            "loss: 0.964597  [ 1152/ 3200]\n",
            "loss: 0.764517  [ 1168/ 3200]\n",
            "loss: 0.986084  [ 1184/ 3200]\n",
            "loss: 0.928557  [ 1200/ 3200]\n",
            "loss: 0.727073  [ 1216/ 3200]\n",
            "loss: 1.004388  [ 1232/ 3200]\n",
            "loss: 0.638815  [ 1248/ 3200]\n",
            "loss: 0.675526  [ 1264/ 3200]\n",
            "loss: 0.518780  [ 1280/ 3200]\n",
            "loss: 0.600667  [ 1296/ 3200]\n",
            "loss: 0.951911  [ 1312/ 3200]\n",
            "loss: 0.847459  [ 1328/ 3200]\n",
            "loss: 1.104192  [ 1344/ 3200]\n",
            "loss: 0.864385  [ 1360/ 3200]\n",
            "loss: 0.807946  [ 1376/ 3200]\n",
            "loss: 0.886868  [ 1392/ 3200]\n",
            "loss: 0.780060  [ 1408/ 3200]\n",
            "loss: 0.574840  [ 1424/ 3200]\n",
            "loss: 0.956698  [ 1440/ 3200]\n",
            "loss: 1.193777  [ 1456/ 3200]\n",
            "loss: 0.746197  [ 1472/ 3200]\n",
            "loss: 0.976216  [ 1488/ 3200]\n",
            "loss: 0.973637  [ 1504/ 3200]\n",
            "loss: 0.614278  [ 1520/ 3200]\n",
            "loss: 0.988003  [ 1536/ 3200]\n",
            "loss: 1.244929  [ 1552/ 3200]\n",
            "loss: 1.154498  [ 1568/ 3200]\n",
            "loss: 1.141992  [ 1584/ 3200]\n",
            "loss: 0.859107  [ 1600/ 3200]\n",
            "loss: 1.145009  [ 1616/ 3200]\n",
            "loss: 1.182796  [ 1632/ 3200]\n",
            "loss: 0.941996  [ 1648/ 3200]\n",
            "loss: 1.256689  [ 1664/ 3200]\n",
            "loss: 0.986205  [ 1680/ 3200]\n",
            "loss: 1.105165  [ 1696/ 3200]\n",
            "loss: 0.864572  [ 1712/ 3200]\n",
            "loss: 0.767799  [ 1728/ 3200]\n",
            "loss: 0.719781  [ 1744/ 3200]\n",
            "loss: 0.749304  [ 1760/ 3200]\n",
            "loss: 1.077615  [ 1776/ 3200]\n",
            "loss: 1.007699  [ 1792/ 3200]\n",
            "loss: 0.915000  [ 1808/ 3200]\n",
            "loss: 0.737685  [ 1824/ 3200]\n",
            "loss: 1.375976  [ 1840/ 3200]\n",
            "loss: 1.049629  [ 1856/ 3200]\n",
            "loss: 0.730858  [ 1872/ 3200]\n",
            "loss: 1.092286  [ 1888/ 3200]\n",
            "loss: 1.069242  [ 1904/ 3200]\n",
            "loss: 1.370544  [ 1920/ 3200]\n",
            "loss: 0.871585  [ 1936/ 3200]\n",
            "loss: 0.835950  [ 1952/ 3200]\n",
            "loss: 1.097102  [ 1968/ 3200]\n",
            "loss: 1.175685  [ 1984/ 3200]\n",
            "loss: 0.838586  [ 2000/ 3200]\n",
            "loss: 0.922839  [ 2016/ 3200]\n",
            "loss: 1.042985  [ 2032/ 3200]\n",
            "loss: 1.150529  [ 2048/ 3200]\n",
            "loss: 0.983195  [ 2064/ 3200]\n",
            "loss: 0.986204  [ 2080/ 3200]\n",
            "loss: 0.930749  [ 2096/ 3200]\n",
            "loss: 0.931565  [ 2112/ 3200]\n",
            "loss: 0.779525  [ 2128/ 3200]\n",
            "loss: 1.070196  [ 2144/ 3200]\n",
            "loss: 1.027438  [ 2160/ 3200]\n",
            "loss: 1.065724  [ 2176/ 3200]\n",
            "loss: 1.112294  [ 2192/ 3200]\n",
            "loss: 1.039018  [ 2208/ 3200]\n",
            "loss: 0.963333  [ 2224/ 3200]\n",
            "loss: 0.695564  [ 2240/ 3200]\n",
            "loss: 0.897425  [ 2256/ 3200]\n",
            "loss: 0.836456  [ 2272/ 3200]\n",
            "loss: 0.922712  [ 2288/ 3200]\n",
            "loss: 0.747812  [ 2304/ 3200]\n",
            "loss: 0.703273  [ 2320/ 3200]\n",
            "loss: 0.954639  [ 2336/ 3200]\n",
            "loss: 0.899770  [ 2352/ 3200]\n",
            "loss: 0.837028  [ 2368/ 3200]\n",
            "loss: 0.924060  [ 2384/ 3200]\n",
            "loss: 0.815069  [ 2400/ 3200]\n",
            "loss: 0.914521  [ 2416/ 3200]\n",
            "loss: 0.812151  [ 2432/ 3200]\n",
            "loss: 0.857894  [ 2448/ 3200]\n",
            "loss: 0.929077  [ 2464/ 3200]\n",
            "loss: 0.945985  [ 2480/ 3200]\n",
            "loss: 0.999985  [ 2496/ 3200]\n",
            "loss: 0.660612  [ 2512/ 3200]\n",
            "loss: 0.729023  [ 2528/ 3200]\n",
            "loss: 0.920560  [ 2544/ 3200]\n",
            "loss: 0.781137  [ 2560/ 3200]\n",
            "loss: 0.875070  [ 2576/ 3200]\n",
            "loss: 0.863439  [ 2592/ 3200]\n",
            "loss: 0.878011  [ 2608/ 3200]\n",
            "loss: 1.185580  [ 2624/ 3200]\n",
            "loss: 0.762573  [ 2640/ 3200]\n",
            "loss: 0.862815  [ 2656/ 3200]\n",
            "loss: 1.014610  [ 2672/ 3200]\n",
            "loss: 1.065198  [ 2688/ 3200]\n",
            "loss: 0.781225  [ 2704/ 3200]\n",
            "loss: 0.713872  [ 2720/ 3200]\n",
            "loss: 1.204145  [ 2736/ 3200]\n",
            "loss: 1.312056  [ 2752/ 3200]\n",
            "loss: 1.015980  [ 2768/ 3200]\n",
            "loss: 0.841064  [ 2784/ 3200]\n",
            "loss: 0.847894  [ 2800/ 3200]\n",
            "loss: 0.895068  [ 2816/ 3200]\n",
            "loss: 0.895870  [ 2832/ 3200]\n",
            "loss: 0.935885  [ 2848/ 3200]\n",
            "loss: 0.902834  [ 2864/ 3200]\n",
            "loss: 0.604068  [ 2880/ 3200]\n",
            "loss: 0.915754  [ 2896/ 3200]\n",
            "loss: 0.859014  [ 2912/ 3200]\n",
            "loss: 1.014687  [ 2928/ 3200]\n",
            "loss: 1.325678  [ 2944/ 3200]\n",
            "loss: 1.001290  [ 2960/ 3200]\n",
            "loss: 0.858885  [ 2976/ 3200]\n",
            "loss: 1.154045  [ 2992/ 3200]\n",
            "loss: 0.979049  [ 3008/ 3200]\n",
            "loss: 0.917568  [ 3024/ 3200]\n",
            "loss: 1.282643  [ 3040/ 3200]\n",
            "loss: 0.805309  [ 3056/ 3200]\n",
            "loss: 1.276772  [ 3072/ 3200]\n",
            "loss: 0.946120  [ 3088/ 3200]\n",
            "loss: 0.955341  [ 3104/ 3200]\n",
            "loss: 1.142380  [ 3120/ 3200]\n",
            "loss: 0.883070  [ 3136/ 3200]\n",
            "loss: 1.236798  [ 3152/ 3200]\n",
            "loss: 1.070284  [ 3168/ 3200]\n",
            "loss: 1.347368  [ 3184/ 3200]\n",
            "current epoch: 29\n",
            "\n",
            "loss: 0.737384  [    0/ 3200]\n",
            "loss: 1.143816  [   16/ 3200]\n",
            "loss: 0.839158  [   32/ 3200]\n",
            "loss: 0.905279  [   48/ 3200]\n",
            "loss: 1.164552  [   64/ 3200]\n",
            "loss: 0.874760  [   80/ 3200]\n",
            "loss: 0.820900  [   96/ 3200]\n",
            "loss: 1.078514  [  112/ 3200]\n",
            "loss: 0.778330  [  128/ 3200]\n",
            "loss: 0.920433  [  144/ 3200]\n",
            "loss: 1.167934  [  160/ 3200]\n",
            "loss: 0.907849  [  176/ 3200]\n",
            "loss: 0.977612  [  192/ 3200]\n",
            "loss: 0.960760  [  208/ 3200]\n",
            "loss: 1.122608  [  224/ 3200]\n",
            "loss: 0.790653  [  240/ 3200]\n",
            "loss: 0.764205  [  256/ 3200]\n",
            "loss: 0.867142  [  272/ 3200]\n",
            "loss: 0.654471  [  288/ 3200]\n",
            "loss: 0.812065  [  304/ 3200]\n",
            "loss: 0.978028  [  320/ 3200]\n",
            "loss: 0.981131  [  336/ 3200]\n",
            "loss: 0.568056  [  352/ 3200]\n",
            "loss: 0.803008  [  368/ 3200]\n",
            "loss: 0.920772  [  384/ 3200]\n",
            "loss: 1.052146  [  400/ 3200]\n",
            "loss: 0.577837  [  416/ 3200]\n",
            "loss: 1.408978  [  432/ 3200]\n",
            "loss: 0.787713  [  448/ 3200]\n",
            "loss: 0.771553  [  464/ 3200]\n",
            "loss: 0.969281  [  480/ 3200]\n",
            "loss: 1.300904  [  496/ 3200]\n",
            "loss: 1.014968  [  512/ 3200]\n",
            "loss: 0.728853  [  528/ 3200]\n",
            "loss: 0.849515  [  544/ 3200]\n",
            "loss: 1.088735  [  560/ 3200]\n",
            "loss: 0.828418  [  576/ 3200]\n",
            "loss: 1.492720  [  592/ 3200]\n",
            "loss: 1.118816  [  608/ 3200]\n",
            "loss: 0.955581  [  624/ 3200]\n",
            "loss: 1.127691  [  640/ 3200]\n",
            "loss: 0.690240  [  656/ 3200]\n",
            "loss: 1.013652  [  672/ 3200]\n",
            "loss: 1.080870  [  688/ 3200]\n",
            "loss: 0.728394  [  704/ 3200]\n",
            "loss: 1.211087  [  720/ 3200]\n",
            "loss: 0.653039  [  736/ 3200]\n",
            "loss: 0.849412  [  752/ 3200]\n",
            "loss: 0.715837  [  768/ 3200]\n",
            "loss: 0.863346  [  784/ 3200]\n",
            "loss: 0.767481  [  800/ 3200]\n",
            "loss: 1.322888  [  816/ 3200]\n",
            "loss: 0.888889  [  832/ 3200]\n",
            "loss: 0.918916  [  848/ 3200]\n",
            "loss: 1.068002  [  864/ 3200]\n",
            "loss: 0.695697  [  880/ 3200]\n",
            "loss: 1.632378  [  896/ 3200]\n",
            "loss: 1.033908  [  912/ 3200]\n",
            "loss: 0.927534  [  928/ 3200]\n",
            "loss: 1.262356  [  944/ 3200]\n",
            "loss: 0.941425  [  960/ 3200]\n",
            "loss: 1.005447  [  976/ 3200]\n",
            "loss: 0.911426  [  992/ 3200]\n",
            "loss: 1.351182  [ 1008/ 3200]\n",
            "loss: 0.911052  [ 1024/ 3200]\n",
            "loss: 1.108137  [ 1040/ 3200]\n",
            "loss: 0.784583  [ 1056/ 3200]\n",
            "loss: 0.630831  [ 1072/ 3200]\n",
            "loss: 0.678353  [ 1088/ 3200]\n",
            "loss: 1.136531  [ 1104/ 3200]\n",
            "loss: 0.998298  [ 1120/ 3200]\n",
            "loss: 0.779412  [ 1136/ 3200]\n",
            "loss: 0.649552  [ 1152/ 3200]\n",
            "loss: 0.907112  [ 1168/ 3200]\n",
            "loss: 0.671577  [ 1184/ 3200]\n",
            "loss: 0.842733  [ 1200/ 3200]\n",
            "loss: 0.752912  [ 1216/ 3200]\n",
            "loss: 1.204648  [ 1232/ 3200]\n",
            "loss: 0.998066  [ 1248/ 3200]\n",
            "loss: 0.653861  [ 1264/ 3200]\n",
            "loss: 0.716599  [ 1280/ 3200]\n",
            "loss: 0.883930  [ 1296/ 3200]\n",
            "loss: 0.902463  [ 1312/ 3200]\n",
            "loss: 0.937922  [ 1328/ 3200]\n",
            "loss: 0.818088  [ 1344/ 3200]\n",
            "loss: 1.120886  [ 1360/ 3200]\n",
            "loss: 1.085900  [ 1376/ 3200]\n",
            "loss: 1.816406  [ 1392/ 3200]\n",
            "loss: 0.844701  [ 1408/ 3200]\n",
            "loss: 1.197559  [ 1424/ 3200]\n",
            "loss: 1.032548  [ 1440/ 3200]\n",
            "loss: 1.077647  [ 1456/ 3200]\n",
            "loss: 0.895398  [ 1472/ 3200]\n",
            "loss: 1.161880  [ 1488/ 3200]\n",
            "loss: 0.918350  [ 1504/ 3200]\n",
            "loss: 0.717308  [ 1520/ 3200]\n",
            "loss: 0.744641  [ 1536/ 3200]\n",
            "loss: 1.198114  [ 1552/ 3200]\n",
            "loss: 0.708066  [ 1568/ 3200]\n",
            "loss: 0.941843  [ 1584/ 3200]\n",
            "loss: 0.855356  [ 1600/ 3200]\n",
            "loss: 0.804799  [ 1616/ 3200]\n",
            "loss: 0.879481  [ 1632/ 3200]\n",
            "loss: 0.714316  [ 1648/ 3200]\n",
            "loss: 0.730323  [ 1664/ 3200]\n",
            "loss: 0.953455  [ 1680/ 3200]\n",
            "loss: 0.896365  [ 1696/ 3200]\n",
            "loss: 0.792489  [ 1712/ 3200]\n",
            "loss: 0.953140  [ 1728/ 3200]\n",
            "loss: 0.816722  [ 1744/ 3200]\n",
            "loss: 0.664910  [ 1760/ 3200]\n",
            "loss: 0.889776  [ 1776/ 3200]\n",
            "loss: 1.088817  [ 1792/ 3200]\n",
            "loss: 0.669480  [ 1808/ 3200]\n",
            "loss: 1.032541  [ 1824/ 3200]\n",
            "loss: 1.152992  [ 1840/ 3200]\n",
            "loss: 1.002932  [ 1856/ 3200]\n",
            "loss: 0.798397  [ 1872/ 3200]\n",
            "loss: 0.942484  [ 1888/ 3200]\n",
            "loss: 1.140969  [ 1904/ 3200]\n",
            "loss: 0.968769  [ 1920/ 3200]\n",
            "loss: 1.094407  [ 1936/ 3200]\n",
            "loss: 1.062114  [ 1952/ 3200]\n",
            "loss: 0.677240  [ 1968/ 3200]\n",
            "loss: 1.270868  [ 1984/ 3200]\n",
            "loss: 0.984670  [ 2000/ 3200]\n",
            "loss: 0.851171  [ 2016/ 3200]\n",
            "loss: 0.895736  [ 2032/ 3200]\n",
            "loss: 1.152701  [ 2048/ 3200]\n",
            "loss: 0.907377  [ 2064/ 3200]\n",
            "loss: 0.719115  [ 2080/ 3200]\n",
            "loss: 0.970531  [ 2096/ 3200]\n",
            "loss: 0.935888  [ 2112/ 3200]\n",
            "loss: 1.122255  [ 2128/ 3200]\n",
            "loss: 0.671285  [ 2144/ 3200]\n",
            "loss: 0.759599  [ 2160/ 3200]\n",
            "loss: 0.926484  [ 2176/ 3200]\n",
            "loss: 1.061747  [ 2192/ 3200]\n",
            "loss: 0.997601  [ 2208/ 3200]\n",
            "loss: 0.829030  [ 2224/ 3200]\n",
            "loss: 1.174877  [ 2240/ 3200]\n",
            "loss: 1.321156  [ 2256/ 3200]\n",
            "loss: 1.096419  [ 2272/ 3200]\n",
            "loss: 0.758080  [ 2288/ 3200]\n",
            "loss: 1.049633  [ 2304/ 3200]\n",
            "loss: 0.786258  [ 2320/ 3200]\n",
            "loss: 0.801868  [ 2336/ 3200]\n",
            "loss: 1.044404  [ 2352/ 3200]\n",
            "loss: 0.688019  [ 2368/ 3200]\n",
            "loss: 0.783140  [ 2384/ 3200]\n",
            "loss: 1.104059  [ 2400/ 3200]\n",
            "loss: 0.736594  [ 2416/ 3200]\n",
            "loss: 0.714875  [ 2432/ 3200]\n",
            "loss: 0.769539  [ 2448/ 3200]\n",
            "loss: 0.866883  [ 2464/ 3200]\n",
            "loss: 1.002393  [ 2480/ 3200]\n",
            "loss: 0.896135  [ 2496/ 3200]\n",
            "loss: 0.827731  [ 2512/ 3200]\n",
            "loss: 0.844408  [ 2528/ 3200]\n",
            "loss: 0.797738  [ 2544/ 3200]\n",
            "loss: 1.138988  [ 2560/ 3200]\n",
            "loss: 0.897502  [ 2576/ 3200]\n",
            "loss: 0.716611  [ 2592/ 3200]\n",
            "loss: 0.957213  [ 2608/ 3200]\n",
            "loss: 0.804214  [ 2624/ 3200]\n",
            "loss: 1.024075  [ 2640/ 3200]\n",
            "loss: 0.993221  [ 2656/ 3200]\n",
            "loss: 0.841690  [ 2672/ 3200]\n",
            "loss: 0.832789  [ 2688/ 3200]\n",
            "loss: 1.021962  [ 2704/ 3200]\n",
            "loss: 0.887935  [ 2720/ 3200]\n",
            "loss: 0.867010  [ 2736/ 3200]\n",
            "loss: 0.995506  [ 2752/ 3200]\n",
            "loss: 0.872190  [ 2768/ 3200]\n",
            "loss: 0.877168  [ 2784/ 3200]\n",
            "loss: 0.971201  [ 2800/ 3200]\n",
            "loss: 0.889080  [ 2816/ 3200]\n",
            "loss: 0.645249  [ 2832/ 3200]\n",
            "loss: 0.826229  [ 2848/ 3200]\n",
            "loss: 0.941260  [ 2864/ 3200]\n",
            "loss: 0.968873  [ 2880/ 3200]\n",
            "loss: 0.591221  [ 2896/ 3200]\n",
            "loss: 0.854229  [ 2912/ 3200]\n",
            "loss: 0.771534  [ 2928/ 3200]\n",
            "loss: 0.865569  [ 2944/ 3200]\n",
            "loss: 1.303221  [ 2960/ 3200]\n",
            "loss: 1.164742  [ 2976/ 3200]\n",
            "loss: 1.009005  [ 2992/ 3200]\n",
            "loss: 0.962546  [ 3008/ 3200]\n",
            "loss: 1.024071  [ 3024/ 3200]\n",
            "loss: 0.786253  [ 3040/ 3200]\n",
            "loss: 0.864316  [ 3056/ 3200]\n",
            "loss: 0.849322  [ 3072/ 3200]\n",
            "loss: 0.931370  [ 3088/ 3200]\n",
            "loss: 0.685832  [ 3104/ 3200]\n",
            "loss: 1.288882  [ 3120/ 3200]\n",
            "loss: 1.074826  [ 3136/ 3200]\n",
            "loss: 0.797514  [ 3152/ 3200]\n",
            "loss: 1.138253  [ 3168/ 3200]\n",
            "loss: 1.211717  [ 3184/ 3200]\n",
            "\n",
            "Total Time for Training in GPU: 12.573621988296509\n",
            "Avg Accuracy: 68.125000%, Avg loss: 0.862097\n",
            "F1 score is: 0.6443109732866287\n",
            "Confusion Matrix:\n",
            "[[177  15   5   3]\n",
            " [ 41  84  18  57]\n",
            " [ 16  33 137  14]\n",
            " [  3  41   9 147]]\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "  device=\"cuda\"\n",
        "print(\"device=\", device)\n",
        "\n",
        "\n",
        "model = NeuralNetwork().to(device)\n",
        "cost_func = nn.CrossEntropyLoss()\n",
        "learning_rate = 0.002\n",
        "num_epochs = 30\n",
        "batch_size = 16\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
        "start_time = time.time()\n",
        "model = train(num_epochs, optimizer, train_dataloader,cost_func,model,device)\n",
        "print(f\"\\nTotal Time for Training in GPU: {time.time() - start_time}\")\n",
        "test_loss, test_f1, test_acc, confmatrix = evaluate(val_dataloader, cost_func, model,device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The time calculated is not representative of the actual processing time, when printing was disabled CPU time was greater than GPU time.\n",
        "Unfortunately, I didn't save the result and could't conduct the experiment again due to colab's limited GPU usage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qM9hmHsMdkPK"
      },
      "source": [
        "## Step 7 Model Selection and Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBX99Q52JGQ2",
        "outputId": "dc861af6-5bf6-42e2-d57a-9124506ad7a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "loss: 1.175381  [  352/ 3200]\n",
            "loss: 1.169015  [  368/ 3200]\n",
            "loss: 1.280321  [  384/ 3200]\n",
            "loss: 1.340634  [  400/ 3200]\n",
            "loss: 1.177653  [  416/ 3200]\n",
            "loss: 1.279228  [  432/ 3200]\n",
            "loss: 1.210498  [  448/ 3200]\n",
            "loss: 1.156305  [  464/ 3200]\n",
            "loss: 1.146421  [  480/ 3200]\n",
            "loss: 1.260819  [  496/ 3200]\n",
            "loss: 1.120504  [  512/ 3200]\n",
            "loss: 1.322996  [  528/ 3200]\n",
            "loss: 1.178998  [  544/ 3200]\n",
            "loss: 1.334173  [  560/ 3200]\n",
            "loss: 1.203482  [  576/ 3200]\n",
            "loss: 1.255793  [  592/ 3200]\n",
            "loss: 1.370736  [  608/ 3200]\n",
            "loss: 1.288966  [  624/ 3200]\n",
            "loss: 1.253666  [  640/ 3200]\n",
            "loss: 1.185452  [  656/ 3200]\n",
            "loss: 1.256722  [  672/ 3200]\n",
            "loss: 1.169144  [  688/ 3200]\n",
            "loss: 1.244200  [  704/ 3200]\n",
            "loss: 1.374501  [  720/ 3200]\n",
            "loss: 1.200307  [  736/ 3200]\n",
            "loss: 1.247524  [  752/ 3200]\n",
            "loss: 1.233574  [  768/ 3200]\n",
            "loss: 1.218867  [  784/ 3200]\n",
            "loss: 1.201267  [  800/ 3200]\n",
            "loss: 1.134461  [  816/ 3200]\n",
            "loss: 1.303767  [  832/ 3200]\n",
            "loss: 1.178594  [  848/ 3200]\n",
            "loss: 1.232268  [  864/ 3200]\n",
            "loss: 1.219251  [  880/ 3200]\n",
            "loss: 1.299304  [  896/ 3200]\n",
            "loss: 1.226453  [  912/ 3200]\n",
            "loss: 1.310979  [  928/ 3200]\n",
            "loss: 1.272344  [  944/ 3200]\n",
            "loss: 1.193254  [  960/ 3200]\n",
            "loss: 1.285697  [  976/ 3200]\n",
            "loss: 1.271119  [  992/ 3200]\n",
            "loss: 1.220474  [ 1008/ 3200]\n",
            "loss: 1.146185  [ 1024/ 3200]\n",
            "loss: 1.179280  [ 1040/ 3200]\n",
            "loss: 1.237749  [ 1056/ 3200]\n",
            "loss: 1.143373  [ 1072/ 3200]\n",
            "loss: 1.179030  [ 1088/ 3200]\n",
            "loss: 1.260122  [ 1104/ 3200]\n",
            "loss: 1.099707  [ 1120/ 3200]\n",
            "loss: 1.413498  [ 1136/ 3200]\n",
            "loss: 1.215654  [ 1152/ 3200]\n",
            "loss: 1.278949  [ 1168/ 3200]\n",
            "loss: 1.209759  [ 1184/ 3200]\n",
            "loss: 1.237293  [ 1200/ 3200]\n",
            "loss: 1.297145  [ 1216/ 3200]\n",
            "loss: 1.246032  [ 1232/ 3200]\n",
            "loss: 1.317661  [ 1248/ 3200]\n",
            "loss: 1.250417  [ 1264/ 3200]\n",
            "loss: 1.233535  [ 1280/ 3200]\n",
            "loss: 1.325723  [ 1296/ 3200]\n",
            "loss: 1.249644  [ 1312/ 3200]\n",
            "loss: 1.286028  [ 1328/ 3200]\n",
            "loss: 1.247991  [ 1344/ 3200]\n",
            "loss: 1.272475  [ 1360/ 3200]\n",
            "loss: 1.191760  [ 1376/ 3200]\n",
            "loss: 1.299430  [ 1392/ 3200]\n",
            "loss: 1.301092  [ 1408/ 3200]\n",
            "loss: 1.194570  [ 1424/ 3200]\n",
            "loss: 1.319908  [ 1440/ 3200]\n",
            "loss: 1.332521  [ 1456/ 3200]\n",
            "loss: 1.173247  [ 1472/ 3200]\n",
            "loss: 1.190235  [ 1488/ 3200]\n",
            "loss: 1.253318  [ 1504/ 3200]\n",
            "loss: 1.362466  [ 1520/ 3200]\n",
            "loss: 1.324338  [ 1536/ 3200]\n",
            "loss: 1.273724  [ 1552/ 3200]\n",
            "loss: 1.163258  [ 1568/ 3200]\n",
            "loss: 1.089795  [ 1584/ 3200]\n",
            "loss: 1.427978  [ 1600/ 3200]\n",
            "loss: 1.114227  [ 1616/ 3200]\n",
            "loss: 1.265148  [ 1632/ 3200]\n",
            "loss: 1.260560  [ 1648/ 3200]\n",
            "loss: 1.180438  [ 1664/ 3200]\n",
            "loss: 1.258775  [ 1680/ 3200]\n",
            "loss: 1.205694  [ 1696/ 3200]\n",
            "loss: 1.183623  [ 1712/ 3200]\n",
            "loss: 1.250048  [ 1728/ 3200]\n",
            "loss: 1.185976  [ 1744/ 3200]\n",
            "loss: 1.230269  [ 1760/ 3200]\n",
            "loss: 1.299940  [ 1776/ 3200]\n",
            "loss: 1.195458  [ 1792/ 3200]\n",
            "loss: 1.114217  [ 1808/ 3200]\n",
            "loss: 1.095606  [ 1824/ 3200]\n",
            "loss: 1.208695  [ 1840/ 3200]\n",
            "loss: 1.210530  [ 1856/ 3200]\n",
            "loss: 1.083468  [ 1872/ 3200]\n",
            "loss: 1.349585  [ 1888/ 3200]\n",
            "loss: 1.083186  [ 1904/ 3200]\n",
            "loss: 1.171808  [ 1920/ 3200]\n",
            "loss: 1.297060  [ 1936/ 3200]\n",
            "loss: 1.256438  [ 1952/ 3200]\n",
            "loss: 1.290878  [ 1968/ 3200]\n",
            "loss: 1.218235  [ 1984/ 3200]\n",
            "loss: 1.279506  [ 2000/ 3200]\n",
            "loss: 1.266074  [ 2016/ 3200]\n",
            "loss: 1.188138  [ 2032/ 3200]\n",
            "loss: 1.191278  [ 2048/ 3200]\n",
            "loss: 1.144429  [ 2064/ 3200]\n",
            "loss: 1.249364  [ 2080/ 3200]\n",
            "loss: 1.290889  [ 2096/ 3200]\n",
            "loss: 1.302185  [ 2112/ 3200]\n",
            "loss: 1.310914  [ 2128/ 3200]\n",
            "loss: 1.348500  [ 2144/ 3200]\n",
            "loss: 1.312303  [ 2160/ 3200]\n",
            "loss: 1.161963  [ 2176/ 3200]\n",
            "loss: 1.132149  [ 2192/ 3200]\n",
            "loss: 1.303614  [ 2208/ 3200]\n",
            "loss: 1.148877  [ 2224/ 3200]\n",
            "loss: 1.264319  [ 2240/ 3200]\n",
            "loss: 1.300227  [ 2256/ 3200]\n",
            "loss: 1.137269  [ 2272/ 3200]\n",
            "loss: 1.146380  [ 2288/ 3200]\n",
            "loss: 1.286013  [ 2304/ 3200]\n",
            "loss: 1.209688  [ 2320/ 3200]\n",
            "loss: 1.274114  [ 2336/ 3200]\n",
            "loss: 1.248336  [ 2352/ 3200]\n",
            "loss: 1.252417  [ 2368/ 3200]\n",
            "loss: 1.180890  [ 2384/ 3200]\n",
            "loss: 1.268610  [ 2400/ 3200]\n",
            "loss: 1.137280  [ 2416/ 3200]\n",
            "loss: 1.161734  [ 2432/ 3200]\n",
            "loss: 1.173392  [ 2448/ 3200]\n",
            "loss: 1.242404  [ 2464/ 3200]\n",
            "loss: 1.192020  [ 2480/ 3200]\n",
            "loss: 1.225339  [ 2496/ 3200]\n",
            "loss: 1.101372  [ 2512/ 3200]\n",
            "loss: 1.213323  [ 2528/ 3200]\n",
            "loss: 1.158106  [ 2544/ 3200]\n",
            "loss: 1.246966  [ 2560/ 3200]\n",
            "loss: 1.142306  [ 2576/ 3200]\n",
            "loss: 1.351558  [ 2592/ 3200]\n",
            "loss: 1.259866  [ 2608/ 3200]\n",
            "loss: 1.221950  [ 2624/ 3200]\n",
            "loss: 1.214635  [ 2640/ 3200]\n",
            "loss: 1.258340  [ 2656/ 3200]\n",
            "loss: 1.179396  [ 2672/ 3200]\n",
            "loss: 1.289553  [ 2688/ 3200]\n",
            "loss: 1.284367  [ 2704/ 3200]\n",
            "loss: 1.177399  [ 2720/ 3200]\n",
            "loss: 1.253429  [ 2736/ 3200]\n",
            "loss: 1.192462  [ 2752/ 3200]\n",
            "loss: 1.154471  [ 2768/ 3200]\n",
            "loss: 1.223568  [ 2784/ 3200]\n",
            "loss: 1.166682  [ 2800/ 3200]\n",
            "loss: 1.230385  [ 2816/ 3200]\n",
            "loss: 1.208900  [ 2832/ 3200]\n",
            "loss: 1.074170  [ 2848/ 3200]\n",
            "loss: 1.357047  [ 2864/ 3200]\n",
            "loss: 1.189511  [ 2880/ 3200]\n",
            "loss: 1.176333  [ 2896/ 3200]\n",
            "loss: 1.090711  [ 2912/ 3200]\n",
            "loss: 1.018261  [ 2928/ 3200]\n",
            "loss: 1.361904  [ 2944/ 3200]\n",
            "loss: 1.246828  [ 2960/ 3200]\n",
            "loss: 1.183090  [ 2976/ 3200]\n",
            "loss: 1.171080  [ 2992/ 3200]\n",
            "loss: 1.271487  [ 3008/ 3200]\n",
            "loss: 1.250135  [ 3024/ 3200]\n",
            "loss: 1.155275  [ 3040/ 3200]\n",
            "loss: 1.187176  [ 3056/ 3200]\n",
            "loss: 1.061998  [ 3072/ 3200]\n",
            "loss: 1.306737  [ 3088/ 3200]\n",
            "loss: 1.149907  [ 3104/ 3200]\n",
            "loss: 1.140681  [ 3120/ 3200]\n",
            "loss: 1.202941  [ 3136/ 3200]\n",
            "loss: 1.180337  [ 3152/ 3200]\n",
            "loss: 1.240957  [ 3168/ 3200]\n",
            "loss: 1.161146  [ 3184/ 3200]\n",
            "Avg Accuracy: 59.125000%, Avg loss: 1.206591\n",
            "F1 score is: 0.5112999302148818\n",
            "Confusion Matrix:\n",
            "[[156  10  25   9]\n",
            " [ 39  14 103  44]\n",
            " [  8   2 185   5]\n",
            " [  9   4  69 118]]\n",
            "current epoch: 7\n",
            "\n",
            "loss: 1.221576  [    0/ 3200]\n",
            "loss: 1.147201  [   16/ 3200]\n",
            "loss: 1.310798  [   32/ 3200]\n",
            "loss: 1.160537  [   48/ 3200]\n",
            "loss: 1.175061  [   64/ 3200]\n",
            "loss: 1.248615  [   80/ 3200]\n",
            "loss: 1.151360  [   96/ 3200]\n",
            "loss: 1.299131  [  112/ 3200]\n",
            "loss: 1.311358  [  128/ 3200]\n",
            "loss: 1.205517  [  144/ 3200]\n",
            "loss: 1.197610  [  160/ 3200]\n",
            "loss: 1.139034  [  176/ 3200]\n",
            "loss: 1.303973  [  192/ 3200]\n",
            "loss: 1.147786  [  208/ 3200]\n",
            "loss: 1.125328  [  224/ 3200]\n",
            "loss: 1.222807  [  240/ 3200]\n",
            "loss: 1.273772  [  256/ 3200]\n",
            "loss: 1.065575  [  272/ 3200]\n",
            "loss: 1.297484  [  288/ 3200]\n",
            "loss: 1.150757  [  304/ 3200]\n",
            "loss: 1.312811  [  320/ 3200]\n",
            "loss: 1.163428  [  336/ 3200]\n",
            "loss: 1.244466  [  352/ 3200]\n",
            "loss: 1.170868  [  368/ 3200]\n",
            "loss: 1.285046  [  384/ 3200]\n",
            "loss: 1.263139  [  400/ 3200]\n",
            "loss: 1.245198  [  416/ 3200]\n",
            "loss: 1.306618  [  432/ 3200]\n",
            "loss: 1.279511  [  448/ 3200]\n",
            "loss: 1.362903  [  464/ 3200]\n",
            "loss: 1.209424  [  480/ 3200]\n",
            "loss: 1.157967  [  496/ 3200]\n",
            "loss: 1.198173  [  512/ 3200]\n",
            "loss: 1.247185  [  528/ 3200]\n",
            "loss: 1.182763  [  544/ 3200]\n",
            "loss: 1.092045  [  560/ 3200]\n",
            "loss: 1.399995  [  576/ 3200]\n",
            "loss: 1.204305  [  592/ 3200]\n",
            "loss: 1.259668  [  608/ 3200]\n",
            "loss: 1.129305  [  624/ 3200]\n",
            "loss: 1.235059  [  640/ 3200]\n",
            "loss: 1.122373  [  656/ 3200]\n",
            "loss: 1.279829  [  672/ 3200]\n",
            "loss: 1.122180  [  688/ 3200]\n",
            "loss: 1.183847  [  704/ 3200]\n",
            "loss: 1.170374  [  720/ 3200]\n",
            "loss: 1.137164  [  736/ 3200]\n",
            "loss: 1.183350  [  752/ 3200]\n",
            "loss: 1.103398  [  768/ 3200]\n",
            "loss: 1.296966  [  784/ 3200]\n",
            "loss: 1.145358  [  800/ 3200]\n",
            "loss: 1.210964  [  816/ 3200]\n",
            "loss: 1.293711  [  832/ 3200]\n",
            "loss: 1.273454  [  848/ 3200]\n",
            "loss: 1.116326  [  864/ 3200]\n",
            "loss: 1.309507  [  880/ 3200]\n",
            "loss: 1.311241  [  896/ 3200]\n",
            "loss: 1.241700  [  912/ 3200]\n",
            "loss: 1.195349  [  928/ 3200]\n",
            "loss: 1.213496  [  944/ 3200]\n",
            "loss: 1.159583  [  960/ 3200]\n",
            "loss: 1.052529  [  976/ 3200]\n",
            "loss: 1.082651  [  992/ 3200]\n",
            "loss: 1.249731  [ 1008/ 3200]\n",
            "loss: 1.157023  [ 1024/ 3200]\n",
            "loss: 1.253788  [ 1040/ 3200]\n",
            "loss: 1.176744  [ 1056/ 3200]\n",
            "loss: 1.211229  [ 1072/ 3200]\n",
            "loss: 1.369103  [ 1088/ 3200]\n",
            "loss: 1.261312  [ 1104/ 3200]\n",
            "loss: 1.252347  [ 1120/ 3200]\n",
            "loss: 1.244403  [ 1136/ 3200]\n",
            "loss: 1.168668  [ 1152/ 3200]\n",
            "loss: 1.400268  [ 1168/ 3200]\n",
            "loss: 1.249283  [ 1184/ 3200]\n",
            "loss: 1.289363  [ 1200/ 3200]\n",
            "loss: 1.223285  [ 1216/ 3200]\n",
            "loss: 1.197578  [ 1232/ 3200]\n",
            "loss: 1.246116  [ 1248/ 3200]\n",
            "loss: 1.115957  [ 1264/ 3200]\n",
            "loss: 0.913689  [ 1280/ 3200]\n",
            "loss: 1.124012  [ 1296/ 3200]\n",
            "loss: 1.388088  [ 1312/ 3200]\n",
            "loss: 1.252355  [ 1328/ 3200]\n",
            "loss: 1.129396  [ 1344/ 3200]\n",
            "loss: 1.217370  [ 1360/ 3200]\n",
            "loss: 1.253827  [ 1376/ 3200]\n",
            "loss: 1.147008  [ 1392/ 3200]\n",
            "loss: 1.217011  [ 1408/ 3200]\n",
            "loss: 1.050114  [ 1424/ 3200]\n",
            "loss: 1.189524  [ 1440/ 3200]\n",
            "loss: 1.207992  [ 1456/ 3200]\n",
            "loss: 1.237995  [ 1472/ 3200]\n",
            "loss: 1.165326  [ 1488/ 3200]\n",
            "loss: 1.173065  [ 1504/ 3200]\n",
            "loss: 1.089559  [ 1520/ 3200]\n",
            "loss: 1.202485  [ 1536/ 3200]\n",
            "loss: 1.205901  [ 1552/ 3200]\n",
            "loss: 1.003565  [ 1568/ 3200]\n",
            "loss: 1.283769  [ 1584/ 3200]\n",
            "loss: 0.977947  [ 1600/ 3200]\n",
            "loss: 1.123829  [ 1616/ 3200]\n",
            "loss: 1.215460  [ 1632/ 3200]\n",
            "loss: 1.135779  [ 1648/ 3200]\n",
            "loss: 1.306100  [ 1664/ 3200]\n",
            "loss: 1.252835  [ 1680/ 3200]\n",
            "loss: 1.237880  [ 1696/ 3200]\n",
            "loss: 1.195978  [ 1712/ 3200]\n",
            "loss: 1.155518  [ 1728/ 3200]\n",
            "loss: 1.145675  [ 1744/ 3200]\n",
            "loss: 1.002363  [ 1760/ 3200]\n",
            "loss: 1.257808  [ 1776/ 3200]\n",
            "loss: 1.244763  [ 1792/ 3200]\n",
            "loss: 1.130480  [ 1808/ 3200]\n",
            "loss: 1.312761  [ 1824/ 3200]\n",
            "loss: 1.232494  [ 1840/ 3200]\n",
            "loss: 1.213264  [ 1856/ 3200]\n",
            "loss: 1.143451  [ 1872/ 3200]\n",
            "loss: 1.255026  [ 1888/ 3200]\n",
            "loss: 1.225570  [ 1904/ 3200]\n",
            "loss: 1.209116  [ 1920/ 3200]\n",
            "loss: 1.296253  [ 1936/ 3200]\n",
            "loss: 1.238769  [ 1952/ 3200]\n",
            "loss: 1.221152  [ 1968/ 3200]\n",
            "loss: 1.208464  [ 1984/ 3200]\n",
            "loss: 1.098099  [ 2000/ 3200]\n",
            "loss: 1.255780  [ 2016/ 3200]\n",
            "loss: 1.218552  [ 2032/ 3200]\n",
            "loss: 1.282264  [ 2048/ 3200]\n",
            "loss: 1.171861  [ 2064/ 3200]\n",
            "loss: 1.269753  [ 2080/ 3200]\n",
            "loss: 1.148908  [ 2096/ 3200]\n",
            "loss: 1.257092  [ 2112/ 3200]\n",
            "loss: 1.177022  [ 2128/ 3200]\n",
            "loss: 1.154574  [ 2144/ 3200]\n",
            "loss: 1.204363  [ 2160/ 3200]\n",
            "loss: 1.129995  [ 2176/ 3200]\n",
            "loss: 1.176427  [ 2192/ 3200]\n",
            "loss: 1.299963  [ 2208/ 3200]\n",
            "loss: 1.289976  [ 2224/ 3200]\n",
            "loss: 1.032543  [ 2240/ 3200]\n",
            "loss: 1.182762  [ 2256/ 3200]\n",
            "loss: 1.212521  [ 2272/ 3200]\n",
            "loss: 1.140484  [ 2288/ 3200]\n",
            "loss: 1.181548  [ 2304/ 3200]\n",
            "loss: 1.134453  [ 2320/ 3200]\n",
            "loss: 1.208396  [ 2336/ 3200]\n",
            "loss: 1.033344  [ 2352/ 3200]\n",
            "loss: 1.225954  [ 2368/ 3200]\n",
            "loss: 1.192060  [ 2384/ 3200]\n",
            "loss: 1.160300  [ 2400/ 3200]\n",
            "loss: 1.178323  [ 2416/ 3200]\n",
            "loss: 1.006207  [ 2432/ 3200]\n",
            "loss: 1.106871  [ 2448/ 3200]\n",
            "loss: 1.157493  [ 2464/ 3200]\n",
            "loss: 1.328714  [ 2480/ 3200]\n",
            "loss: 1.188204  [ 2496/ 3200]\n",
            "loss: 1.208211  [ 2512/ 3200]\n",
            "loss: 1.148374  [ 2528/ 3200]\n",
            "loss: 1.152736  [ 2544/ 3200]\n",
            "loss: 1.143321  [ 2560/ 3200]\n",
            "loss: 1.166690  [ 2576/ 3200]\n",
            "loss: 1.071902  [ 2592/ 3200]\n",
            "loss: 1.343108  [ 2608/ 3200]\n",
            "loss: 1.132500  [ 2624/ 3200]\n",
            "loss: 1.129456  [ 2640/ 3200]\n",
            "loss: 1.112166  [ 2656/ 3200]\n",
            "loss: 1.173228  [ 2672/ 3200]\n",
            "loss: 1.193699  [ 2688/ 3200]\n",
            "loss: 1.130671  [ 2704/ 3200]\n",
            "loss: 1.125429  [ 2720/ 3200]\n",
            "loss: 1.183975  [ 2736/ 3200]\n",
            "loss: 1.193210  [ 2752/ 3200]\n",
            "loss: 1.215008  [ 2768/ 3200]\n",
            "loss: 1.184946  [ 2784/ 3200]\n",
            "loss: 1.149802  [ 2800/ 3200]\n",
            "loss: 1.258128  [ 2816/ 3200]\n",
            "loss: 1.238077  [ 2832/ 3200]\n",
            "loss: 1.149708  [ 2848/ 3200]\n",
            "loss: 1.107565  [ 2864/ 3200]\n",
            "loss: 1.254697  [ 2880/ 3200]\n",
            "loss: 1.219739  [ 2896/ 3200]\n",
            "loss: 1.303213  [ 2912/ 3200]\n",
            "loss: 1.074955  [ 2928/ 3200]\n",
            "loss: 1.157674  [ 2944/ 3200]\n",
            "loss: 1.156427  [ 2960/ 3200]\n",
            "loss: 1.145906  [ 2976/ 3200]\n",
            "loss: 1.362139  [ 2992/ 3200]\n",
            "loss: 1.247872  [ 3008/ 3200]\n",
            "loss: 1.175857  [ 3024/ 3200]\n",
            "loss: 1.275123  [ 3040/ 3200]\n",
            "loss: 1.102772  [ 3056/ 3200]\n",
            "loss: 1.257405  [ 3072/ 3200]\n",
            "loss: 1.221043  [ 3088/ 3200]\n",
            "loss: 1.185848  [ 3104/ 3200]\n",
            "loss: 1.287042  [ 3120/ 3200]\n",
            "loss: 1.329709  [ 3136/ 3200]\n",
            "loss: 0.943031  [ 3152/ 3200]\n",
            "loss: 1.146318  [ 3168/ 3200]\n",
            "loss: 1.244885  [ 3184/ 3200]\n",
            "Avg Accuracy: 54.250000%, Avg loss: 1.182931\n",
            "F1 score is: 0.42948929846286776\n",
            "Confusion Matrix:\n",
            "[[190   0   1   9]\n",
            " [ 76   0   2 122]\n",
            " [ 43   0  64  93]\n",
            " [ 18   0   2 180]]\n",
            "current epoch: 8\n",
            "\n",
            "loss: 1.206394  [    0/ 3200]\n",
            "loss: 1.078503  [   16/ 3200]\n",
            "loss: 1.154560  [   32/ 3200]\n",
            "loss: 1.268216  [   48/ 3200]\n",
            "loss: 1.061794  [   64/ 3200]\n",
            "loss: 1.147881  [   80/ 3200]\n",
            "loss: 1.189094  [   96/ 3200]\n",
            "loss: 1.314705  [  112/ 3200]\n",
            "loss: 1.105368  [  128/ 3200]\n",
            "loss: 1.271587  [  144/ 3200]\n",
            "loss: 1.227305  [  160/ 3200]\n",
            "loss: 1.188983  [  176/ 3200]\n",
            "loss: 1.251626  [  192/ 3200]\n",
            "loss: 1.212857  [  208/ 3200]\n",
            "loss: 1.283815  [  224/ 3200]\n",
            "loss: 1.015186  [  240/ 3200]\n",
            "loss: 1.166979  [  256/ 3200]\n",
            "loss: 1.226709  [  272/ 3200]\n",
            "loss: 1.296291  [  288/ 3200]\n",
            "loss: 1.157710  [  304/ 3200]\n",
            "loss: 1.237542  [  320/ 3200]\n",
            "loss: 1.160467  [  336/ 3200]\n",
            "loss: 1.198529  [  352/ 3200]\n",
            "loss: 1.165825  [  368/ 3200]\n",
            "loss: 1.295336  [  384/ 3200]\n",
            "loss: 1.281062  [  400/ 3200]\n",
            "loss: 1.024540  [  416/ 3200]\n",
            "loss: 1.127312  [  432/ 3200]\n",
            "loss: 1.176108  [  448/ 3200]\n",
            "loss: 1.134814  [  464/ 3200]\n",
            "loss: 1.223709  [  480/ 3200]\n",
            "loss: 1.217467  [  496/ 3200]\n",
            "loss: 1.105186  [  512/ 3200]\n",
            "loss: 1.230096  [  528/ 3200]\n",
            "loss: 1.193207  [  544/ 3200]\n",
            "loss: 1.284544  [  560/ 3200]\n",
            "loss: 1.282617  [  576/ 3200]\n",
            "loss: 1.121880  [  592/ 3200]\n",
            "loss: 1.219017  [  608/ 3200]\n",
            "loss: 1.360964  [  624/ 3200]\n",
            "loss: 1.234280  [  640/ 3200]\n",
            "loss: 1.336992  [  656/ 3200]\n",
            "loss: 0.952737  [  672/ 3200]\n",
            "loss: 1.166210  [  688/ 3200]\n",
            "loss: 1.093924  [  704/ 3200]\n",
            "loss: 1.215871  [  720/ 3200]\n",
            "loss: 1.228007  [  736/ 3200]\n",
            "loss: 1.309391  [  752/ 3200]\n",
            "loss: 1.168030  [  768/ 3200]\n",
            "loss: 1.169877  [  784/ 3200]\n",
            "loss: 1.170223  [  800/ 3200]\n",
            "loss: 1.279758  [  816/ 3200]\n",
            "loss: 1.102965  [  832/ 3200]\n",
            "loss: 1.195431  [  848/ 3200]\n",
            "loss: 1.127598  [  864/ 3200]\n",
            "loss: 1.148015  [  880/ 3200]\n",
            "loss: 1.179905  [  896/ 3200]\n",
            "loss: 1.116063  [  912/ 3200]\n",
            "loss: 1.121092  [  928/ 3200]\n",
            "loss: 1.364405  [  944/ 3200]\n",
            "loss: 1.087915  [  960/ 3200]\n",
            "loss: 1.222446  [  976/ 3200]\n",
            "loss: 1.235848  [  992/ 3200]\n",
            "loss: 1.122474  [ 1008/ 3200]\n",
            "loss: 1.215862  [ 1024/ 3200]\n",
            "loss: 1.229897  [ 1040/ 3200]\n",
            "loss: 1.104022  [ 1056/ 3200]\n",
            "loss: 1.195848  [ 1072/ 3200]\n",
            "loss: 1.168165  [ 1088/ 3200]\n",
            "loss: 1.159235  [ 1104/ 3200]\n",
            "loss: 1.193544  [ 1120/ 3200]\n",
            "loss: 1.213303  [ 1136/ 3200]\n",
            "loss: 1.282252  [ 1152/ 3200]\n",
            "loss: 1.265943  [ 1168/ 3200]\n",
            "loss: 1.152000  [ 1184/ 3200]\n",
            "loss: 1.138751  [ 1200/ 3200]\n",
            "loss: 1.166023  [ 1216/ 3200]\n",
            "loss: 1.088542  [ 1232/ 3200]\n",
            "loss: 1.329833  [ 1248/ 3200]\n",
            "loss: 1.186608  [ 1264/ 3200]\n",
            "loss: 1.031047  [ 1280/ 3200]\n",
            "loss: 1.224297  [ 1296/ 3200]\n",
            "loss: 1.243286  [ 1312/ 3200]\n",
            "loss: 1.285686  [ 1328/ 3200]\n",
            "loss: 1.223833  [ 1344/ 3200]\n",
            "loss: 1.253147  [ 1360/ 3200]\n",
            "loss: 1.225503  [ 1376/ 3200]\n",
            "loss: 1.170109  [ 1392/ 3200]\n",
            "loss: 1.081073  [ 1408/ 3200]\n",
            "loss: 1.218215  [ 1424/ 3200]\n",
            "loss: 1.197075  [ 1440/ 3200]\n",
            "loss: 1.064195  [ 1456/ 3200]\n",
            "loss: 1.138677  [ 1472/ 3200]\n",
            "loss: 1.111041  [ 1488/ 3200]\n",
            "loss: 1.077071  [ 1504/ 3200]\n",
            "loss: 1.092878  [ 1520/ 3200]\n",
            "loss: 1.191969  [ 1536/ 3200]\n",
            "loss: 1.143253  [ 1552/ 3200]\n",
            "loss: 1.191706  [ 1568/ 3200]\n",
            "loss: 1.042590  [ 1584/ 3200]\n",
            "loss: 1.279099  [ 1600/ 3200]\n",
            "loss: 1.219206  [ 1616/ 3200]\n",
            "loss: 1.237004  [ 1632/ 3200]\n",
            "loss: 1.229165  [ 1648/ 3200]\n",
            "loss: 1.159846  [ 1664/ 3200]\n",
            "loss: 1.204753  [ 1680/ 3200]\n",
            "loss: 1.133829  [ 1696/ 3200]\n",
            "loss: 1.072313  [ 1712/ 3200]\n",
            "loss: 1.227725  [ 1728/ 3200]\n",
            "loss: 1.292158  [ 1744/ 3200]\n",
            "loss: 1.128876  [ 1760/ 3200]\n",
            "loss: 1.061142  [ 1776/ 3200]\n",
            "loss: 1.174084  [ 1792/ 3200]\n",
            "loss: 1.093548  [ 1808/ 3200]\n",
            "loss: 1.173089  [ 1824/ 3200]\n",
            "loss: 1.129718  [ 1840/ 3200]\n",
            "loss: 1.329393  [ 1856/ 3200]\n",
            "loss: 1.138376  [ 1872/ 3200]\n",
            "loss: 1.191250  [ 1888/ 3200]\n",
            "loss: 1.101265  [ 1904/ 3200]\n",
            "loss: 1.203700  [ 1920/ 3200]\n",
            "loss: 1.210527  [ 1936/ 3200]\n",
            "loss: 1.114831  [ 1952/ 3200]\n",
            "loss: 1.160574  [ 1968/ 3200]\n",
            "loss: 1.139183  [ 1984/ 3200]\n",
            "loss: 1.147480  [ 2000/ 3200]\n",
            "loss: 1.204270  [ 2016/ 3200]\n",
            "loss: 1.411895  [ 2032/ 3200]\n",
            "loss: 1.073101  [ 2048/ 3200]\n",
            "loss: 1.070797  [ 2064/ 3200]\n",
            "loss: 1.142926  [ 2080/ 3200]\n",
            "loss: 1.108066  [ 2096/ 3200]\n",
            "loss: 1.181686  [ 2112/ 3200]\n",
            "loss: 0.989702  [ 2128/ 3200]\n",
            "loss: 1.115349  [ 2144/ 3200]\n",
            "loss: 1.232723  [ 2160/ 3200]\n",
            "loss: 1.113423  [ 2176/ 3200]\n",
            "loss: 1.142303  [ 2192/ 3200]\n",
            "loss: 1.163787  [ 2208/ 3200]\n",
            "loss: 1.254162  [ 2224/ 3200]\n",
            "loss: 1.072382  [ 2240/ 3200]\n",
            "loss: 1.129168  [ 2256/ 3200]\n",
            "loss: 0.980334  [ 2272/ 3200]\n",
            "loss: 1.256794  [ 2288/ 3200]\n",
            "loss: 1.061909  [ 2304/ 3200]\n",
            "loss: 1.032890  [ 2320/ 3200]\n",
            "loss: 1.229415  [ 2336/ 3200]\n",
            "loss: 0.990931  [ 2352/ 3200]\n",
            "loss: 1.224983  [ 2368/ 3200]\n",
            "loss: 1.228882  [ 2384/ 3200]\n",
            "loss: 1.358116  [ 2400/ 3200]\n",
            "loss: 1.103740  [ 2416/ 3200]\n",
            "loss: 1.020233  [ 2432/ 3200]\n",
            "loss: 1.216877  [ 2448/ 3200]\n",
            "loss: 1.182133  [ 2464/ 3200]\n",
            "loss: 1.271731  [ 2480/ 3200]\n",
            "loss: 1.090456  [ 2496/ 3200]\n",
            "loss: 1.067082  [ 2512/ 3200]\n",
            "loss: 1.095598  [ 2528/ 3200]\n",
            "loss: 1.323005  [ 2544/ 3200]\n",
            "loss: 1.180157  [ 2560/ 3200]\n",
            "loss: 1.277083  [ 2576/ 3200]\n",
            "loss: 1.196147  [ 2592/ 3200]\n",
            "loss: 1.111518  [ 2608/ 3200]\n",
            "loss: 1.243468  [ 2624/ 3200]\n",
            "loss: 1.141181  [ 2640/ 3200]\n",
            "loss: 1.084612  [ 2656/ 3200]\n",
            "loss: 1.093081  [ 2672/ 3200]\n",
            "loss: 1.224873  [ 2688/ 3200]\n",
            "loss: 1.111594  [ 2704/ 3200]\n",
            "loss: 1.230499  [ 2720/ 3200]\n",
            "loss: 1.182766  [ 2736/ 3200]\n",
            "loss: 1.123261  [ 2752/ 3200]\n",
            "loss: 1.109500  [ 2768/ 3200]\n",
            "loss: 1.084662  [ 2784/ 3200]\n",
            "loss: 1.116752  [ 2800/ 3200]\n",
            "loss: 1.233762  [ 2816/ 3200]\n",
            "loss: 1.224906  [ 2832/ 3200]\n",
            "loss: 1.158382  [ 2848/ 3200]\n",
            "loss: 1.152474  [ 2864/ 3200]\n",
            "loss: 1.111878  [ 2880/ 3200]\n",
            "loss: 1.203460  [ 2896/ 3200]\n",
            "loss: 1.196382  [ 2912/ 3200]\n",
            "loss: 1.340066  [ 2928/ 3200]\n",
            "loss: 1.211095  [ 2944/ 3200]\n",
            "loss: 1.223941  [ 2960/ 3200]\n",
            "loss: 1.172569  [ 2976/ 3200]\n",
            "loss: 1.226766  [ 2992/ 3200]\n",
            "loss: 1.205729  [ 3008/ 3200]\n",
            "loss: 1.338077  [ 3024/ 3200]\n",
            "loss: 1.147231  [ 3040/ 3200]\n",
            "loss: 1.111065  [ 3056/ 3200]\n",
            "loss: 1.252839  [ 3072/ 3200]\n",
            "loss: 0.977437  [ 3088/ 3200]\n",
            "loss: 1.146199  [ 3104/ 3200]\n",
            "loss: 1.145456  [ 3120/ 3200]\n",
            "loss: 1.070493  [ 3136/ 3200]\n",
            "loss: 1.112290  [ 3152/ 3200]\n",
            "loss: 0.953478  [ 3168/ 3200]\n",
            "loss: 1.050975  [ 3184/ 3200]\n",
            "Avg Accuracy: 62.000000%, Avg loss: 1.168789\n",
            "F1 score is: 0.519065563082695\n",
            "Confusion Matrix:\n",
            "[[183   0  12   5]\n",
            " [ 64   0  76  60]\n",
            " [ 19   0 174   7]\n",
            " [ 16   0  45 139]]\n",
            "current epoch: 9\n",
            "\n",
            "loss: 1.010233  [    0/ 3200]\n",
            "loss: 1.215323  [   16/ 3200]\n",
            "loss: 1.307424  [   32/ 3200]\n",
            "loss: 1.155217  [   48/ 3200]\n",
            "loss: 1.004591  [   64/ 3200]\n",
            "loss: 1.237702  [   80/ 3200]\n",
            "loss: 1.228681  [   96/ 3200]\n",
            "loss: 0.947227  [  112/ 3200]\n",
            "loss: 1.174671  [  128/ 3200]\n",
            "loss: 1.023732  [  144/ 3200]\n",
            "loss: 1.038680  [  160/ 3200]\n",
            "loss: 1.151757  [  176/ 3200]\n",
            "loss: 1.056431  [  192/ 3200]\n",
            "loss: 1.269192  [  208/ 3200]\n",
            "loss: 1.235431  [  224/ 3200]\n",
            "loss: 1.328839  [  240/ 3200]\n",
            "loss: 1.274189  [  256/ 3200]\n",
            "loss: 1.172103  [  272/ 3200]\n",
            "loss: 1.334706  [  288/ 3200]\n",
            "loss: 1.057352  [  304/ 3200]\n",
            "loss: 1.160706  [  320/ 3200]\n",
            "loss: 1.254260  [  336/ 3200]\n",
            "loss: 1.197290  [  352/ 3200]\n",
            "loss: 1.148237  [  368/ 3200]\n",
            "loss: 1.060180  [  384/ 3200]\n",
            "loss: 1.206371  [  400/ 3200]\n",
            "loss: 1.280548  [  416/ 3200]\n",
            "loss: 1.120270  [  432/ 3200]\n",
            "loss: 1.084223  [  448/ 3200]\n",
            "loss: 1.155460  [  464/ 3200]\n",
            "loss: 1.248987  [  480/ 3200]\n",
            "loss: 1.215108  [  496/ 3200]\n",
            "loss: 1.070096  [  512/ 3200]\n",
            "loss: 1.273342  [  528/ 3200]\n",
            "loss: 1.163876  [  544/ 3200]\n",
            "loss: 1.301426  [  560/ 3200]\n",
            "loss: 1.161631  [  576/ 3200]\n",
            "loss: 1.326156  [  592/ 3200]\n",
            "loss: 1.068410  [  608/ 3200]\n",
            "loss: 1.177946  [  624/ 3200]\n",
            "loss: 1.090439  [  640/ 3200]\n",
            "loss: 1.087203  [  656/ 3200]\n",
            "loss: 1.220040  [  672/ 3200]\n",
            "loss: 1.252027  [  688/ 3200]\n",
            "loss: 1.226221  [  704/ 3200]\n",
            "loss: 1.150689  [  720/ 3200]\n",
            "loss: 1.105730  [  736/ 3200]\n",
            "loss: 1.032076  [  752/ 3200]\n",
            "loss: 1.052714  [  768/ 3200]\n",
            "loss: 1.179204  [  784/ 3200]\n",
            "loss: 1.117712  [  800/ 3200]\n",
            "loss: 1.156670  [  816/ 3200]\n",
            "loss: 1.043260  [  832/ 3200]\n",
            "loss: 1.066418  [  848/ 3200]\n",
            "loss: 1.173029  [  864/ 3200]\n",
            "loss: 1.231036  [  880/ 3200]\n",
            "loss: 1.225372  [  896/ 3200]\n",
            "loss: 1.191423  [  912/ 3200]\n",
            "loss: 1.010069  [  928/ 3200]\n",
            "loss: 1.083743  [  944/ 3200]\n",
            "loss: 1.079769  [  960/ 3200]\n",
            "loss: 1.171539  [  976/ 3200]\n",
            "loss: 1.136811  [  992/ 3200]\n",
            "loss: 1.225465  [ 1008/ 3200]\n",
            "loss: 1.114698  [ 1024/ 3200]\n",
            "loss: 1.093363  [ 1040/ 3200]\n",
            "loss: 1.155345  [ 1056/ 3200]\n",
            "loss: 1.048251  [ 1072/ 3200]\n",
            "loss: 1.074067  [ 1088/ 3200]\n",
            "loss: 1.113091  [ 1104/ 3200]\n",
            "loss: 1.095847  [ 1120/ 3200]\n",
            "loss: 1.276837  [ 1136/ 3200]\n",
            "loss: 1.156929  [ 1152/ 3200]\n",
            "loss: 1.109369  [ 1168/ 3200]\n",
            "loss: 1.107811  [ 1184/ 3200]\n",
            "loss: 1.172088  [ 1200/ 3200]\n",
            "loss: 1.081844  [ 1216/ 3200]\n",
            "loss: 1.252958  [ 1232/ 3200]\n",
            "loss: 1.044963  [ 1248/ 3200]\n",
            "loss: 1.044185  [ 1264/ 3200]\n",
            "loss: 1.260772  [ 1280/ 3200]\n",
            "loss: 1.057159  [ 1296/ 3200]\n",
            "loss: 1.036476  [ 1312/ 3200]\n",
            "loss: 1.126620  [ 1328/ 3200]\n",
            "loss: 1.101624  [ 1344/ 3200]\n",
            "loss: 1.173074  [ 1360/ 3200]\n",
            "loss: 1.135745  [ 1376/ 3200]\n",
            "loss: 1.051477  [ 1392/ 3200]\n",
            "loss: 1.359586  [ 1408/ 3200]\n",
            "loss: 1.252108  [ 1424/ 3200]\n",
            "loss: 1.149627  [ 1440/ 3200]\n",
            "loss: 1.148291  [ 1456/ 3200]\n",
            "loss: 1.420005  [ 1472/ 3200]\n",
            "loss: 1.130309  [ 1488/ 3200]\n",
            "loss: 1.262360  [ 1504/ 3200]\n",
            "loss: 1.100223  [ 1520/ 3200]\n",
            "loss: 1.268586  [ 1536/ 3200]\n",
            "loss: 1.082571  [ 1552/ 3200]\n",
            "loss: 1.025651  [ 1568/ 3200]\n",
            "loss: 1.126888  [ 1584/ 3200]\n",
            "loss: 1.055698  [ 1600/ 3200]\n",
            "loss: 1.144449  [ 1616/ 3200]\n",
            "loss: 1.281606  [ 1632/ 3200]\n",
            "loss: 1.160485  [ 1648/ 3200]\n",
            "loss: 1.103320  [ 1664/ 3200]\n",
            "loss: 1.183637  [ 1680/ 3200]\n",
            "loss: 1.147076  [ 1696/ 3200]\n",
            "loss: 1.137576  [ 1712/ 3200]\n",
            "loss: 1.065524  [ 1728/ 3200]\n",
            "loss: 1.064474  [ 1744/ 3200]\n",
            "loss: 1.117833  [ 1760/ 3200]\n",
            "loss: 1.241013  [ 1776/ 3200]\n",
            "loss: 1.118257  [ 1792/ 3200]\n",
            "loss: 1.021079  [ 1808/ 3200]\n",
            "loss: 1.140417  [ 1824/ 3200]\n",
            "loss: 1.214958  [ 1840/ 3200]\n",
            "loss: 1.098057  [ 1856/ 3200]\n",
            "loss: 1.123093  [ 1872/ 3200]\n",
            "loss: 1.205998  [ 1888/ 3200]\n",
            "loss: 0.992081  [ 1904/ 3200]\n",
            "loss: 1.106267  [ 1920/ 3200]\n",
            "loss: 1.236486  [ 1936/ 3200]\n",
            "loss: 1.207397  [ 1952/ 3200]\n",
            "loss: 1.194582  [ 1968/ 3200]\n",
            "loss: 1.116753  [ 1984/ 3200]\n",
            "loss: 1.256203  [ 2000/ 3200]\n",
            "loss: 1.080918  [ 2016/ 3200]\n",
            "loss: 1.217053  [ 2032/ 3200]\n",
            "loss: 1.097181  [ 2048/ 3200]\n",
            "loss: 1.076213  [ 2064/ 3200]\n",
            "loss: 1.154541  [ 2080/ 3200]\n",
            "loss: 1.117786  [ 2096/ 3200]\n",
            "loss: 1.135947  [ 2112/ 3200]\n",
            "loss: 1.023985  [ 2128/ 3200]\n",
            "loss: 1.131020  [ 2144/ 3200]\n",
            "loss: 1.139173  [ 2160/ 3200]\n",
            "loss: 0.992773  [ 2176/ 3200]\n",
            "loss: 1.276521  [ 2192/ 3200]\n",
            "loss: 1.182560  [ 2208/ 3200]\n",
            "loss: 1.113874  [ 2224/ 3200]\n",
            "loss: 1.062519  [ 2240/ 3200]\n",
            "loss: 1.095535  [ 2256/ 3200]\n",
            "loss: 0.951360  [ 2272/ 3200]\n",
            "loss: 1.101608  [ 2288/ 3200]\n",
            "loss: 1.162622  [ 2304/ 3200]\n",
            "loss: 1.100195  [ 2320/ 3200]\n",
            "loss: 1.303310  [ 2336/ 3200]\n",
            "loss: 1.273139  [ 2352/ 3200]\n",
            "loss: 1.121128  [ 2368/ 3200]\n",
            "loss: 1.136858  [ 2384/ 3200]\n",
            "loss: 1.206481  [ 2400/ 3200]\n",
            "loss: 1.165328  [ 2416/ 3200]\n",
            "loss: 1.112090  [ 2432/ 3200]\n",
            "loss: 0.988389  [ 2448/ 3200]\n",
            "loss: 1.147053  [ 2464/ 3200]\n",
            "loss: 1.029228  [ 2480/ 3200]\n",
            "loss: 1.413991  [ 2496/ 3200]\n",
            "loss: 1.224785  [ 2512/ 3200]\n",
            "loss: 1.150073  [ 2528/ 3200]\n",
            "loss: 1.092603  [ 2544/ 3200]\n",
            "loss: 1.151241  [ 2560/ 3200]\n",
            "loss: 1.030874  [ 2576/ 3200]\n",
            "loss: 1.113555  [ 2592/ 3200]\n",
            "loss: 1.085280  [ 2608/ 3200]\n",
            "loss: 1.091039  [ 2624/ 3200]\n",
            "loss: 1.317837  [ 2640/ 3200]\n",
            "loss: 1.330870  [ 2656/ 3200]\n",
            "loss: 1.355259  [ 2672/ 3200]\n",
            "loss: 1.114596  [ 2688/ 3200]\n",
            "loss: 1.145309  [ 2704/ 3200]\n",
            "loss: 0.875966  [ 2720/ 3200]\n",
            "loss: 1.113422  [ 2736/ 3200]\n",
            "loss: 1.273206  [ 2752/ 3200]\n",
            "loss: 1.066783  [ 2768/ 3200]\n",
            "loss: 1.239504  [ 2784/ 3200]\n",
            "loss: 0.966107  [ 2800/ 3200]\n",
            "loss: 1.091639  [ 2816/ 3200]\n",
            "loss: 1.104414  [ 2832/ 3200]\n",
            "loss: 0.991951  [ 2848/ 3200]\n",
            "loss: 1.139203  [ 2864/ 3200]\n",
            "loss: 1.214352  [ 2880/ 3200]\n",
            "loss: 1.146253  [ 2896/ 3200]\n",
            "loss: 1.379857  [ 2912/ 3200]\n",
            "loss: 1.152087  [ 2928/ 3200]\n",
            "loss: 1.025757  [ 2944/ 3200]\n",
            "loss: 1.141694  [ 2960/ 3200]\n",
            "loss: 1.114963  [ 2976/ 3200]\n",
            "loss: 1.153512  [ 2992/ 3200]\n",
            "loss: 1.099586  [ 3008/ 3200]\n",
            "loss: 1.139826  [ 3024/ 3200]\n",
            "loss: 1.094557  [ 3040/ 3200]\n",
            "loss: 1.228026  [ 3056/ 3200]\n",
            "loss: 1.033378  [ 3072/ 3200]\n",
            "loss: 1.126593  [ 3088/ 3200]\n",
            "loss: 1.047681  [ 3104/ 3200]\n",
            "loss: 1.104951  [ 3120/ 3200]\n",
            "loss: 1.234878  [ 3136/ 3200]\n",
            "loss: 1.270508  [ 3152/ 3200]\n",
            "loss: 1.090627  [ 3168/ 3200]\n",
            "loss: 1.367291  [ 3184/ 3200]\n",
            "Avg Accuracy: 63.250000%, Avg loss: 1.123276\n",
            "F1 score is: 0.5486439859867096\n",
            "Confusion Matrix:\n",
            "[[161   6  17  16]\n",
            " [ 34   6  41 119]\n",
            " [ 10   1 162  27]\n",
            " [  5   0  18 177]]\n",
            "current epoch: 10\n",
            "\n",
            "loss: 1.150183  [    0/ 3200]\n",
            "loss: 1.210469  [   16/ 3200]\n",
            "loss: 1.147376  [   32/ 3200]\n",
            "loss: 1.160514  [   48/ 3200]\n",
            "loss: 1.163392  [   64/ 3200]\n",
            "loss: 1.158363  [   80/ 3200]\n",
            "loss: 1.159883  [   96/ 3200]\n",
            "loss: 1.342399  [  112/ 3200]\n",
            "loss: 1.103891  [  128/ 3200]\n",
            "loss: 1.180414  [  144/ 3200]\n",
            "loss: 1.026427  [  160/ 3200]\n",
            "loss: 1.153431  [  176/ 3200]\n",
            "loss: 0.946982  [  192/ 3200]\n",
            "loss: 1.162082  [  208/ 3200]\n",
            "loss: 1.033643  [  224/ 3200]\n",
            "loss: 1.061090  [  240/ 3200]\n",
            "loss: 1.210927  [  256/ 3200]\n",
            "loss: 1.081450  [  272/ 3200]\n",
            "loss: 1.228229  [  288/ 3200]\n",
            "loss: 1.094723  [  304/ 3200]\n",
            "loss: 1.144333  [  320/ 3200]\n",
            "loss: 1.022023  [  336/ 3200]\n",
            "loss: 1.115517  [  352/ 3200]\n",
            "loss: 1.278402  [  368/ 3200]\n",
            "loss: 1.118652  [  384/ 3200]\n",
            "loss: 1.234883  [  400/ 3200]\n",
            "loss: 1.241601  [  416/ 3200]\n",
            "loss: 1.165119  [  432/ 3200]\n",
            "loss: 1.120210  [  448/ 3200]\n",
            "loss: 1.282506  [  464/ 3200]\n",
            "loss: 1.142670  [  480/ 3200]\n",
            "loss: 1.204034  [  496/ 3200]\n",
            "loss: 1.185419  [  512/ 3200]\n",
            "loss: 1.086865  [  528/ 3200]\n",
            "loss: 1.102720  [  544/ 3200]\n",
            "loss: 0.998313  [  560/ 3200]\n",
            "loss: 1.125960  [  576/ 3200]\n",
            "loss: 1.118481  [  592/ 3200]\n",
            "loss: 1.270732  [  608/ 3200]\n",
            "loss: 1.200910  [  624/ 3200]\n",
            "loss: 1.085882  [  640/ 3200]\n",
            "loss: 1.134708  [  656/ 3200]\n",
            "loss: 1.043279  [  672/ 3200]\n",
            "loss: 1.021038  [  688/ 3200]\n",
            "loss: 1.197445  [  704/ 3200]\n",
            "loss: 1.162875  [  720/ 3200]\n",
            "loss: 1.236886  [  736/ 3200]\n",
            "loss: 0.961475  [  752/ 3200]\n",
            "loss: 1.126655  [  768/ 3200]\n",
            "loss: 1.137773  [  784/ 3200]\n",
            "loss: 1.139497  [  800/ 3200]\n",
            "loss: 1.174501  [  816/ 3200]\n",
            "loss: 1.317532  [  832/ 3200]\n",
            "loss: 1.191844  [  848/ 3200]\n",
            "loss: 1.112787  [  864/ 3200]\n",
            "loss: 1.174745  [  880/ 3200]\n",
            "loss: 1.085822  [  896/ 3200]\n",
            "loss: 1.105581  [  912/ 3200]\n",
            "loss: 1.154734  [  928/ 3200]\n",
            "loss: 1.106289  [  944/ 3200]\n",
            "loss: 1.088606  [  960/ 3200]\n",
            "loss: 1.260140  [  976/ 3200]\n",
            "loss: 1.214847  [  992/ 3200]\n",
            "loss: 1.327379  [ 1008/ 3200]\n",
            "loss: 1.085063  [ 1024/ 3200]\n",
            "loss: 1.325327  [ 1040/ 3200]\n",
            "loss: 1.170051  [ 1056/ 3200]\n",
            "loss: 1.049777  [ 1072/ 3200]\n",
            "loss: 1.319894  [ 1088/ 3200]\n",
            "loss: 1.149740  [ 1104/ 3200]\n",
            "loss: 1.092772  [ 1120/ 3200]\n",
            "loss: 1.123071  [ 1136/ 3200]\n",
            "loss: 0.921889  [ 1152/ 3200]\n",
            "loss: 0.993325  [ 1168/ 3200]\n",
            "loss: 1.163559  [ 1184/ 3200]\n",
            "loss: 1.108199  [ 1200/ 3200]\n",
            "loss: 1.109731  [ 1216/ 3200]\n",
            "loss: 1.292694  [ 1232/ 3200]\n",
            "loss: 1.011824  [ 1248/ 3200]\n",
            "loss: 0.976120  [ 1264/ 3200]\n",
            "loss: 1.159447  [ 1280/ 3200]\n",
            "loss: 1.073611  [ 1296/ 3200]\n",
            "loss: 1.170809  [ 1312/ 3200]\n",
            "loss: 1.049217  [ 1328/ 3200]\n",
            "loss: 0.979446  [ 1344/ 3200]\n",
            "loss: 1.155712  [ 1360/ 3200]\n",
            "loss: 1.149269  [ 1376/ 3200]\n",
            "loss: 1.196454  [ 1392/ 3200]\n",
            "loss: 1.155485  [ 1408/ 3200]\n",
            "loss: 1.303906  [ 1424/ 3200]\n",
            "loss: 1.139904  [ 1440/ 3200]\n",
            "loss: 1.279061  [ 1456/ 3200]\n",
            "loss: 1.105809  [ 1472/ 3200]\n",
            "loss: 0.971068  [ 1488/ 3200]\n",
            "loss: 1.075070  [ 1504/ 3200]\n",
            "loss: 1.295241  [ 1520/ 3200]\n",
            "loss: 1.226099  [ 1536/ 3200]\n",
            "loss: 1.200942  [ 1552/ 3200]\n",
            "loss: 1.156691  [ 1568/ 3200]\n",
            "loss: 1.362414  [ 1584/ 3200]\n",
            "loss: 1.262509  [ 1600/ 3200]\n",
            "loss: 1.104716  [ 1616/ 3200]\n",
            "loss: 1.179214  [ 1632/ 3200]\n",
            "loss: 1.158212  [ 1648/ 3200]\n",
            "loss: 1.187189  [ 1664/ 3200]\n",
            "loss: 0.900546  [ 1680/ 3200]\n",
            "loss: 1.159181  [ 1696/ 3200]\n",
            "loss: 1.193525  [ 1712/ 3200]\n",
            "loss: 1.062667  [ 1728/ 3200]\n",
            "loss: 1.064948  [ 1744/ 3200]\n",
            "loss: 1.254065  [ 1760/ 3200]\n",
            "loss: 1.021965  [ 1776/ 3200]\n",
            "loss: 1.075075  [ 1792/ 3200]\n",
            "loss: 1.102901  [ 1808/ 3200]\n",
            "loss: 1.188716  [ 1824/ 3200]\n",
            "loss: 1.004777  [ 1840/ 3200]\n",
            "loss: 1.100008  [ 1856/ 3200]\n",
            "loss: 1.278283  [ 1872/ 3200]\n",
            "loss: 1.147934  [ 1888/ 3200]\n",
            "loss: 1.283473  [ 1904/ 3200]\n",
            "loss: 1.026505  [ 1920/ 3200]\n",
            "loss: 0.965282  [ 1936/ 3200]\n",
            "loss: 1.193028  [ 1952/ 3200]\n",
            "loss: 0.978289  [ 1968/ 3200]\n",
            "loss: 0.971627  [ 1984/ 3200]\n",
            "loss: 1.013178  [ 2000/ 3200]\n",
            "loss: 0.822882  [ 2016/ 3200]\n",
            "loss: 1.196473  [ 2032/ 3200]\n",
            "loss: 1.310666  [ 2048/ 3200]\n",
            "loss: 1.023699  [ 2064/ 3200]\n",
            "loss: 1.131735  [ 2080/ 3200]\n",
            "loss: 1.008588  [ 2096/ 3200]\n",
            "loss: 1.323106  [ 2112/ 3200]\n",
            "loss: 1.253772  [ 2128/ 3200]\n",
            "loss: 0.896157  [ 2144/ 3200]\n",
            "loss: 1.115501  [ 2160/ 3200]\n",
            "loss: 1.078601  [ 2176/ 3200]\n",
            "loss: 1.026289  [ 2192/ 3200]\n",
            "loss: 1.085714  [ 2208/ 3200]\n",
            "loss: 1.202847  [ 2224/ 3200]\n",
            "loss: 1.077190  [ 2240/ 3200]\n",
            "loss: 1.019914  [ 2256/ 3200]\n",
            "loss: 1.153643  [ 2272/ 3200]\n",
            "loss: 0.873563  [ 2288/ 3200]\n",
            "loss: 1.043104  [ 2304/ 3200]\n",
            "loss: 1.039129  [ 2320/ 3200]\n",
            "loss: 1.137176  [ 2336/ 3200]\n",
            "loss: 0.965052  [ 2352/ 3200]\n",
            "loss: 1.216193  [ 2368/ 3200]\n",
            "loss: 1.232957  [ 2384/ 3200]\n",
            "loss: 1.091896  [ 2400/ 3200]\n",
            "loss: 1.177230  [ 2416/ 3200]\n",
            "loss: 1.033904  [ 2432/ 3200]\n",
            "loss: 1.113707  [ 2448/ 3200]\n",
            "loss: 1.125237  [ 2464/ 3200]\n",
            "loss: 0.995145  [ 2480/ 3200]\n",
            "loss: 0.922762  [ 2496/ 3200]\n",
            "loss: 1.161557  [ 2512/ 3200]\n",
            "loss: 1.409902  [ 2528/ 3200]\n",
            "loss: 1.027074  [ 2544/ 3200]\n",
            "loss: 1.191167  [ 2560/ 3200]\n",
            "loss: 1.192354  [ 2576/ 3200]\n",
            "loss: 0.998275  [ 2592/ 3200]\n",
            "loss: 1.044271  [ 2608/ 3200]\n",
            "loss: 1.113745  [ 2624/ 3200]\n",
            "loss: 0.921811  [ 2640/ 3200]\n",
            "loss: 0.913463  [ 2656/ 3200]\n",
            "loss: 0.988836  [ 2672/ 3200]\n",
            "loss: 0.994818  [ 2688/ 3200]\n",
            "loss: 1.172205  [ 2704/ 3200]\n",
            "loss: 1.037377  [ 2720/ 3200]\n",
            "loss: 1.048768  [ 2736/ 3200]\n",
            "loss: 1.125897  [ 2752/ 3200]\n",
            "loss: 1.222384  [ 2768/ 3200]\n",
            "loss: 0.989633  [ 2784/ 3200]\n",
            "loss: 1.283897  [ 2800/ 3200]\n",
            "loss: 1.030237  [ 2816/ 3200]\n",
            "loss: 1.148574  [ 2832/ 3200]\n",
            "loss: 1.174649  [ 2848/ 3200]\n",
            "loss: 1.137945  [ 2864/ 3200]\n",
            "loss: 0.983094  [ 2880/ 3200]\n",
            "loss: 1.089531  [ 2896/ 3200]\n",
            "loss: 1.033956  [ 2912/ 3200]\n",
            "loss: 1.218642  [ 2928/ 3200]\n",
            "loss: 1.310074  [ 2944/ 3200]\n",
            "loss: 1.179583  [ 2960/ 3200]\n",
            "loss: 0.992014  [ 2976/ 3200]\n",
            "loss: 1.067316  [ 2992/ 3200]\n",
            "loss: 1.350065  [ 3008/ 3200]\n",
            "loss: 1.014225  [ 3024/ 3200]\n",
            "loss: 1.172098  [ 3040/ 3200]\n",
            "loss: 1.083239  [ 3056/ 3200]\n",
            "loss: 1.064018  [ 3072/ 3200]\n",
            "loss: 1.331412  [ 3088/ 3200]\n",
            "loss: 1.186913  [ 3104/ 3200]\n",
            "loss: 1.273064  [ 3120/ 3200]\n",
            "loss: 1.061819  [ 3136/ 3200]\n",
            "loss: 1.057058  [ 3152/ 3200]\n",
            "loss: 1.050105  [ 3168/ 3200]\n",
            "loss: 0.984188  [ 3184/ 3200]\n",
            "Avg Accuracy: 59.375000%, Avg loss: 1.101893\n",
            "F1 score is: 0.5527824598550797\n",
            "Confusion Matrix:\n",
            "[[189   7   0   4]\n",
            " [ 88  70  14  28]\n",
            " [ 36  57 100   7]\n",
            " [ 32  37  15 116]]\n",
            "current epoch: 11\n",
            "\n",
            "loss: 1.016567  [    0/ 3200]\n",
            "loss: 1.071591  [   16/ 3200]\n",
            "loss: 1.259606  [   32/ 3200]\n",
            "loss: 0.876849  [   48/ 3200]\n",
            "loss: 1.106275  [   64/ 3200]\n",
            "loss: 1.022386  [   80/ 3200]\n",
            "loss: 1.141388  [   96/ 3200]\n",
            "loss: 0.963888  [  112/ 3200]\n",
            "loss: 1.274945  [  128/ 3200]\n",
            "loss: 1.243268  [  144/ 3200]\n",
            "loss: 0.993425  [  160/ 3200]\n",
            "loss: 1.258598  [  176/ 3200]\n",
            "loss: 0.966791  [  192/ 3200]\n",
            "loss: 1.048172  [  208/ 3200]\n",
            "loss: 1.076476  [  224/ 3200]\n",
            "loss: 0.963385  [  240/ 3200]\n",
            "loss: 1.190795  [  256/ 3200]\n",
            "loss: 1.044253  [  272/ 3200]\n",
            "loss: 1.061369  [  288/ 3200]\n",
            "loss: 0.895786  [  304/ 3200]\n",
            "loss: 0.994382  [  320/ 3200]\n",
            "loss: 1.165828  [  336/ 3200]\n",
            "loss: 1.089932  [  352/ 3200]\n",
            "loss: 1.190320  [  368/ 3200]\n",
            "loss: 1.120594  [  384/ 3200]\n",
            "loss: 0.945241  [  400/ 3200]\n",
            "loss: 1.234104  [  416/ 3200]\n",
            "loss: 0.908587  [  432/ 3200]\n",
            "loss: 1.383545  [  448/ 3200]\n",
            "loss: 0.994971  [  464/ 3200]\n",
            "loss: 1.183310  [  480/ 3200]\n",
            "loss: 1.097154  [  496/ 3200]\n",
            "loss: 1.189688  [  512/ 3200]\n",
            "loss: 1.192949  [  528/ 3200]\n",
            "loss: 1.033840  [  544/ 3200]\n",
            "loss: 0.989919  [  560/ 3200]\n",
            "loss: 1.153493  [  576/ 3200]\n",
            "loss: 1.171488  [  592/ 3200]\n",
            "loss: 1.210628  [  608/ 3200]\n",
            "loss: 1.041346  [  624/ 3200]\n",
            "loss: 0.999768  [  640/ 3200]\n",
            "loss: 1.047446  [  656/ 3200]\n",
            "loss: 1.143972  [  672/ 3200]\n",
            "loss: 1.157019  [  688/ 3200]\n",
            "loss: 1.000013  [  704/ 3200]\n",
            "loss: 1.112872  [  720/ 3200]\n",
            "loss: 1.273395  [  736/ 3200]\n",
            "loss: 1.080752  [  752/ 3200]\n",
            "loss: 1.232713  [  768/ 3200]\n",
            "loss: 1.097396  [  784/ 3200]\n",
            "loss: 0.884661  [  800/ 3200]\n",
            "loss: 0.946636  [  816/ 3200]\n",
            "loss: 1.108726  [  832/ 3200]\n",
            "loss: 1.112004  [  848/ 3200]\n",
            "loss: 1.078772  [  864/ 3200]\n",
            "loss: 1.297111  [  880/ 3200]\n",
            "loss: 1.057638  [  896/ 3200]\n",
            "loss: 1.219306  [  912/ 3200]\n",
            "loss: 0.964719  [  928/ 3200]\n",
            "loss: 1.090781  [  944/ 3200]\n",
            "loss: 1.076798  [  960/ 3200]\n",
            "loss: 1.080619  [  976/ 3200]\n",
            "loss: 1.015751  [  992/ 3200]\n",
            "loss: 0.862159  [ 1008/ 3200]\n",
            "loss: 1.086479  [ 1024/ 3200]\n",
            "loss: 1.349357  [ 1040/ 3200]\n",
            "loss: 1.007943  [ 1056/ 3200]\n",
            "loss: 1.183213  [ 1072/ 3200]\n",
            "loss: 1.178215  [ 1088/ 3200]\n",
            "loss: 1.167587  [ 1104/ 3200]\n",
            "loss: 0.863226  [ 1120/ 3200]\n",
            "loss: 0.905676  [ 1136/ 3200]\n",
            "loss: 1.084142  [ 1152/ 3200]\n",
            "loss: 1.312285  [ 1168/ 3200]\n",
            "loss: 1.257657  [ 1184/ 3200]\n",
            "loss: 1.151716  [ 1200/ 3200]\n",
            "loss: 0.862400  [ 1216/ 3200]\n",
            "loss: 1.123844  [ 1232/ 3200]\n",
            "loss: 1.189638  [ 1248/ 3200]\n",
            "loss: 1.160164  [ 1264/ 3200]\n",
            "loss: 1.024419  [ 1280/ 3200]\n",
            "loss: 1.115735  [ 1296/ 3200]\n",
            "loss: 1.216010  [ 1312/ 3200]\n",
            "loss: 1.179359  [ 1328/ 3200]\n",
            "loss: 1.250116  [ 1344/ 3200]\n",
            "loss: 1.285584  [ 1360/ 3200]\n",
            "loss: 1.059441  [ 1376/ 3200]\n",
            "loss: 0.970297  [ 1392/ 3200]\n",
            "loss: 1.175186  [ 1408/ 3200]\n",
            "loss: 1.023746  [ 1424/ 3200]\n",
            "loss: 1.178936  [ 1440/ 3200]\n",
            "loss: 0.972521  [ 1456/ 3200]\n",
            "loss: 1.208000  [ 1472/ 3200]\n",
            "loss: 1.024605  [ 1488/ 3200]\n",
            "loss: 1.060822  [ 1504/ 3200]\n",
            "loss: 1.076205  [ 1520/ 3200]\n",
            "loss: 1.110263  [ 1536/ 3200]\n",
            "loss: 1.038258  [ 1552/ 3200]\n",
            "loss: 1.157560  [ 1568/ 3200]\n",
            "loss: 1.336458  [ 1584/ 3200]\n",
            "loss: 1.007247  [ 1600/ 3200]\n",
            "loss: 1.103045  [ 1616/ 3200]\n",
            "loss: 1.236536  [ 1632/ 3200]\n",
            "loss: 1.175291  [ 1648/ 3200]\n",
            "loss: 1.296884  [ 1664/ 3200]\n",
            "loss: 1.005785  [ 1680/ 3200]\n",
            "loss: 1.051316  [ 1696/ 3200]\n",
            "loss: 1.103779  [ 1712/ 3200]\n",
            "loss: 1.117410  [ 1728/ 3200]\n",
            "loss: 1.139919  [ 1744/ 3200]\n",
            "loss: 1.115259  [ 1760/ 3200]\n",
            "loss: 1.099987  [ 1776/ 3200]\n",
            "loss: 0.969530  [ 1792/ 3200]\n",
            "loss: 1.113873  [ 1808/ 3200]\n",
            "loss: 1.223692  [ 1824/ 3200]\n",
            "loss: 1.198982  [ 1840/ 3200]\n",
            "loss: 1.112696  [ 1856/ 3200]\n",
            "loss: 0.779020  [ 1872/ 3200]\n",
            "loss: 1.049214  [ 1888/ 3200]\n",
            "loss: 0.967245  [ 1904/ 3200]\n",
            "loss: 1.204297  [ 1920/ 3200]\n",
            "loss: 1.090853  [ 1936/ 3200]\n",
            "loss: 1.063673  [ 1952/ 3200]\n",
            "loss: 0.965633  [ 1968/ 3200]\n",
            "loss: 1.072051  [ 1984/ 3200]\n",
            "loss: 0.974185  [ 2000/ 3200]\n",
            "loss: 1.105113  [ 2016/ 3200]\n",
            "loss: 0.995677  [ 2032/ 3200]\n",
            "loss: 1.219589  [ 2048/ 3200]\n",
            "loss: 1.285200  [ 2064/ 3200]\n",
            "loss: 1.044265  [ 2080/ 3200]\n",
            "loss: 1.143491  [ 2096/ 3200]\n",
            "loss: 1.152677  [ 2112/ 3200]\n",
            "loss: 1.092948  [ 2128/ 3200]\n",
            "loss: 1.175452  [ 2144/ 3200]\n",
            "loss: 1.113243  [ 2160/ 3200]\n",
            "loss: 1.196324  [ 2176/ 3200]\n",
            "loss: 0.974868  [ 2192/ 3200]\n",
            "loss: 0.988855  [ 2208/ 3200]\n",
            "loss: 0.950597  [ 2224/ 3200]\n",
            "loss: 1.068596  [ 2240/ 3200]\n",
            "loss: 1.089020  [ 2256/ 3200]\n",
            "loss: 1.136954  [ 2272/ 3200]\n",
            "loss: 0.997481  [ 2288/ 3200]\n",
            "loss: 1.194014  [ 2304/ 3200]\n",
            "loss: 1.003305  [ 2320/ 3200]\n",
            "loss: 1.013143  [ 2336/ 3200]\n",
            "loss: 0.931161  [ 2352/ 3200]\n",
            "loss: 0.898003  [ 2368/ 3200]\n",
            "loss: 1.131084  [ 2384/ 3200]\n",
            "loss: 1.115787  [ 2400/ 3200]\n",
            "loss: 0.997550  [ 2416/ 3200]\n",
            "loss: 1.105467  [ 2432/ 3200]\n",
            "loss: 1.095595  [ 2448/ 3200]\n",
            "loss: 1.037450  [ 2464/ 3200]\n",
            "loss: 0.797401  [ 2480/ 3200]\n",
            "loss: 1.180065  [ 2496/ 3200]\n",
            "loss: 1.214956  [ 2512/ 3200]\n",
            "loss: 1.071737  [ 2528/ 3200]\n",
            "loss: 1.091867  [ 2544/ 3200]\n",
            "loss: 0.998574  [ 2560/ 3200]\n",
            "loss: 1.106353  [ 2576/ 3200]\n",
            "loss: 1.228154  [ 2592/ 3200]\n",
            "loss: 1.094732  [ 2608/ 3200]\n",
            "loss: 1.104773  [ 2624/ 3200]\n",
            "loss: 1.044489  [ 2640/ 3200]\n",
            "loss: 0.904228  [ 2656/ 3200]\n",
            "loss: 1.186541  [ 2672/ 3200]\n",
            "loss: 1.107028  [ 2688/ 3200]\n",
            "loss: 1.103293  [ 2704/ 3200]\n",
            "loss: 1.095408  [ 2720/ 3200]\n",
            "loss: 1.199787  [ 2736/ 3200]\n",
            "loss: 0.970489  [ 2752/ 3200]\n",
            "loss: 1.006910  [ 2768/ 3200]\n",
            "loss: 1.102477  [ 2784/ 3200]\n",
            "loss: 1.210290  [ 2800/ 3200]\n",
            "loss: 1.118608  [ 2816/ 3200]\n",
            "loss: 1.164967  [ 2832/ 3200]\n",
            "loss: 1.180383  [ 2848/ 3200]\n",
            "loss: 1.004064  [ 2864/ 3200]\n",
            "loss: 1.060430  [ 2880/ 3200]\n",
            "loss: 1.161075  [ 2896/ 3200]\n",
            "loss: 1.082726  [ 2912/ 3200]\n",
            "loss: 1.142954  [ 2928/ 3200]\n",
            "loss: 1.258161  [ 2944/ 3200]\n",
            "loss: 1.114881  [ 2960/ 3200]\n",
            "loss: 1.144821  [ 2976/ 3200]\n",
            "loss: 1.048401  [ 2992/ 3200]\n",
            "loss: 1.093485  [ 3008/ 3200]\n",
            "loss: 1.085364  [ 3024/ 3200]\n",
            "loss: 1.134539  [ 3040/ 3200]\n",
            "loss: 1.006305  [ 3056/ 3200]\n",
            "loss: 0.865917  [ 3072/ 3200]\n",
            "loss: 0.908141  [ 3088/ 3200]\n",
            "loss: 1.145204  [ 3104/ 3200]\n",
            "loss: 1.117577  [ 3120/ 3200]\n",
            "loss: 1.262055  [ 3136/ 3200]\n",
            "loss: 1.105914  [ 3152/ 3200]\n",
            "loss: 1.219550  [ 3168/ 3200]\n",
            "loss: 0.953946  [ 3184/ 3200]\n",
            "Avg Accuracy: 42.125000%, Avg loss: 1.152013\n",
            "F1 score is: 0.3270977797359228\n",
            "Confusion Matrix:\n",
            "[[106   9  82   3]\n",
            " [ 21  12 165   2]\n",
            " [  1   1 198   0]\n",
            " [  1   7 171  21]]\n",
            "current epoch: 12\n",
            "\n",
            "loss: 1.041379  [    0/ 3200]\n",
            "loss: 1.250095  [   16/ 3200]\n",
            "loss: 0.997362  [   32/ 3200]\n",
            "loss: 0.940541  [   48/ 3200]\n",
            "loss: 1.049192  [   64/ 3200]\n",
            "loss: 1.038389  [   80/ 3200]\n",
            "loss: 1.182417  [   96/ 3200]\n",
            "loss: 1.159190  [  112/ 3200]\n",
            "loss: 1.141637  [  128/ 3200]\n",
            "loss: 0.993662  [  144/ 3200]\n",
            "loss: 1.074151  [  160/ 3200]\n",
            "loss: 1.055703  [  176/ 3200]\n",
            "loss: 0.937391  [  192/ 3200]\n",
            "loss: 0.891163  [  208/ 3200]\n",
            "loss: 0.992439  [  224/ 3200]\n",
            "loss: 1.083369  [  240/ 3200]\n",
            "loss: 0.934169  [  256/ 3200]\n",
            "loss: 1.194880  [  272/ 3200]\n",
            "loss: 1.294717  [  288/ 3200]\n",
            "loss: 1.035816  [  304/ 3200]\n",
            "loss: 1.274095  [  320/ 3200]\n",
            "loss: 1.185357  [  336/ 3200]\n",
            "loss: 0.838469  [  352/ 3200]\n",
            "loss: 1.108297  [  368/ 3200]\n",
            "loss: 1.145515  [  384/ 3200]\n",
            "loss: 1.011460  [  400/ 3200]\n",
            "loss: 1.142110  [  416/ 3200]\n",
            "loss: 1.165939  [  432/ 3200]\n",
            "loss: 1.233616  [  448/ 3200]\n",
            "loss: 1.367113  [  464/ 3200]\n",
            "loss: 0.979776  [  480/ 3200]\n",
            "loss: 0.893692  [  496/ 3200]\n",
            "loss: 1.118578  [  512/ 3200]\n",
            "loss: 1.040038  [  528/ 3200]\n",
            "loss: 1.247802  [  544/ 3200]\n",
            "loss: 0.996361  [  560/ 3200]\n",
            "loss: 1.072787  [  576/ 3200]\n",
            "loss: 1.262469  [  592/ 3200]\n",
            "loss: 0.869357  [  608/ 3200]\n",
            "loss: 1.009505  [  624/ 3200]\n",
            "loss: 1.130280  [  640/ 3200]\n",
            "loss: 1.072211  [  656/ 3200]\n",
            "loss: 1.089154  [  672/ 3200]\n",
            "loss: 1.177124  [  688/ 3200]\n",
            "loss: 1.064739  [  704/ 3200]\n",
            "loss: 1.000131  [  720/ 3200]\n",
            "loss: 1.093525  [  736/ 3200]\n",
            "loss: 1.223386  [  752/ 3200]\n",
            "loss: 1.058322  [  768/ 3200]\n",
            "loss: 0.928859  [  784/ 3200]\n",
            "loss: 1.051113  [  800/ 3200]\n",
            "loss: 1.161347  [  816/ 3200]\n",
            "loss: 1.166325  [  832/ 3200]\n",
            "loss: 1.024217  [  848/ 3200]\n",
            "loss: 1.000074  [  864/ 3200]\n",
            "loss: 0.907293  [  880/ 3200]\n",
            "loss: 1.077294  [  896/ 3200]\n",
            "loss: 1.098786  [  912/ 3200]\n",
            "loss: 1.014091  [  928/ 3200]\n",
            "loss: 0.870727  [  944/ 3200]\n",
            "loss: 1.351279  [  960/ 3200]\n",
            "loss: 1.196522  [  976/ 3200]\n",
            "loss: 0.997437  [  992/ 3200]\n",
            "loss: 1.059205  [ 1008/ 3200]\n",
            "loss: 1.082489  [ 1024/ 3200]\n",
            "loss: 1.063869  [ 1040/ 3200]\n",
            "loss: 1.120002  [ 1056/ 3200]\n",
            "loss: 1.093313  [ 1072/ 3200]\n",
            "loss: 1.271334  [ 1088/ 3200]\n",
            "loss: 0.834049  [ 1104/ 3200]\n",
            "loss: 1.282834  [ 1120/ 3200]\n",
            "loss: 1.009256  [ 1136/ 3200]\n",
            "loss: 0.960266  [ 1152/ 3200]\n",
            "loss: 0.959918  [ 1168/ 3200]\n",
            "loss: 1.210336  [ 1184/ 3200]\n",
            "loss: 1.010990  [ 1200/ 3200]\n",
            "loss: 1.017651  [ 1216/ 3200]\n",
            "loss: 1.024553  [ 1232/ 3200]\n",
            "loss: 1.091164  [ 1248/ 3200]\n",
            "loss: 1.179371  [ 1264/ 3200]\n",
            "loss: 1.080947  [ 1280/ 3200]\n",
            "loss: 1.106439  [ 1296/ 3200]\n",
            "loss: 0.928727  [ 1312/ 3200]\n",
            "loss: 1.333521  [ 1328/ 3200]\n",
            "loss: 0.844890  [ 1344/ 3200]\n",
            "loss: 1.036364  [ 1360/ 3200]\n",
            "loss: 1.154622  [ 1376/ 3200]\n",
            "loss: 0.987433  [ 1392/ 3200]\n",
            "loss: 1.129142  [ 1408/ 3200]\n",
            "loss: 0.957197  [ 1424/ 3200]\n",
            "loss: 1.151531  [ 1440/ 3200]\n",
            "loss: 1.032743  [ 1456/ 3200]\n",
            "loss: 1.072655  [ 1472/ 3200]\n",
            "loss: 1.350307  [ 1488/ 3200]\n",
            "loss: 1.118139  [ 1504/ 3200]\n",
            "loss: 1.148654  [ 1520/ 3200]\n",
            "loss: 0.950279  [ 1536/ 3200]\n",
            "loss: 0.964270  [ 1552/ 3200]\n",
            "loss: 1.058715  [ 1568/ 3200]\n",
            "loss: 1.137275  [ 1584/ 3200]\n",
            "loss: 0.957282  [ 1600/ 3200]\n",
            "loss: 1.128919  [ 1616/ 3200]\n",
            "loss: 1.055718  [ 1632/ 3200]\n",
            "loss: 1.133430  [ 1648/ 3200]\n",
            "loss: 1.088696  [ 1664/ 3200]\n",
            "loss: 1.173855  [ 1680/ 3200]\n",
            "loss: 1.049150  [ 1696/ 3200]\n",
            "loss: 1.172439  [ 1712/ 3200]\n",
            "loss: 1.003800  [ 1728/ 3200]\n",
            "loss: 1.027088  [ 1744/ 3200]\n",
            "loss: 1.259576  [ 1760/ 3200]\n",
            "loss: 1.204346  [ 1776/ 3200]\n",
            "loss: 1.023702  [ 1792/ 3200]\n",
            "loss: 1.226766  [ 1808/ 3200]\n",
            "loss: 1.339826  [ 1824/ 3200]\n",
            "loss: 0.940964  [ 1840/ 3200]\n",
            "loss: 1.029740  [ 1856/ 3200]\n",
            "loss: 0.966538  [ 1872/ 3200]\n",
            "loss: 0.927554  [ 1888/ 3200]\n",
            "loss: 1.398282  [ 1904/ 3200]\n",
            "loss: 1.112499  [ 1920/ 3200]\n",
            "loss: 0.996773  [ 1936/ 3200]\n",
            "loss: 0.904790  [ 1952/ 3200]\n",
            "loss: 1.061983  [ 1968/ 3200]\n",
            "loss: 1.030456  [ 1984/ 3200]\n",
            "loss: 1.064496  [ 2000/ 3200]\n",
            "loss: 1.008078  [ 2016/ 3200]\n",
            "loss: 1.367595  [ 2032/ 3200]\n",
            "loss: 1.084380  [ 2048/ 3200]\n",
            "loss: 1.006206  [ 2064/ 3200]\n",
            "loss: 1.149737  [ 2080/ 3200]\n",
            "loss: 1.176078  [ 2096/ 3200]\n",
            "loss: 1.022719  [ 2112/ 3200]\n",
            "loss: 0.932561  [ 2128/ 3200]\n",
            "loss: 1.047663  [ 2144/ 3200]\n",
            "loss: 1.117288  [ 2160/ 3200]\n",
            "loss: 1.163288  [ 2176/ 3200]\n",
            "loss: 0.972517  [ 2192/ 3200]\n",
            "loss: 1.121882  [ 2208/ 3200]\n",
            "loss: 1.092622  [ 2224/ 3200]\n",
            "loss: 0.999013  [ 2240/ 3200]\n",
            "loss: 1.068715  [ 2256/ 3200]\n",
            "loss: 1.021233  [ 2272/ 3200]\n",
            "loss: 0.985735  [ 2288/ 3200]\n",
            "loss: 1.471940  [ 2304/ 3200]\n",
            "loss: 0.982458  [ 2320/ 3200]\n",
            "loss: 1.082185  [ 2336/ 3200]\n",
            "loss: 0.903370  [ 2352/ 3200]\n",
            "loss: 1.107043  [ 2368/ 3200]\n",
            "loss: 1.055475  [ 2384/ 3200]\n",
            "loss: 1.245706  [ 2400/ 3200]\n",
            "loss: 1.009824  [ 2416/ 3200]\n",
            "loss: 1.024863  [ 2432/ 3200]\n",
            "loss: 0.996451  [ 2448/ 3200]\n",
            "loss: 1.176683  [ 2464/ 3200]\n",
            "loss: 1.012408  [ 2480/ 3200]\n",
            "loss: 1.061070  [ 2496/ 3200]\n",
            "loss: 0.989857  [ 2512/ 3200]\n",
            "loss: 1.221796  [ 2528/ 3200]\n",
            "loss: 1.107028  [ 2544/ 3200]\n",
            "loss: 1.218985  [ 2560/ 3200]\n",
            "loss: 1.044770  [ 2576/ 3200]\n",
            "loss: 1.140094  [ 2592/ 3200]\n",
            "loss: 1.164998  [ 2608/ 3200]\n",
            "loss: 1.072715  [ 2624/ 3200]\n",
            "loss: 1.097338  [ 2640/ 3200]\n",
            "loss: 1.067295  [ 2656/ 3200]\n",
            "loss: 1.298517  [ 2672/ 3200]\n",
            "loss: 1.042462  [ 2688/ 3200]\n",
            "loss: 1.180701  [ 2704/ 3200]\n",
            "loss: 1.166211  [ 2720/ 3200]\n",
            "loss: 0.993585  [ 2736/ 3200]\n",
            "loss: 0.795909  [ 2752/ 3200]\n",
            "loss: 1.042341  [ 2768/ 3200]\n",
            "loss: 1.134051  [ 2784/ 3200]\n",
            "loss: 1.053736  [ 2800/ 3200]\n",
            "loss: 1.248492  [ 2816/ 3200]\n",
            "loss: 0.996651  [ 2832/ 3200]\n",
            "loss: 1.135051  [ 2848/ 3200]\n",
            "loss: 0.943546  [ 2864/ 3200]\n",
            "loss: 0.937602  [ 2880/ 3200]\n",
            "loss: 0.919132  [ 2896/ 3200]\n",
            "loss: 1.208964  [ 2912/ 3200]\n",
            "loss: 1.134236  [ 2928/ 3200]\n",
            "loss: 1.186296  [ 2944/ 3200]\n",
            "loss: 1.110043  [ 2960/ 3200]\n",
            "loss: 1.288002  [ 2976/ 3200]\n",
            "loss: 0.957092  [ 2992/ 3200]\n",
            "loss: 1.028328  [ 3008/ 3200]\n",
            "loss: 0.992112  [ 3024/ 3200]\n",
            "loss: 0.731707  [ 3040/ 3200]\n",
            "loss: 1.272165  [ 3056/ 3200]\n",
            "loss: 0.859871  [ 3072/ 3200]\n",
            "loss: 0.943818  [ 3088/ 3200]\n",
            "loss: 1.110403  [ 3104/ 3200]\n",
            "loss: 1.175959  [ 3120/ 3200]\n",
            "loss: 1.247602  [ 3136/ 3200]\n",
            "loss: 1.217499  [ 3152/ 3200]\n",
            "loss: 1.092887  [ 3168/ 3200]\n",
            "loss: 1.094530  [ 3184/ 3200]\n",
            "Avg Accuracy: 52.625000%, Avg loss: 1.103150\n",
            "F1 score is: 0.4505733919143677\n",
            "Confusion Matrix:\n",
            "[[101  35  58   6]\n",
            " [ 18  16 120  46]\n",
            " [  1   2 192   5]\n",
            " [  0  10  78 112]]\n",
            "current epoch: 13\n",
            "\n",
            "loss: 1.080917  [    0/ 3200]\n",
            "loss: 0.907279  [   16/ 3200]\n",
            "loss: 0.912080  [   32/ 3200]\n",
            "loss: 0.863375  [   48/ 3200]\n",
            "loss: 1.116142  [   64/ 3200]\n",
            "loss: 1.054077  [   80/ 3200]\n",
            "loss: 0.982364  [   96/ 3200]\n",
            "loss: 0.885892  [  112/ 3200]\n",
            "loss: 1.114591  [  128/ 3200]\n",
            "loss: 1.031522  [  144/ 3200]\n",
            "loss: 0.962932  [  160/ 3200]\n",
            "loss: 1.150057  [  176/ 3200]\n",
            "loss: 0.979690  [  192/ 3200]\n",
            "loss: 1.220778  [  208/ 3200]\n",
            "loss: 0.953672  [  224/ 3200]\n",
            "loss: 1.077678  [  240/ 3200]\n",
            "loss: 1.319469  [  256/ 3200]\n",
            "loss: 0.866117  [  272/ 3200]\n",
            "loss: 1.133249  [  288/ 3200]\n",
            "loss: 1.246151  [  304/ 3200]\n",
            "loss: 0.998546  [  320/ 3200]\n",
            "loss: 0.949807  [  336/ 3200]\n",
            "loss: 1.141838  [  352/ 3200]\n",
            "loss: 1.037257  [  368/ 3200]\n",
            "loss: 1.171296  [  384/ 3200]\n",
            "loss: 1.154116  [  400/ 3200]\n",
            "loss: 1.318932  [  416/ 3200]\n",
            "loss: 1.641453  [  432/ 3200]\n",
            "loss: 1.064933  [  448/ 3200]\n",
            "loss: 1.162428  [  464/ 3200]\n",
            "loss: 0.878184  [  480/ 3200]\n",
            "loss: 1.045720  [  496/ 3200]\n",
            "loss: 1.127591  [  512/ 3200]\n",
            "loss: 1.050390  [  528/ 3200]\n",
            "loss: 1.048926  [  544/ 3200]\n",
            "loss: 1.115021  [  560/ 3200]\n",
            "loss: 1.023200  [  576/ 3200]\n",
            "loss: 1.001484  [  592/ 3200]\n",
            "loss: 0.775191  [  608/ 3200]\n",
            "loss: 1.025528  [  624/ 3200]\n",
            "loss: 0.915920  [  640/ 3200]\n",
            "loss: 1.153256  [  656/ 3200]\n",
            "loss: 0.950901  [  672/ 3200]\n",
            "loss: 0.887505  [  688/ 3200]\n",
            "loss: 1.271291  [  704/ 3200]\n",
            "loss: 1.042762  [  720/ 3200]\n",
            "loss: 0.942212  [  736/ 3200]\n",
            "loss: 0.975780  [  752/ 3200]\n",
            "loss: 1.314766  [  768/ 3200]\n",
            "loss: 1.145673  [  784/ 3200]\n",
            "loss: 1.079817  [  800/ 3200]\n",
            "loss: 1.029815  [  816/ 3200]\n",
            "loss: 0.921352  [  832/ 3200]\n",
            "loss: 0.903358  [  848/ 3200]\n",
            "loss: 0.924734  [  864/ 3200]\n",
            "loss: 0.969864  [  880/ 3200]\n",
            "loss: 1.118894  [  896/ 3200]\n",
            "loss: 0.898382  [  912/ 3200]\n",
            "loss: 0.919419  [  928/ 3200]\n",
            "loss: 0.773528  [  944/ 3200]\n",
            "loss: 1.029495  [  960/ 3200]\n",
            "loss: 1.085316  [  976/ 3200]\n",
            "loss: 0.949059  [  992/ 3200]\n",
            "loss: 1.142423  [ 1008/ 3200]\n",
            "loss: 1.080284  [ 1024/ 3200]\n",
            "loss: 1.000483  [ 1040/ 3200]\n",
            "loss: 0.993835  [ 1056/ 3200]\n",
            "loss: 1.261865  [ 1072/ 3200]\n",
            "loss: 1.189676  [ 1088/ 3200]\n",
            "loss: 1.045250  [ 1104/ 3200]\n",
            "loss: 1.134148  [ 1120/ 3200]\n",
            "loss: 1.000673  [ 1136/ 3200]\n",
            "loss: 1.156841  [ 1152/ 3200]\n",
            "loss: 1.201993  [ 1168/ 3200]\n",
            "loss: 1.187032  [ 1184/ 3200]\n",
            "loss: 1.035860  [ 1200/ 3200]\n",
            "loss: 1.173121  [ 1216/ 3200]\n",
            "loss: 0.999682  [ 1232/ 3200]\n",
            "loss: 1.003595  [ 1248/ 3200]\n",
            "loss: 0.976047  [ 1264/ 3200]\n",
            "loss: 1.110301  [ 1280/ 3200]\n",
            "loss: 1.168823  [ 1296/ 3200]\n",
            "loss: 0.805395  [ 1312/ 3200]\n",
            "loss: 1.221108  [ 1328/ 3200]\n",
            "loss: 1.136569  [ 1344/ 3200]\n",
            "loss: 1.212440  [ 1360/ 3200]\n",
            "loss: 1.061164  [ 1376/ 3200]\n",
            "loss: 1.061767  [ 1392/ 3200]\n",
            "loss: 1.147032  [ 1408/ 3200]\n",
            "loss: 1.049464  [ 1424/ 3200]\n",
            "loss: 0.873312  [ 1440/ 3200]\n",
            "loss: 1.331852  [ 1456/ 3200]\n",
            "loss: 1.100482  [ 1472/ 3200]\n",
            "loss: 1.125126  [ 1488/ 3200]\n",
            "loss: 1.203011  [ 1504/ 3200]\n",
            "loss: 1.100472  [ 1520/ 3200]\n",
            "loss: 0.960089  [ 1536/ 3200]\n",
            "loss: 1.213727  [ 1552/ 3200]\n",
            "loss: 0.877031  [ 1568/ 3200]\n",
            "loss: 1.145076  [ 1584/ 3200]\n",
            "loss: 0.908462  [ 1600/ 3200]\n",
            "loss: 0.951207  [ 1616/ 3200]\n",
            "loss: 1.021240  [ 1632/ 3200]\n",
            "loss: 0.810947  [ 1648/ 3200]\n",
            "loss: 0.921498  [ 1664/ 3200]\n",
            "loss: 0.864689  [ 1680/ 3200]\n",
            "loss: 1.151328  [ 1696/ 3200]\n",
            "loss: 1.018348  [ 1712/ 3200]\n",
            "loss: 1.106346  [ 1728/ 3200]\n",
            "loss: 1.198929  [ 1744/ 3200]\n",
            "loss: 0.939662  [ 1760/ 3200]\n",
            "loss: 1.153447  [ 1776/ 3200]\n",
            "loss: 0.941085  [ 1792/ 3200]\n",
            "loss: 0.882828  [ 1808/ 3200]\n",
            "loss: 1.029539  [ 1824/ 3200]\n",
            "loss: 0.959322  [ 1840/ 3200]\n",
            "loss: 1.162420  [ 1856/ 3200]\n",
            "loss: 1.085410  [ 1872/ 3200]\n",
            "loss: 1.105064  [ 1888/ 3200]\n",
            "loss: 1.024257  [ 1904/ 3200]\n",
            "loss: 0.986820  [ 1920/ 3200]\n",
            "loss: 1.072576  [ 1936/ 3200]\n",
            "loss: 1.026041  [ 1952/ 3200]\n",
            "loss: 0.986098  [ 1968/ 3200]\n",
            "loss: 1.105375  [ 1984/ 3200]\n",
            "loss: 0.846182  [ 2000/ 3200]\n",
            "loss: 0.915121  [ 2016/ 3200]\n",
            "loss: 1.345221  [ 2032/ 3200]\n",
            "loss: 1.004101  [ 2048/ 3200]\n",
            "loss: 1.143994  [ 2064/ 3200]\n",
            "loss: 1.113119  [ 2080/ 3200]\n",
            "loss: 1.162405  [ 2096/ 3200]\n",
            "loss: 1.067324  [ 2112/ 3200]\n",
            "loss: 0.993206  [ 2128/ 3200]\n",
            "loss: 1.025428  [ 2144/ 3200]\n",
            "loss: 1.234211  [ 2160/ 3200]\n",
            "loss: 1.276767  [ 2176/ 3200]\n",
            "loss: 1.155080  [ 2192/ 3200]\n",
            "loss: 0.962781  [ 2208/ 3200]\n",
            "loss: 1.057891  [ 2224/ 3200]\n",
            "loss: 1.078631  [ 2240/ 3200]\n",
            "loss: 1.009888  [ 2256/ 3200]\n",
            "loss: 0.881767  [ 2272/ 3200]\n",
            "loss: 0.856720  [ 2288/ 3200]\n",
            "loss: 1.194070  [ 2304/ 3200]\n",
            "loss: 0.960469  [ 2320/ 3200]\n",
            "loss: 1.225335  [ 2336/ 3200]\n",
            "loss: 1.351212  [ 2352/ 3200]\n",
            "loss: 1.002362  [ 2368/ 3200]\n",
            "loss: 0.923646  [ 2384/ 3200]\n",
            "loss: 1.066396  [ 2400/ 3200]\n",
            "loss: 1.075331  [ 2416/ 3200]\n",
            "loss: 1.070757  [ 2432/ 3200]\n",
            "loss: 0.965914  [ 2448/ 3200]\n",
            "loss: 1.103407  [ 2464/ 3200]\n",
            "loss: 1.118553  [ 2480/ 3200]\n",
            "loss: 0.851990  [ 2496/ 3200]\n",
            "loss: 1.267263  [ 2512/ 3200]\n",
            "loss: 1.068458  [ 2528/ 3200]\n",
            "loss: 1.070102  [ 2544/ 3200]\n",
            "loss: 0.992256  [ 2560/ 3200]\n",
            "loss: 0.987580  [ 2576/ 3200]\n",
            "loss: 1.115005  [ 2592/ 3200]\n",
            "loss: 0.905965  [ 2608/ 3200]\n",
            "loss: 1.093281  [ 2624/ 3200]\n",
            "loss: 1.005939  [ 2640/ 3200]\n",
            "loss: 1.041349  [ 2656/ 3200]\n",
            "loss: 1.028839  [ 2672/ 3200]\n",
            "loss: 0.905644  [ 2688/ 3200]\n",
            "loss: 1.153982  [ 2704/ 3200]\n",
            "loss: 0.974597  [ 2720/ 3200]\n",
            "loss: 1.197912  [ 2736/ 3200]\n",
            "loss: 1.195928  [ 2752/ 3200]\n",
            "loss: 1.043078  [ 2768/ 3200]\n",
            "loss: 1.177180  [ 2784/ 3200]\n",
            "loss: 1.078868  [ 2800/ 3200]\n",
            "loss: 1.225758  [ 2816/ 3200]\n",
            "loss: 1.094823  [ 2832/ 3200]\n",
            "loss: 1.137063  [ 2848/ 3200]\n",
            "loss: 1.035856  [ 2864/ 3200]\n",
            "loss: 0.912430  [ 2880/ 3200]\n",
            "loss: 1.207921  [ 2896/ 3200]\n",
            "loss: 1.063158  [ 2912/ 3200]\n",
            "loss: 1.119556  [ 2928/ 3200]\n",
            "loss: 1.202962  [ 2944/ 3200]\n",
            "loss: 0.851046  [ 2960/ 3200]\n",
            "loss: 1.092355  [ 2976/ 3200]\n",
            "loss: 1.099632  [ 2992/ 3200]\n",
            "loss: 1.329564  [ 3008/ 3200]\n",
            "loss: 1.182305  [ 3024/ 3200]\n",
            "loss: 1.091794  [ 3040/ 3200]\n",
            "loss: 1.377580  [ 3056/ 3200]\n",
            "loss: 1.057496  [ 3072/ 3200]\n",
            "loss: 1.057467  [ 3088/ 3200]\n",
            "loss: 1.014787  [ 3104/ 3200]\n",
            "loss: 1.121749  [ 3120/ 3200]\n",
            "loss: 0.990799  [ 3136/ 3200]\n",
            "loss: 1.087257  [ 3152/ 3200]\n",
            "loss: 0.992180  [ 3168/ 3200]\n",
            "loss: 0.953251  [ 3184/ 3200]\n",
            "Avg Accuracy: 63.875000%, Avg loss: 1.031552\n",
            "F1 score is: 0.5990788596868515\n",
            "Confusion Matrix:\n",
            "[[172  24   1   3]\n",
            " [ 51 104  31  14]\n",
            " [ 16  54 126   4]\n",
            " [  9  58  24 109]]\n",
            "current epoch: 14\n",
            "\n",
            "loss: 1.078940  [    0/ 3200]\n",
            "loss: 1.245088  [   16/ 3200]\n",
            "loss: 0.834328  [   32/ 3200]\n",
            "loss: 1.201145  [   48/ 3200]\n",
            "loss: 1.018020  [   64/ 3200]\n",
            "loss: 1.025477  [   80/ 3200]\n",
            "loss: 1.049534  [   96/ 3200]\n",
            "loss: 1.071671  [  112/ 3200]\n",
            "loss: 1.123655  [  128/ 3200]\n",
            "loss: 0.857989  [  144/ 3200]\n",
            "loss: 0.837417  [  160/ 3200]\n",
            "loss: 0.840402  [  176/ 3200]\n",
            "loss: 1.015450  [  192/ 3200]\n",
            "loss: 0.928852  [  208/ 3200]\n",
            "loss: 0.931866  [  224/ 3200]\n",
            "loss: 1.143000  [  240/ 3200]\n",
            "loss: 0.988244  [  256/ 3200]\n",
            "loss: 1.041024  [  272/ 3200]\n",
            "loss: 1.039087  [  288/ 3200]\n",
            "loss: 1.088567  [  304/ 3200]\n",
            "loss: 1.090884  [  320/ 3200]\n",
            "loss: 0.972491  [  336/ 3200]\n",
            "loss: 1.173904  [  352/ 3200]\n",
            "loss: 1.143289  [  368/ 3200]\n",
            "loss: 1.182454  [  384/ 3200]\n",
            "loss: 1.019825  [  400/ 3200]\n",
            "loss: 0.902286  [  416/ 3200]\n",
            "loss: 1.275915  [  432/ 3200]\n",
            "loss: 1.234420  [  448/ 3200]\n",
            "loss: 1.043831  [  464/ 3200]\n",
            "loss: 1.168960  [  480/ 3200]\n",
            "loss: 1.069059  [  496/ 3200]\n",
            "loss: 1.080410  [  512/ 3200]\n",
            "loss: 1.161791  [  528/ 3200]\n",
            "loss: 1.077165  [  544/ 3200]\n",
            "loss: 1.328382  [  560/ 3200]\n",
            "loss: 1.053746  [  576/ 3200]\n",
            "loss: 1.067869  [  592/ 3200]\n",
            "loss: 0.838839  [  608/ 3200]\n",
            "loss: 1.058635  [  624/ 3200]\n",
            "loss: 1.019707  [  640/ 3200]\n",
            "loss: 0.903899  [  656/ 3200]\n",
            "loss: 0.912764  [  672/ 3200]\n",
            "loss: 1.039107  [  688/ 3200]\n",
            "loss: 1.182995  [  704/ 3200]\n",
            "loss: 1.184258  [  720/ 3200]\n",
            "loss: 0.933693  [  736/ 3200]\n",
            "loss: 0.922755  [  752/ 3200]\n",
            "loss: 0.919720  [  768/ 3200]\n",
            "loss: 1.409498  [  784/ 3200]\n",
            "loss: 0.898862  [  800/ 3200]\n",
            "loss: 1.181322  [  816/ 3200]\n",
            "loss: 1.136822  [  832/ 3200]\n",
            "loss: 0.920524  [  848/ 3200]\n",
            "loss: 1.013287  [  864/ 3200]\n",
            "loss: 1.150851  [  880/ 3200]\n",
            "loss: 1.083439  [  896/ 3200]\n",
            "loss: 0.815627  [  912/ 3200]\n",
            "loss: 0.822462  [  928/ 3200]\n",
            "loss: 1.205806  [  944/ 3200]\n",
            "loss: 0.953372  [  960/ 3200]\n",
            "loss: 1.061388  [  976/ 3200]\n",
            "loss: 0.964054  [  992/ 3200]\n",
            "loss: 1.183703  [ 1008/ 3200]\n",
            "loss: 0.857869  [ 1024/ 3200]\n",
            "loss: 1.040734  [ 1040/ 3200]\n",
            "loss: 1.104593  [ 1056/ 3200]\n",
            "loss: 1.028775  [ 1072/ 3200]\n",
            "loss: 0.899363  [ 1088/ 3200]\n",
            "loss: 1.194109  [ 1104/ 3200]\n",
            "loss: 1.046656  [ 1120/ 3200]\n",
            "loss: 0.976506  [ 1136/ 3200]\n",
            "loss: 1.010846  [ 1152/ 3200]\n",
            "loss: 1.086767  [ 1168/ 3200]\n",
            "loss: 1.367674  [ 1184/ 3200]\n",
            "loss: 1.063136  [ 1200/ 3200]\n",
            "loss: 1.224398  [ 1216/ 3200]\n",
            "loss: 1.150728  [ 1232/ 3200]\n",
            "loss: 1.121793  [ 1248/ 3200]\n",
            "loss: 0.962771  [ 1264/ 3200]\n",
            "loss: 1.037475  [ 1280/ 3200]\n",
            "loss: 1.248675  [ 1296/ 3200]\n",
            "loss: 0.853258  [ 1312/ 3200]\n",
            "loss: 0.963817  [ 1328/ 3200]\n",
            "loss: 0.976700  [ 1344/ 3200]\n",
            "loss: 0.936325  [ 1360/ 3200]\n",
            "loss: 1.023441  [ 1376/ 3200]\n",
            "loss: 1.118611  [ 1392/ 3200]\n",
            "loss: 1.023751  [ 1408/ 3200]\n",
            "loss: 1.007762  [ 1424/ 3200]\n",
            "loss: 0.999482  [ 1440/ 3200]\n",
            "loss: 0.783257  [ 1456/ 3200]\n",
            "loss: 0.875558  [ 1472/ 3200]\n",
            "loss: 1.317903  [ 1488/ 3200]\n",
            "loss: 0.961610  [ 1504/ 3200]\n",
            "loss: 1.150255  [ 1520/ 3200]\n",
            "loss: 1.010451  [ 1536/ 3200]\n",
            "loss: 1.265605  [ 1552/ 3200]\n",
            "loss: 0.977442  [ 1568/ 3200]\n",
            "loss: 0.943736  [ 1584/ 3200]\n",
            "loss: 1.044627  [ 1600/ 3200]\n",
            "loss: 1.006988  [ 1616/ 3200]\n",
            "loss: 1.023903  [ 1632/ 3200]\n",
            "loss: 0.997115  [ 1648/ 3200]\n",
            "loss: 1.168928  [ 1664/ 3200]\n",
            "loss: 1.013811  [ 1680/ 3200]\n",
            "loss: 1.152528  [ 1696/ 3200]\n",
            "loss: 1.345677  [ 1712/ 3200]\n",
            "loss: 0.836076  [ 1728/ 3200]\n",
            "loss: 0.868246  [ 1744/ 3200]\n",
            "loss: 0.967906  [ 1760/ 3200]\n",
            "loss: 0.855008  [ 1776/ 3200]\n",
            "loss: 1.267122  [ 1792/ 3200]\n",
            "loss: 1.076814  [ 1808/ 3200]\n",
            "loss: 1.141681  [ 1824/ 3200]\n",
            "loss: 0.922026  [ 1840/ 3200]\n",
            "loss: 0.883223  [ 1856/ 3200]\n",
            "loss: 1.061635  [ 1872/ 3200]\n",
            "loss: 1.024048  [ 1888/ 3200]\n",
            "loss: 1.040002  [ 1904/ 3200]\n",
            "loss: 0.827509  [ 1920/ 3200]\n",
            "loss: 1.060685  [ 1936/ 3200]\n",
            "loss: 0.988811  [ 1952/ 3200]\n",
            "loss: 1.076938  [ 1968/ 3200]\n",
            "loss: 1.042926  [ 1984/ 3200]\n",
            "loss: 0.926461  [ 2000/ 3200]\n",
            "loss: 1.034684  [ 2016/ 3200]\n",
            "loss: 0.898744  [ 2032/ 3200]\n",
            "loss: 1.102438  [ 2048/ 3200]\n",
            "loss: 0.982604  [ 2064/ 3200]\n",
            "loss: 1.083001  [ 2080/ 3200]\n",
            "loss: 1.029121  [ 2096/ 3200]\n",
            "loss: 0.900283  [ 2112/ 3200]\n",
            "loss: 1.078139  [ 2128/ 3200]\n",
            "loss: 1.088307  [ 2144/ 3200]\n",
            "loss: 1.148996  [ 2160/ 3200]\n",
            "loss: 1.158036  [ 2176/ 3200]\n",
            "loss: 1.034381  [ 2192/ 3200]\n",
            "loss: 0.982754  [ 2208/ 3200]\n",
            "loss: 1.043665  [ 2224/ 3200]\n",
            "loss: 1.171716  [ 2240/ 3200]\n",
            "loss: 1.066622  [ 2256/ 3200]\n",
            "loss: 0.974592  [ 2272/ 3200]\n",
            "loss: 1.259795  [ 2288/ 3200]\n",
            "loss: 0.933061  [ 2304/ 3200]\n",
            "loss: 0.991058  [ 2320/ 3200]\n",
            "loss: 0.905205  [ 2336/ 3200]\n",
            "loss: 1.047487  [ 2352/ 3200]\n",
            "loss: 1.226308  [ 2368/ 3200]\n",
            "loss: 1.009157  [ 2384/ 3200]\n",
            "loss: 0.934087  [ 2400/ 3200]\n",
            "loss: 1.235816  [ 2416/ 3200]\n",
            "loss: 0.827883  [ 2432/ 3200]\n",
            "loss: 1.039699  [ 2448/ 3200]\n",
            "loss: 0.952280  [ 2464/ 3200]\n",
            "loss: 1.213865  [ 2480/ 3200]\n",
            "loss: 0.943024  [ 2496/ 3200]\n",
            "loss: 1.173269  [ 2512/ 3200]\n",
            "loss: 1.088021  [ 2528/ 3200]\n",
            "loss: 1.111528  [ 2544/ 3200]\n",
            "loss: 0.833374  [ 2560/ 3200]\n",
            "loss: 1.155144  [ 2576/ 3200]\n",
            "loss: 1.152900  [ 2592/ 3200]\n",
            "loss: 1.039728  [ 2608/ 3200]\n",
            "loss: 1.124700  [ 2624/ 3200]\n",
            "loss: 1.120170  [ 2640/ 3200]\n",
            "loss: 0.876348  [ 2656/ 3200]\n",
            "loss: 1.053257  [ 2672/ 3200]\n",
            "loss: 1.036947  [ 2688/ 3200]\n",
            "loss: 0.940204  [ 2704/ 3200]\n",
            "loss: 0.899663  [ 2720/ 3200]\n",
            "loss: 0.797772  [ 2736/ 3200]\n",
            "loss: 1.270710  [ 2752/ 3200]\n",
            "loss: 0.987126  [ 2768/ 3200]\n",
            "loss: 1.037390  [ 2784/ 3200]\n",
            "loss: 0.917144  [ 2800/ 3200]\n",
            "loss: 0.943082  [ 2816/ 3200]\n",
            "loss: 1.103946  [ 2832/ 3200]\n",
            "loss: 0.993649  [ 2848/ 3200]\n",
            "loss: 0.863803  [ 2864/ 3200]\n",
            "loss: 1.161180  [ 2880/ 3200]\n",
            "loss: 0.870201  [ 2896/ 3200]\n",
            "loss: 1.109931  [ 2912/ 3200]\n",
            "loss: 1.281548  [ 2928/ 3200]\n",
            "loss: 0.941993  [ 2944/ 3200]\n",
            "loss: 1.145407  [ 2960/ 3200]\n",
            "loss: 1.101811  [ 2976/ 3200]\n",
            "loss: 1.005427  [ 2992/ 3200]\n",
            "loss: 0.974680  [ 3008/ 3200]\n",
            "loss: 0.878103  [ 3024/ 3200]\n",
            "loss: 1.068042  [ 3040/ 3200]\n",
            "loss: 0.978671  [ 3056/ 3200]\n",
            "loss: 1.211285  [ 3072/ 3200]\n",
            "loss: 1.013197  [ 3088/ 3200]\n",
            "loss: 1.015981  [ 3104/ 3200]\n",
            "loss: 1.141618  [ 3120/ 3200]\n",
            "loss: 1.079889  [ 3136/ 3200]\n",
            "loss: 0.848874  [ 3152/ 3200]\n",
            "loss: 0.941538  [ 3168/ 3200]\n",
            "loss: 1.142403  [ 3184/ 3200]\n",
            "Avg Accuracy: 65.000000%, Avg loss: 1.014661\n",
            "F1 score is: 0.5924450051784516\n",
            "Confusion Matrix:\n",
            "[[187   7   3   3]\n",
            " [ 71  58  50  21]\n",
            " [ 22  15 159   4]\n",
            " [ 20  33  31 116]]\n",
            "current epoch: 15\n",
            "\n",
            "loss: 0.960671  [    0/ 3200]\n",
            "loss: 1.066473  [   16/ 3200]\n",
            "loss: 1.174313  [   32/ 3200]\n",
            "loss: 1.023390  [   48/ 3200]\n",
            "loss: 1.172943  [   64/ 3200]\n",
            "loss: 1.052970  [   80/ 3200]\n",
            "loss: 0.885634  [   96/ 3200]\n",
            "loss: 1.137428  [  112/ 3200]\n",
            "loss: 1.076569  [  128/ 3200]\n",
            "loss: 1.151549  [  144/ 3200]\n",
            "loss: 1.235307  [  160/ 3200]\n",
            "loss: 0.987377  [  176/ 3200]\n",
            "loss: 1.147906  [  192/ 3200]\n",
            "loss: 1.005159  [  208/ 3200]\n",
            "loss: 1.006359  [  224/ 3200]\n",
            "loss: 0.869031  [  240/ 3200]\n",
            "loss: 1.032300  [  256/ 3200]\n",
            "loss: 0.919750  [  272/ 3200]\n",
            "loss: 0.860755  [  288/ 3200]\n",
            "loss: 1.161449  [  304/ 3200]\n",
            "loss: 1.169618  [  320/ 3200]\n",
            "loss: 0.972181  [  336/ 3200]\n",
            "loss: 0.874038  [  352/ 3200]\n",
            "loss: 0.963204  [  368/ 3200]\n",
            "loss: 1.002116  [  384/ 3200]\n",
            "loss: 0.886854  [  400/ 3200]\n",
            "loss: 0.817459  [  416/ 3200]\n",
            "loss: 1.125677  [  432/ 3200]\n",
            "loss: 0.873428  [  448/ 3200]\n",
            "loss: 0.971008  [  464/ 3200]\n",
            "loss: 1.230868  [  480/ 3200]\n",
            "loss: 0.984501  [  496/ 3200]\n",
            "loss: 0.742253  [  512/ 3200]\n",
            "loss: 1.136739  [  528/ 3200]\n",
            "loss: 0.936726  [  544/ 3200]\n",
            "loss: 0.919517  [  560/ 3200]\n",
            "loss: 1.115850  [  576/ 3200]\n",
            "loss: 0.959410  [  592/ 3200]\n",
            "loss: 0.931913  [  608/ 3200]\n",
            "loss: 1.037169  [  624/ 3200]\n",
            "loss: 1.029397  [  640/ 3200]\n",
            "loss: 1.004380  [  656/ 3200]\n",
            "loss: 0.966395  [  672/ 3200]\n",
            "loss: 1.091979  [  688/ 3200]\n",
            "loss: 0.984502  [  704/ 3200]\n",
            "loss: 1.089266  [  720/ 3200]\n",
            "loss: 1.056957  [  736/ 3200]\n",
            "loss: 1.210382  [  752/ 3200]\n",
            "loss: 0.997020  [  768/ 3200]\n",
            "loss: 1.269807  [  784/ 3200]\n",
            "loss: 0.936718  [  800/ 3200]\n",
            "loss: 1.033475  [  816/ 3200]\n",
            "loss: 0.807189  [  832/ 3200]\n",
            "loss: 0.997126  [  848/ 3200]\n",
            "loss: 1.123094  [  864/ 3200]\n",
            "loss: 1.075018  [  880/ 3200]\n",
            "loss: 0.926445  [  896/ 3200]\n",
            "loss: 0.952184  [  912/ 3200]\n",
            "loss: 1.107138  [  928/ 3200]\n",
            "loss: 1.199514  [  944/ 3200]\n",
            "loss: 0.993921  [  960/ 3200]\n",
            "loss: 0.849909  [  976/ 3200]\n",
            "loss: 0.875821  [  992/ 3200]\n",
            "loss: 0.987041  [ 1008/ 3200]\n",
            "loss: 1.414736  [ 1024/ 3200]\n",
            "loss: 0.968578  [ 1040/ 3200]\n",
            "loss: 0.914105  [ 1056/ 3200]\n",
            "loss: 1.298656  [ 1072/ 3200]\n",
            "loss: 1.079687  [ 1088/ 3200]\n",
            "loss: 1.021856  [ 1104/ 3200]\n",
            "loss: 1.247438  [ 1120/ 3200]\n",
            "loss: 0.983054  [ 1136/ 3200]\n",
            "loss: 1.052426  [ 1152/ 3200]\n",
            "loss: 1.039365  [ 1168/ 3200]\n",
            "loss: 1.103690  [ 1184/ 3200]\n",
            "loss: 1.006118  [ 1200/ 3200]\n",
            "loss: 1.118292  [ 1216/ 3200]\n",
            "loss: 1.373315  [ 1232/ 3200]\n",
            "loss: 1.042193  [ 1248/ 3200]\n",
            "loss: 1.103260  [ 1264/ 3200]\n",
            "loss: 0.847339  [ 1280/ 3200]\n",
            "loss: 0.822633  [ 1296/ 3200]\n",
            "loss: 0.929789  [ 1312/ 3200]\n",
            "loss: 1.030964  [ 1328/ 3200]\n",
            "loss: 0.977448  [ 1344/ 3200]\n",
            "loss: 0.979721  [ 1360/ 3200]\n",
            "loss: 1.081857  [ 1376/ 3200]\n",
            "loss: 1.090976  [ 1392/ 3200]\n",
            "loss: 1.207133  [ 1408/ 3200]\n",
            "loss: 0.949683  [ 1424/ 3200]\n",
            "loss: 1.064049  [ 1440/ 3200]\n",
            "loss: 0.936528  [ 1456/ 3200]\n",
            "loss: 0.897460  [ 1472/ 3200]\n",
            "loss: 0.942780  [ 1488/ 3200]\n",
            "loss: 1.085026  [ 1504/ 3200]\n",
            "loss: 1.359900  [ 1520/ 3200]\n",
            "loss: 0.931646  [ 1536/ 3200]\n",
            "loss: 1.062576  [ 1552/ 3200]\n",
            "loss: 0.947112  [ 1568/ 3200]\n",
            "loss: 1.357838  [ 1584/ 3200]\n",
            "loss: 1.073379  [ 1600/ 3200]\n",
            "loss: 1.208109  [ 1616/ 3200]\n",
            "loss: 1.146949  [ 1632/ 3200]\n",
            "loss: 0.787864  [ 1648/ 3200]\n",
            "loss: 0.987499  [ 1664/ 3200]\n",
            "loss: 0.913745  [ 1680/ 3200]\n",
            "loss: 0.944388  [ 1696/ 3200]\n",
            "loss: 1.142395  [ 1712/ 3200]\n",
            "loss: 0.790625  [ 1728/ 3200]\n",
            "loss: 1.136240  [ 1744/ 3200]\n",
            "loss: 1.127680  [ 1760/ 3200]\n",
            "loss: 0.949061  [ 1776/ 3200]\n",
            "loss: 0.831309  [ 1792/ 3200]\n",
            "loss: 1.076814  [ 1808/ 3200]\n",
            "loss: 1.055689  [ 1824/ 3200]\n",
            "loss: 1.010067  [ 1840/ 3200]\n",
            "loss: 1.050262  [ 1856/ 3200]\n",
            "loss: 0.648323  [ 1872/ 3200]\n",
            "loss: 1.056128  [ 1888/ 3200]\n",
            "loss: 0.994500  [ 1904/ 3200]\n",
            "loss: 1.096328  [ 1920/ 3200]\n",
            "loss: 1.243481  [ 1936/ 3200]\n",
            "loss: 1.130328  [ 1952/ 3200]\n",
            "loss: 1.182085  [ 1968/ 3200]\n",
            "loss: 1.340880  [ 1984/ 3200]\n",
            "loss: 1.196540  [ 2000/ 3200]\n",
            "loss: 1.053943  [ 2016/ 3200]\n",
            "loss: 1.057306  [ 2032/ 3200]\n",
            "loss: 1.008751  [ 2048/ 3200]\n",
            "loss: 0.947138  [ 2064/ 3200]\n",
            "loss: 0.915166  [ 2080/ 3200]\n",
            "loss: 0.963022  [ 2096/ 3200]\n",
            "loss: 0.981758  [ 2112/ 3200]\n",
            "loss: 0.947782  [ 2128/ 3200]\n",
            "loss: 1.076780  [ 2144/ 3200]\n",
            "loss: 1.150204  [ 2160/ 3200]\n",
            "loss: 0.974876  [ 2176/ 3200]\n",
            "loss: 1.165648  [ 2192/ 3200]\n",
            "loss: 0.938253  [ 2208/ 3200]\n",
            "loss: 1.095903  [ 2224/ 3200]\n",
            "loss: 1.035462  [ 2240/ 3200]\n",
            "loss: 1.088021  [ 2256/ 3200]\n",
            "loss: 1.096775  [ 2272/ 3200]\n",
            "loss: 1.194357  [ 2288/ 3200]\n",
            "loss: 0.978588  [ 2304/ 3200]\n",
            "loss: 1.071671  [ 2320/ 3200]\n",
            "loss: 1.163092  [ 2336/ 3200]\n",
            "loss: 0.986199  [ 2352/ 3200]\n",
            "loss: 1.010080  [ 2368/ 3200]\n",
            "loss: 0.920626  [ 2384/ 3200]\n",
            "loss: 0.890164  [ 2400/ 3200]\n",
            "loss: 1.216553  [ 2416/ 3200]\n",
            "loss: 0.977993  [ 2432/ 3200]\n",
            "loss: 0.914500  [ 2448/ 3200]\n",
            "loss: 0.994850  [ 2464/ 3200]\n",
            "loss: 1.030961  [ 2480/ 3200]\n",
            "loss: 0.846690  [ 2496/ 3200]\n",
            "loss: 0.816680  [ 2512/ 3200]\n",
            "loss: 0.886638  [ 2528/ 3200]\n",
            "loss: 0.925269  [ 2544/ 3200]\n",
            "loss: 1.219990  [ 2560/ 3200]\n",
            "loss: 0.833325  [ 2576/ 3200]\n",
            "loss: 1.283877  [ 2592/ 3200]\n",
            "loss: 0.949177  [ 2608/ 3200]\n",
            "loss: 0.810694  [ 2624/ 3200]\n",
            "loss: 1.077536  [ 2640/ 3200]\n",
            "loss: 0.953456  [ 2656/ 3200]\n",
            "loss: 0.938865  [ 2672/ 3200]\n",
            "loss: 1.046085  [ 2688/ 3200]\n",
            "loss: 0.967302  [ 2704/ 3200]\n",
            "loss: 1.324941  [ 2720/ 3200]\n",
            "loss: 0.973123  [ 2736/ 3200]\n",
            "loss: 0.978130  [ 2752/ 3200]\n",
            "loss: 0.930626  [ 2768/ 3200]\n",
            "loss: 0.680941  [ 2784/ 3200]\n",
            "loss: 0.840433  [ 2800/ 3200]\n",
            "loss: 0.956511  [ 2816/ 3200]\n",
            "loss: 0.954734  [ 2832/ 3200]\n",
            "loss: 0.993419  [ 2848/ 3200]\n",
            "loss: 0.757343  [ 2864/ 3200]\n",
            "loss: 1.037139  [ 2880/ 3200]\n",
            "loss: 1.341664  [ 2896/ 3200]\n",
            "loss: 1.061424  [ 2912/ 3200]\n",
            "loss: 1.272117  [ 2928/ 3200]\n",
            "loss: 1.182953  [ 2944/ 3200]\n",
            "loss: 1.030890  [ 2960/ 3200]\n",
            "loss: 1.145642  [ 2976/ 3200]\n",
            "loss: 1.011000  [ 2992/ 3200]\n",
            "loss: 1.167731  [ 3008/ 3200]\n",
            "loss: 1.081782  [ 3024/ 3200]\n",
            "loss: 1.136572  [ 3040/ 3200]\n",
            "loss: 0.778243  [ 3056/ 3200]\n",
            "loss: 1.073435  [ 3072/ 3200]\n",
            "loss: 1.331619  [ 3088/ 3200]\n",
            "loss: 0.924677  [ 3104/ 3200]\n",
            "loss: 1.133854  [ 3120/ 3200]\n",
            "loss: 0.975747  [ 3136/ 3200]\n",
            "loss: 0.933582  [ 3152/ 3200]\n",
            "loss: 0.923369  [ 3168/ 3200]\n",
            "loss: 1.263588  [ 3184/ 3200]\n",
            "Avg Accuracy: 59.125000%, Avg loss: 1.036493\n",
            "F1 score is: 0.5370228597521782\n",
            "Confusion Matrix:\n",
            "[[105  52  38   5]\n",
            " [ 18  48  82  52]\n",
            " [  2   6 187   5]\n",
            " [  0  21  46 133]]\n",
            "current epoch: 16\n",
            "\n",
            "loss: 1.020639  [    0/ 3200]\n",
            "loss: 1.269270  [   16/ 3200]\n",
            "loss: 1.062187  [   32/ 3200]\n",
            "loss: 1.148494  [   48/ 3200]\n",
            "loss: 1.139043  [   64/ 3200]\n",
            "loss: 1.118096  [   80/ 3200]\n",
            "loss: 1.088850  [   96/ 3200]\n",
            "loss: 1.135544  [  112/ 3200]\n",
            "loss: 0.959369  [  128/ 3200]\n",
            "loss: 0.794979  [  144/ 3200]\n",
            "loss: 0.979325  [  160/ 3200]\n",
            "loss: 1.003380  [  176/ 3200]\n",
            "loss: 0.957132  [  192/ 3200]\n",
            "loss: 1.110233  [  208/ 3200]\n",
            "loss: 0.842792  [  224/ 3200]\n",
            "loss: 1.243117  [  240/ 3200]\n",
            "loss: 0.946934  [  256/ 3200]\n",
            "loss: 1.048654  [  272/ 3200]\n",
            "loss: 1.008047  [  288/ 3200]\n",
            "loss: 0.658539  [  304/ 3200]\n",
            "loss: 1.011614  [  320/ 3200]\n",
            "loss: 1.143155  [  336/ 3200]\n",
            "loss: 1.174065  [  352/ 3200]\n",
            "loss: 1.104193  [  368/ 3200]\n",
            "loss: 0.907583  [  384/ 3200]\n",
            "loss: 1.257906  [  400/ 3200]\n",
            "loss: 1.040509  [  416/ 3200]\n",
            "loss: 1.037697  [  432/ 3200]\n",
            "loss: 1.052021  [  448/ 3200]\n",
            "loss: 1.041015  [  464/ 3200]\n",
            "loss: 1.206708  [  480/ 3200]\n",
            "loss: 1.030150  [  496/ 3200]\n",
            "loss: 0.859009  [  512/ 3200]\n",
            "loss: 1.007499  [  528/ 3200]\n",
            "loss: 1.047383  [  544/ 3200]\n",
            "loss: 1.015843  [  560/ 3200]\n",
            "loss: 0.947689  [  576/ 3200]\n",
            "loss: 0.992356  [  592/ 3200]\n",
            "loss: 0.964139  [  608/ 3200]\n",
            "loss: 0.975829  [  624/ 3200]\n",
            "loss: 0.946534  [  640/ 3200]\n",
            "loss: 1.099079  [  656/ 3200]\n",
            "loss: 0.975483  [  672/ 3200]\n",
            "loss: 0.977429  [  688/ 3200]\n",
            "loss: 1.103284  [  704/ 3200]\n",
            "loss: 0.827734  [  720/ 3200]\n",
            "loss: 0.884763  [  736/ 3200]\n",
            "loss: 1.075689  [  752/ 3200]\n",
            "loss: 0.933868  [  768/ 3200]\n",
            "loss: 1.110887  [  784/ 3200]\n",
            "loss: 1.142227  [  800/ 3200]\n",
            "loss: 0.804109  [  816/ 3200]\n",
            "loss: 1.261147  [  832/ 3200]\n",
            "loss: 1.030925  [  848/ 3200]\n",
            "loss: 0.941356  [  864/ 3200]\n",
            "loss: 0.993292  [  880/ 3200]\n",
            "loss: 1.183982  [  896/ 3200]\n",
            "loss: 1.237652  [  912/ 3200]\n",
            "loss: 0.914136  [  928/ 3200]\n",
            "loss: 1.241264  [  944/ 3200]\n",
            "loss: 1.018207  [  960/ 3200]\n",
            "loss: 0.891253  [  976/ 3200]\n",
            "loss: 1.233476  [  992/ 3200]\n",
            "loss: 0.988304  [ 1008/ 3200]\n",
            "loss: 0.953858  [ 1024/ 3200]\n",
            "loss: 0.968812  [ 1040/ 3200]\n",
            "loss: 1.313302  [ 1056/ 3200]\n",
            "loss: 0.943491  [ 1072/ 3200]\n",
            "loss: 0.918755  [ 1088/ 3200]\n",
            "loss: 1.241637  [ 1104/ 3200]\n",
            "loss: 1.007675  [ 1120/ 3200]\n",
            "loss: 0.966009  [ 1136/ 3200]\n",
            "loss: 1.080957  [ 1152/ 3200]\n",
            "loss: 0.888980  [ 1168/ 3200]\n",
            "loss: 1.014761  [ 1184/ 3200]\n",
            "loss: 0.839763  [ 1200/ 3200]\n",
            "loss: 1.144765  [ 1216/ 3200]\n",
            "loss: 1.078598  [ 1232/ 3200]\n",
            "loss: 0.893742  [ 1248/ 3200]\n",
            "loss: 0.940758  [ 1264/ 3200]\n",
            "loss: 0.992564  [ 1280/ 3200]\n",
            "loss: 0.868862  [ 1296/ 3200]\n",
            "loss: 1.020061  [ 1312/ 3200]\n",
            "loss: 0.845359  [ 1328/ 3200]\n",
            "loss: 0.937302  [ 1344/ 3200]\n",
            "loss: 1.080560  [ 1360/ 3200]\n",
            "loss: 0.942438  [ 1376/ 3200]\n",
            "loss: 0.951858  [ 1392/ 3200]\n",
            "loss: 1.031263  [ 1408/ 3200]\n",
            "loss: 0.812983  [ 1424/ 3200]\n",
            "loss: 0.977719  [ 1440/ 3200]\n",
            "loss: 0.992699  [ 1456/ 3200]\n",
            "loss: 0.900622  [ 1472/ 3200]\n",
            "loss: 1.019970  [ 1488/ 3200]\n",
            "loss: 0.989393  [ 1504/ 3200]\n",
            "loss: 0.981223  [ 1520/ 3200]\n",
            "loss: 0.953022  [ 1536/ 3200]\n",
            "loss: 0.813572  [ 1552/ 3200]\n",
            "loss: 0.993319  [ 1568/ 3200]\n",
            "loss: 1.053078  [ 1584/ 3200]\n",
            "loss: 0.889985  [ 1600/ 3200]\n",
            "loss: 1.012010  [ 1616/ 3200]\n",
            "loss: 1.071408  [ 1632/ 3200]\n",
            "loss: 1.192580  [ 1648/ 3200]\n",
            "loss: 1.037102  [ 1664/ 3200]\n",
            "loss: 1.335697  [ 1680/ 3200]\n",
            "loss: 1.084447  [ 1696/ 3200]\n",
            "loss: 0.891443  [ 1712/ 3200]\n",
            "loss: 0.846722  [ 1728/ 3200]\n",
            "loss: 1.223451  [ 1744/ 3200]\n",
            "loss: 1.009536  [ 1760/ 3200]\n",
            "loss: 1.118938  [ 1776/ 3200]\n",
            "loss: 1.153741  [ 1792/ 3200]\n",
            "loss: 0.855983  [ 1808/ 3200]\n",
            "loss: 1.122382  [ 1824/ 3200]\n",
            "loss: 1.135370  [ 1840/ 3200]\n",
            "loss: 0.879730  [ 1856/ 3200]\n",
            "loss: 1.138132  [ 1872/ 3200]\n",
            "loss: 0.883131  [ 1888/ 3200]\n",
            "loss: 1.018439  [ 1904/ 3200]\n",
            "loss: 1.025619  [ 1920/ 3200]\n",
            "loss: 0.960719  [ 1936/ 3200]\n",
            "loss: 1.077331  [ 1952/ 3200]\n",
            "loss: 0.812161  [ 1968/ 3200]\n",
            "loss: 1.130430  [ 1984/ 3200]\n",
            "loss: 0.897749  [ 2000/ 3200]\n",
            "loss: 0.947723  [ 2016/ 3200]\n",
            "loss: 0.908782  [ 2032/ 3200]\n",
            "loss: 1.011515  [ 2048/ 3200]\n",
            "loss: 1.109145  [ 2064/ 3200]\n",
            "loss: 1.184410  [ 2080/ 3200]\n",
            "loss: 1.038623  [ 2096/ 3200]\n",
            "loss: 0.980418  [ 2112/ 3200]\n",
            "loss: 0.902709  [ 2128/ 3200]\n",
            "loss: 1.082881  [ 2144/ 3200]\n",
            "loss: 0.660014  [ 2160/ 3200]\n",
            "loss: 1.176837  [ 2176/ 3200]\n",
            "loss: 0.965024  [ 2192/ 3200]\n",
            "loss: 0.998862  [ 2208/ 3200]\n",
            "loss: 0.977870  [ 2224/ 3200]\n",
            "loss: 1.130626  [ 2240/ 3200]\n",
            "loss: 1.328017  [ 2256/ 3200]\n",
            "loss: 1.029788  [ 2272/ 3200]\n",
            "loss: 0.933921  [ 2288/ 3200]\n",
            "loss: 0.891955  [ 2304/ 3200]\n",
            "loss: 0.887744  [ 2320/ 3200]\n",
            "loss: 1.004262  [ 2336/ 3200]\n",
            "loss: 0.963993  [ 2352/ 3200]\n",
            "loss: 1.113996  [ 2368/ 3200]\n",
            "loss: 1.127658  [ 2384/ 3200]\n",
            "loss: 0.979153  [ 2400/ 3200]\n",
            "loss: 1.397364  [ 2416/ 3200]\n",
            "loss: 1.027849  [ 2432/ 3200]\n",
            "loss: 1.175681  [ 2448/ 3200]\n",
            "loss: 0.955801  [ 2464/ 3200]\n",
            "loss: 0.924392  [ 2480/ 3200]\n",
            "loss: 1.141200  [ 2496/ 3200]\n",
            "loss: 0.747954  [ 2512/ 3200]\n",
            "loss: 1.138184  [ 2528/ 3200]\n",
            "loss: 1.181887  [ 2544/ 3200]\n",
            "loss: 0.778882  [ 2560/ 3200]\n",
            "loss: 0.785709  [ 2576/ 3200]\n",
            "loss: 0.783414  [ 2592/ 3200]\n",
            "loss: 0.959109  [ 2608/ 3200]\n",
            "loss: 1.018414  [ 2624/ 3200]\n",
            "loss: 0.797886  [ 2640/ 3200]\n",
            "loss: 1.059828  [ 2656/ 3200]\n",
            "loss: 0.894224  [ 2672/ 3200]\n",
            "loss: 1.118324  [ 2688/ 3200]\n",
            "loss: 1.055002  [ 2704/ 3200]\n",
            "loss: 1.046200  [ 2720/ 3200]\n",
            "loss: 1.067819  [ 2736/ 3200]\n",
            "loss: 1.167954  [ 2752/ 3200]\n",
            "loss: 0.731997  [ 2768/ 3200]\n",
            "loss: 1.291897  [ 2784/ 3200]\n",
            "loss: 0.823626  [ 2800/ 3200]\n",
            "loss: 1.257766  [ 2816/ 3200]\n",
            "loss: 1.258171  [ 2832/ 3200]\n",
            "loss: 1.042505  [ 2848/ 3200]\n",
            "loss: 0.987916  [ 2864/ 3200]\n",
            "loss: 1.092722  [ 2880/ 3200]\n",
            "loss: 1.133092  [ 2896/ 3200]\n",
            "loss: 1.045055  [ 2912/ 3200]\n",
            "loss: 1.058996  [ 2928/ 3200]\n",
            "loss: 0.815468  [ 2944/ 3200]\n",
            "loss: 0.967511  [ 2960/ 3200]\n",
            "loss: 1.051147  [ 2976/ 3200]\n",
            "loss: 0.965739  [ 2992/ 3200]\n",
            "loss: 1.140011  [ 3008/ 3200]\n",
            "loss: 0.662889  [ 3024/ 3200]\n",
            "loss: 1.110352  [ 3040/ 3200]\n",
            "loss: 0.855468  [ 3056/ 3200]\n",
            "loss: 0.904979  [ 3072/ 3200]\n",
            "loss: 1.049877  [ 3088/ 3200]\n",
            "loss: 1.162204  [ 3104/ 3200]\n",
            "loss: 1.139123  [ 3120/ 3200]\n",
            "loss: 0.773910  [ 3136/ 3200]\n",
            "loss: 0.930357  [ 3152/ 3200]\n",
            "loss: 1.222257  [ 3168/ 3200]\n",
            "loss: 1.147321  [ 3184/ 3200]\n",
            "Avg Accuracy: 52.875000%, Avg loss: 1.025724\n",
            "F1 score is: 0.45763990402221677\n",
            "Confusion Matrix:\n",
            "[[144   9  44   3]\n",
            " [ 39  39 117   5]\n",
            " [  8   3 188   1]\n",
            " [  3  34 111  52]]\n",
            "current epoch: 17\n",
            "\n",
            "loss: 0.796193  [    0/ 3200]\n",
            "loss: 0.977334  [   16/ 3200]\n",
            "loss: 1.142960  [   32/ 3200]\n",
            "loss: 0.979095  [   48/ 3200]\n",
            "loss: 1.004769  [   64/ 3200]\n",
            "loss: 1.196340  [   80/ 3200]\n",
            "loss: 1.013330  [   96/ 3200]\n",
            "loss: 0.777954  [  112/ 3200]\n",
            "loss: 1.157863  [  128/ 3200]\n",
            "loss: 0.877299  [  144/ 3200]\n",
            "loss: 0.707738  [  160/ 3200]\n",
            "loss: 1.099012  [  176/ 3200]\n",
            "loss: 1.061950  [  192/ 3200]\n",
            "loss: 0.964484  [  208/ 3200]\n",
            "loss: 0.863979  [  224/ 3200]\n",
            "loss: 0.830300  [  240/ 3200]\n",
            "loss: 1.009686  [  256/ 3200]\n",
            "loss: 0.957704  [  272/ 3200]\n",
            "loss: 1.098668  [  288/ 3200]\n",
            "loss: 0.911133  [  304/ 3200]\n",
            "loss: 0.853181  [  320/ 3200]\n",
            "loss: 1.090063  [  336/ 3200]\n",
            "loss: 0.881745  [  352/ 3200]\n",
            "loss: 1.140621  [  368/ 3200]\n",
            "loss: 1.031011  [  384/ 3200]\n",
            "loss: 0.803056  [  400/ 3200]\n",
            "loss: 1.040112  [  416/ 3200]\n",
            "loss: 1.135525  [  432/ 3200]\n",
            "loss: 1.305037  [  448/ 3200]\n",
            "loss: 0.919481  [  464/ 3200]\n",
            "loss: 1.012344  [  480/ 3200]\n",
            "loss: 1.111445  [  496/ 3200]\n",
            "loss: 0.974251  [  512/ 3200]\n",
            "loss: 0.907882  [  528/ 3200]\n",
            "loss: 1.150783  [  544/ 3200]\n",
            "loss: 1.218259  [  560/ 3200]\n",
            "loss: 0.968876  [  576/ 3200]\n",
            "loss: 1.077877  [  592/ 3200]\n",
            "loss: 0.869845  [  608/ 3200]\n",
            "loss: 1.142174  [  624/ 3200]\n",
            "loss: 0.796390  [  640/ 3200]\n",
            "loss: 1.050015  [  656/ 3200]\n",
            "loss: 0.657446  [  672/ 3200]\n",
            "loss: 1.195813  [  688/ 3200]\n",
            "loss: 0.893090  [  704/ 3200]\n",
            "loss: 1.114027  [  720/ 3200]\n",
            "loss: 0.826768  [  736/ 3200]\n",
            "loss: 0.859184  [  752/ 3200]\n",
            "loss: 0.811883  [  768/ 3200]\n",
            "loss: 0.922948  [  784/ 3200]\n",
            "loss: 1.040729  [  800/ 3200]\n",
            "loss: 0.939813  [  816/ 3200]\n",
            "loss: 0.905171  [  832/ 3200]\n",
            "loss: 0.977623  [  848/ 3200]\n",
            "loss: 0.965867  [  864/ 3200]\n",
            "loss: 0.942358  [  880/ 3200]\n",
            "loss: 0.994374  [  896/ 3200]\n",
            "loss: 1.045026  [  912/ 3200]\n",
            "loss: 1.004023  [  928/ 3200]\n",
            "loss: 0.876417  [  944/ 3200]\n",
            "loss: 1.335364  [  960/ 3200]\n",
            "loss: 1.029324  [  976/ 3200]\n",
            "loss: 0.893647  [  992/ 3200]\n",
            "loss: 0.682722  [ 1008/ 3200]\n",
            "loss: 1.117608  [ 1024/ 3200]\n",
            "loss: 1.074383  [ 1040/ 3200]\n",
            "loss: 0.805277  [ 1056/ 3200]\n",
            "loss: 1.009098  [ 1072/ 3200]\n",
            "loss: 1.231815  [ 1088/ 3200]\n",
            "loss: 0.864200  [ 1104/ 3200]\n",
            "loss: 1.088611  [ 1120/ 3200]\n",
            "loss: 0.903842  [ 1136/ 3200]\n",
            "loss: 0.979349  [ 1152/ 3200]\n",
            "loss: 0.742495  [ 1168/ 3200]\n",
            "loss: 1.076354  [ 1184/ 3200]\n",
            "loss: 0.817447  [ 1200/ 3200]\n",
            "loss: 0.891927  [ 1216/ 3200]\n",
            "loss: 1.203641  [ 1232/ 3200]\n",
            "loss: 1.183994  [ 1248/ 3200]\n",
            "loss: 1.220718  [ 1264/ 3200]\n",
            "loss: 1.048061  [ 1280/ 3200]\n",
            "loss: 0.943349  [ 1296/ 3200]\n",
            "loss: 0.954176  [ 1312/ 3200]\n",
            "loss: 1.043107  [ 1328/ 3200]\n",
            "loss: 1.066568  [ 1344/ 3200]\n",
            "loss: 1.068747  [ 1360/ 3200]\n",
            "loss: 1.142706  [ 1376/ 3200]\n",
            "loss: 1.391774  [ 1392/ 3200]\n",
            "loss: 0.927402  [ 1408/ 3200]\n",
            "loss: 0.934776  [ 1424/ 3200]\n",
            "loss: 1.339136  [ 1440/ 3200]\n",
            "loss: 0.809272  [ 1456/ 3200]\n",
            "loss: 0.869388  [ 1472/ 3200]\n",
            "loss: 1.181923  [ 1488/ 3200]\n",
            "loss: 0.903406  [ 1504/ 3200]\n",
            "loss: 1.139035  [ 1520/ 3200]\n",
            "loss: 0.986276  [ 1536/ 3200]\n",
            "loss: 0.947968  [ 1552/ 3200]\n",
            "loss: 1.034987  [ 1568/ 3200]\n",
            "loss: 0.969722  [ 1584/ 3200]\n",
            "loss: 0.720549  [ 1600/ 3200]\n",
            "loss: 1.094922  [ 1616/ 3200]\n",
            "loss: 0.964548  [ 1632/ 3200]\n",
            "loss: 1.128877  [ 1648/ 3200]\n",
            "loss: 1.101311  [ 1664/ 3200]\n",
            "loss: 1.283309  [ 1680/ 3200]\n",
            "loss: 1.076075  [ 1696/ 3200]\n",
            "loss: 0.945242  [ 1712/ 3200]\n",
            "loss: 1.215908  [ 1728/ 3200]\n",
            "loss: 1.119051  [ 1744/ 3200]\n",
            "loss: 1.173183  [ 1760/ 3200]\n",
            "loss: 1.146155  [ 1776/ 3200]\n",
            "loss: 0.941802  [ 1792/ 3200]\n",
            "loss: 1.054946  [ 1808/ 3200]\n",
            "loss: 0.967100  [ 1824/ 3200]\n",
            "loss: 1.093384  [ 1840/ 3200]\n",
            "loss: 1.114064  [ 1856/ 3200]\n",
            "loss: 1.135759  [ 1872/ 3200]\n",
            "loss: 0.860617  [ 1888/ 3200]\n",
            "loss: 0.764064  [ 1904/ 3200]\n",
            "loss: 0.999307  [ 1920/ 3200]\n",
            "loss: 1.107175  [ 1936/ 3200]\n",
            "loss: 1.101233  [ 1952/ 3200]\n",
            "loss: 0.999880  [ 1968/ 3200]\n",
            "loss: 1.130724  [ 1984/ 3200]\n",
            "loss: 0.995230  [ 2000/ 3200]\n",
            "loss: 0.776770  [ 2016/ 3200]\n",
            "loss: 0.962049  [ 2032/ 3200]\n",
            "loss: 1.296048  [ 2048/ 3200]\n",
            "loss: 0.838369  [ 2064/ 3200]\n",
            "loss: 0.966983  [ 2080/ 3200]\n",
            "loss: 0.929620  [ 2096/ 3200]\n",
            "loss: 0.894234  [ 2112/ 3200]\n",
            "loss: 0.955287  [ 2128/ 3200]\n",
            "loss: 1.016060  [ 2144/ 3200]\n",
            "loss: 0.929799  [ 2160/ 3200]\n",
            "loss: 0.881364  [ 2176/ 3200]\n",
            "loss: 1.064945  [ 2192/ 3200]\n",
            "loss: 0.797881  [ 2208/ 3200]\n",
            "loss: 0.959108  [ 2224/ 3200]\n",
            "loss: 0.955525  [ 2240/ 3200]\n",
            "loss: 0.906058  [ 2256/ 3200]\n",
            "loss: 1.143194  [ 2272/ 3200]\n",
            "loss: 1.197189  [ 2288/ 3200]\n",
            "loss: 1.150040  [ 2304/ 3200]\n",
            "loss: 0.950056  [ 2320/ 3200]\n",
            "loss: 1.458852  [ 2336/ 3200]\n",
            "loss: 0.905461  [ 2352/ 3200]\n",
            "loss: 0.991696  [ 2368/ 3200]\n",
            "loss: 0.843395  [ 2384/ 3200]\n",
            "loss: 0.895424  [ 2400/ 3200]\n",
            "loss: 0.860934  [ 2416/ 3200]\n",
            "loss: 0.909696  [ 2432/ 3200]\n",
            "loss: 1.040658  [ 2448/ 3200]\n",
            "loss: 0.923102  [ 2464/ 3200]\n",
            "loss: 0.991981  [ 2480/ 3200]\n",
            "loss: 1.113918  [ 2496/ 3200]\n",
            "loss: 0.885772  [ 2512/ 3200]\n",
            "loss: 1.001685  [ 2528/ 3200]\n",
            "loss: 1.243145  [ 2544/ 3200]\n",
            "loss: 1.012927  [ 2560/ 3200]\n",
            "loss: 1.000568  [ 2576/ 3200]\n",
            "loss: 1.066054  [ 2592/ 3200]\n",
            "loss: 0.762419  [ 2608/ 3200]\n",
            "loss: 0.862907  [ 2624/ 3200]\n",
            "loss: 1.063540  [ 2640/ 3200]\n",
            "loss: 0.998638  [ 2656/ 3200]\n",
            "loss: 0.885221  [ 2672/ 3200]\n",
            "loss: 0.980012  [ 2688/ 3200]\n",
            "loss: 0.949892  [ 2704/ 3200]\n",
            "loss: 1.129501  [ 2720/ 3200]\n",
            "loss: 0.873670  [ 2736/ 3200]\n",
            "loss: 0.755396  [ 2752/ 3200]\n",
            "loss: 1.091243  [ 2768/ 3200]\n",
            "loss: 1.123212  [ 2784/ 3200]\n",
            "loss: 0.965745  [ 2800/ 3200]\n",
            "loss: 1.072188  [ 2816/ 3200]\n",
            "loss: 0.932526  [ 2832/ 3200]\n",
            "loss: 1.109983  [ 2848/ 3200]\n",
            "loss: 0.996505  [ 2864/ 3200]\n",
            "loss: 1.114656  [ 2880/ 3200]\n",
            "loss: 1.125680  [ 2896/ 3200]\n",
            "loss: 1.083429  [ 2912/ 3200]\n",
            "loss: 0.969167  [ 2928/ 3200]\n",
            "loss: 0.881432  [ 2944/ 3200]\n",
            "loss: 0.977931  [ 2960/ 3200]\n",
            "loss: 1.087880  [ 2976/ 3200]\n",
            "loss: 1.017146  [ 2992/ 3200]\n",
            "loss: 1.153323  [ 3008/ 3200]\n",
            "loss: 1.046874  [ 3024/ 3200]\n",
            "loss: 0.996263  [ 3040/ 3200]\n",
            "loss: 0.827156  [ 3056/ 3200]\n",
            "loss: 0.908026  [ 3072/ 3200]\n",
            "loss: 0.926582  [ 3088/ 3200]\n",
            "loss: 0.999671  [ 3104/ 3200]\n",
            "loss: 0.983540  [ 3120/ 3200]\n",
            "loss: 0.980987  [ 3136/ 3200]\n",
            "loss: 1.024227  [ 3152/ 3200]\n",
            "loss: 1.055511  [ 3168/ 3200]\n",
            "loss: 1.177889  [ 3184/ 3200]\n",
            "Avg Accuracy: 63.375000%, Avg loss: 0.971871\n",
            "F1 score is: 0.5897397744655609\n",
            "Confusion Matrix:\n",
            "[[160  33   4   3]\n",
            " [ 38 111  40  11]\n",
            " [ 11  43 144   2]\n",
            " [  2  80  26  92]]\n",
            "current epoch: 18\n",
            "\n",
            "loss: 0.942511  [    0/ 3200]\n",
            "loss: 0.805022  [   16/ 3200]\n",
            "loss: 0.972004  [   32/ 3200]\n",
            "loss: 1.057871  [   48/ 3200]\n",
            "loss: 1.029721  [   64/ 3200]\n",
            "loss: 0.774673  [   80/ 3200]\n",
            "loss: 0.866342  [   96/ 3200]\n",
            "loss: 1.131372  [  112/ 3200]\n",
            "loss: 1.212366  [  128/ 3200]\n",
            "loss: 1.203929  [  144/ 3200]\n",
            "loss: 0.863721  [  160/ 3200]\n",
            "loss: 0.912868  [  176/ 3200]\n",
            "loss: 1.108348  [  192/ 3200]\n",
            "loss: 1.033247  [  208/ 3200]\n",
            "loss: 0.975629  [  224/ 3200]\n",
            "loss: 1.003203  [  240/ 3200]\n",
            "loss: 0.881486  [  256/ 3200]\n",
            "loss: 0.941337  [  272/ 3200]\n",
            "loss: 0.984529  [  288/ 3200]\n",
            "loss: 1.205759  [  304/ 3200]\n",
            "loss: 0.807452  [  320/ 3200]\n",
            "loss: 0.913702  [  336/ 3200]\n",
            "loss: 1.332406  [  352/ 3200]\n",
            "loss: 0.732698  [  368/ 3200]\n",
            "loss: 1.029216  [  384/ 3200]\n",
            "loss: 1.126867  [  400/ 3200]\n",
            "loss: 0.952489  [  416/ 3200]\n",
            "loss: 0.853647  [  432/ 3200]\n",
            "loss: 1.156534  [  448/ 3200]\n",
            "loss: 1.149536  [  464/ 3200]\n",
            "loss: 1.099787  [  480/ 3200]\n",
            "loss: 1.128298  [  496/ 3200]\n",
            "loss: 1.108241  [  512/ 3200]\n",
            "loss: 0.989671  [  528/ 3200]\n",
            "loss: 0.826707  [  544/ 3200]\n",
            "loss: 1.164880  [  560/ 3200]\n",
            "loss: 0.792628  [  576/ 3200]\n",
            "loss: 0.864916  [  592/ 3200]\n",
            "loss: 1.030295  [  608/ 3200]\n",
            "loss: 0.859719  [  624/ 3200]\n",
            "loss: 1.023488  [  640/ 3200]\n",
            "loss: 0.919381  [  656/ 3200]\n",
            "loss: 1.086451  [  672/ 3200]\n",
            "loss: 1.020696  [  688/ 3200]\n",
            "loss: 1.007369  [  704/ 3200]\n",
            "loss: 0.911912  [  720/ 3200]\n",
            "loss: 0.952262  [  736/ 3200]\n",
            "loss: 0.987850  [  752/ 3200]\n",
            "loss: 1.020221  [  768/ 3200]\n",
            "loss: 1.028801  [  784/ 3200]\n",
            "loss: 0.890153  [  800/ 3200]\n",
            "loss: 1.050563  [  816/ 3200]\n",
            "loss: 1.003817  [  832/ 3200]\n",
            "loss: 0.865927  [  848/ 3200]\n",
            "loss: 1.238541  [  864/ 3200]\n",
            "loss: 0.909226  [  880/ 3200]\n",
            "loss: 0.925215  [  896/ 3200]\n",
            "loss: 0.707561  [  912/ 3200]\n",
            "loss: 0.925215  [  928/ 3200]\n",
            "loss: 1.052988  [  944/ 3200]\n",
            "loss: 0.840139  [  960/ 3200]\n",
            "loss: 0.982562  [  976/ 3200]\n",
            "loss: 0.959897  [  992/ 3200]\n",
            "loss: 1.029984  [ 1008/ 3200]\n",
            "loss: 1.158037  [ 1024/ 3200]\n",
            "loss: 1.001214  [ 1040/ 3200]\n",
            "loss: 1.131798  [ 1056/ 3200]\n",
            "loss: 1.003060  [ 1072/ 3200]\n",
            "loss: 1.002351  [ 1088/ 3200]\n",
            "loss: 1.040562  [ 1104/ 3200]\n",
            "loss: 0.957901  [ 1120/ 3200]\n",
            "loss: 0.940134  [ 1136/ 3200]\n",
            "loss: 1.018335  [ 1152/ 3200]\n",
            "loss: 1.140856  [ 1168/ 3200]\n",
            "loss: 1.035066  [ 1184/ 3200]\n",
            "loss: 0.926025  [ 1200/ 3200]\n",
            "loss: 1.187683  [ 1216/ 3200]\n",
            "loss: 1.034172  [ 1232/ 3200]\n",
            "loss: 1.070843  [ 1248/ 3200]\n",
            "loss: 0.866831  [ 1264/ 3200]\n",
            "loss: 0.889649  [ 1280/ 3200]\n",
            "loss: 1.105157  [ 1296/ 3200]\n",
            "loss: 0.741347  [ 1312/ 3200]\n",
            "loss: 1.014402  [ 1328/ 3200]\n",
            "loss: 1.175138  [ 1344/ 3200]\n",
            "loss: 0.991267  [ 1360/ 3200]\n",
            "loss: 0.858161  [ 1376/ 3200]\n",
            "loss: 0.988161  [ 1392/ 3200]\n",
            "loss: 0.988858  [ 1408/ 3200]\n",
            "loss: 1.279088  [ 1424/ 3200]\n",
            "loss: 1.042574  [ 1440/ 3200]\n",
            "loss: 0.789922  [ 1456/ 3200]\n",
            "loss: 0.948332  [ 1472/ 3200]\n",
            "loss: 0.958224  [ 1488/ 3200]\n",
            "loss: 0.955647  [ 1504/ 3200]\n",
            "loss: 0.743410  [ 1520/ 3200]\n",
            "loss: 1.051426  [ 1536/ 3200]\n",
            "loss: 1.230969  [ 1552/ 3200]\n",
            "loss: 0.975112  [ 1568/ 3200]\n",
            "loss: 1.098534  [ 1584/ 3200]\n",
            "loss: 0.939675  [ 1600/ 3200]\n",
            "loss: 1.176297  [ 1616/ 3200]\n",
            "loss: 0.793037  [ 1632/ 3200]\n",
            "loss: 0.954162  [ 1648/ 3200]\n",
            "loss: 1.061897  [ 1664/ 3200]\n",
            "loss: 0.798967  [ 1680/ 3200]\n",
            "loss: 0.989737  [ 1696/ 3200]\n",
            "loss: 0.925661  [ 1712/ 3200]\n",
            "loss: 1.071594  [ 1728/ 3200]\n",
            "loss: 1.200703  [ 1744/ 3200]\n",
            "loss: 0.841299  [ 1760/ 3200]\n",
            "loss: 0.838671  [ 1776/ 3200]\n",
            "loss: 0.971047  [ 1792/ 3200]\n",
            "loss: 0.788879  [ 1808/ 3200]\n",
            "loss: 0.766493  [ 1824/ 3200]\n",
            "loss: 1.019225  [ 1840/ 3200]\n",
            "loss: 0.973813  [ 1856/ 3200]\n",
            "loss: 0.843938  [ 1872/ 3200]\n",
            "loss: 0.933343  [ 1888/ 3200]\n",
            "loss: 1.034534  [ 1904/ 3200]\n",
            "loss: 0.962365  [ 1920/ 3200]\n",
            "loss: 1.016435  [ 1936/ 3200]\n",
            "loss: 1.184690  [ 1952/ 3200]\n",
            "loss: 0.750808  [ 1968/ 3200]\n",
            "loss: 0.818348  [ 1984/ 3200]\n",
            "loss: 1.037540  [ 2000/ 3200]\n",
            "loss: 1.247963  [ 2016/ 3200]\n",
            "loss: 1.081967  [ 2032/ 3200]\n",
            "loss: 0.950922  [ 2048/ 3200]\n",
            "loss: 1.074535  [ 2064/ 3200]\n",
            "loss: 1.066664  [ 2080/ 3200]\n",
            "loss: 1.000225  [ 2096/ 3200]\n",
            "loss: 0.978983  [ 2112/ 3200]\n",
            "loss: 0.693228  [ 2128/ 3200]\n",
            "loss: 0.861057  [ 2144/ 3200]\n",
            "loss: 0.734047  [ 2160/ 3200]\n",
            "loss: 1.145567  [ 2176/ 3200]\n",
            "loss: 1.020569  [ 2192/ 3200]\n",
            "loss: 0.971141  [ 2208/ 3200]\n",
            "loss: 0.939836  [ 2224/ 3200]\n",
            "loss: 0.858027  [ 2240/ 3200]\n",
            "loss: 0.889738  [ 2256/ 3200]\n",
            "loss: 0.860884  [ 2272/ 3200]\n",
            "loss: 1.041073  [ 2288/ 3200]\n",
            "loss: 0.955742  [ 2304/ 3200]\n",
            "loss: 1.267653  [ 2320/ 3200]\n",
            "loss: 1.021169  [ 2336/ 3200]\n",
            "loss: 0.990622  [ 2352/ 3200]\n",
            "loss: 1.100822  [ 2368/ 3200]\n",
            "loss: 0.921494  [ 2384/ 3200]\n",
            "loss: 1.191157  [ 2400/ 3200]\n",
            "loss: 0.835561  [ 2416/ 3200]\n",
            "loss: 0.999523  [ 2432/ 3200]\n",
            "loss: 0.868992  [ 2448/ 3200]\n",
            "loss: 0.998413  [ 2464/ 3200]\n",
            "loss: 1.228604  [ 2480/ 3200]\n",
            "loss: 1.364580  [ 2496/ 3200]\n",
            "loss: 1.011351  [ 2512/ 3200]\n",
            "loss: 0.735980  [ 2528/ 3200]\n",
            "loss: 0.891031  [ 2544/ 3200]\n",
            "loss: 1.069995  [ 2560/ 3200]\n",
            "loss: 0.951459  [ 2576/ 3200]\n",
            "loss: 0.876301  [ 2592/ 3200]\n",
            "loss: 0.813820  [ 2608/ 3200]\n",
            "loss: 1.051570  [ 2624/ 3200]\n",
            "loss: 1.016940  [ 2640/ 3200]\n",
            "loss: 0.679884  [ 2656/ 3200]\n",
            "loss: 1.241375  [ 2672/ 3200]\n",
            "loss: 1.109383  [ 2688/ 3200]\n",
            "loss: 0.981103  [ 2704/ 3200]\n",
            "loss: 0.892711  [ 2720/ 3200]\n",
            "loss: 1.201623  [ 2736/ 3200]\n",
            "loss: 0.969870  [ 2752/ 3200]\n",
            "loss: 1.099508  [ 2768/ 3200]\n",
            "loss: 1.054553  [ 2784/ 3200]\n",
            "loss: 0.784406  [ 2800/ 3200]\n",
            "loss: 1.015319  [ 2816/ 3200]\n",
            "loss: 1.122638  [ 2832/ 3200]\n",
            "loss: 1.379392  [ 2848/ 3200]\n",
            "loss: 0.884947  [ 2864/ 3200]\n",
            "loss: 0.842114  [ 2880/ 3200]\n",
            "loss: 1.457615  [ 2896/ 3200]\n",
            "loss: 0.959575  [ 2912/ 3200]\n",
            "loss: 1.475556  [ 2928/ 3200]\n",
            "loss: 1.005825  [ 2944/ 3200]\n",
            "loss: 1.079964  [ 2960/ 3200]\n",
            "loss: 0.905193  [ 2976/ 3200]\n",
            "loss: 1.011263  [ 2992/ 3200]\n",
            "loss: 1.027896  [ 3008/ 3200]\n",
            "loss: 0.999218  [ 3024/ 3200]\n",
            "loss: 0.855283  [ 3040/ 3200]\n",
            "loss: 1.028643  [ 3056/ 3200]\n",
            "loss: 0.927633  [ 3072/ 3200]\n",
            "loss: 1.101144  [ 3088/ 3200]\n",
            "loss: 1.291657  [ 3104/ 3200]\n",
            "loss: 1.164736  [ 3120/ 3200]\n",
            "loss: 1.082103  [ 3136/ 3200]\n",
            "loss: 1.046837  [ 3152/ 3200]\n",
            "loss: 0.872904  [ 3168/ 3200]\n",
            "loss: 0.974611  [ 3184/ 3200]\n",
            "Avg Accuracy: 59.250000%, Avg loss: 0.985270\n",
            "F1 score is: 0.5405845159292221\n",
            "Confusion Matrix:\n",
            "[[116  64   5  15]\n",
            " [ 20  36  16 128]\n",
            " [  2  28 134  36]\n",
            " [  0   5   7 188]]\n",
            "current epoch: 19\n",
            "\n",
            "loss: 1.022982  [    0/ 3200]\n",
            "loss: 1.057444  [   16/ 3200]\n",
            "loss: 1.045131  [   32/ 3200]\n",
            "loss: 0.892856  [   48/ 3200]\n",
            "loss: 0.732511  [   64/ 3200]\n",
            "loss: 0.924825  [   80/ 3200]\n",
            "loss: 0.689441  [   96/ 3200]\n",
            "loss: 0.808823  [  112/ 3200]\n",
            "loss: 0.860205  [  128/ 3200]\n",
            "loss: 1.017480  [  144/ 3200]\n",
            "loss: 0.694033  [  160/ 3200]\n",
            "loss: 0.960881  [  176/ 3200]\n",
            "loss: 0.980027  [  192/ 3200]\n",
            "loss: 0.919835  [  208/ 3200]\n",
            "loss: 0.762586  [  224/ 3200]\n",
            "loss: 0.837035  [  240/ 3200]\n",
            "loss: 0.770112  [  256/ 3200]\n",
            "loss: 0.670956  [  272/ 3200]\n",
            "loss: 1.249552  [  288/ 3200]\n",
            "loss: 1.079270  [  304/ 3200]\n",
            "loss: 0.907340  [  320/ 3200]\n",
            "loss: 0.947331  [  336/ 3200]\n",
            "loss: 1.051542  [  352/ 3200]\n",
            "loss: 1.280850  [  368/ 3200]\n",
            "loss: 1.088502  [  384/ 3200]\n",
            "loss: 0.825666  [  400/ 3200]\n",
            "loss: 0.820909  [  416/ 3200]\n",
            "loss: 1.224276  [  432/ 3200]\n",
            "loss: 1.022417  [  448/ 3200]\n",
            "loss: 1.075051  [  464/ 3200]\n",
            "loss: 0.970248  [  480/ 3200]\n",
            "loss: 0.711930  [  496/ 3200]\n",
            "loss: 0.921679  [  512/ 3200]\n",
            "loss: 0.960993  [  528/ 3200]\n",
            "loss: 1.029616  [  544/ 3200]\n",
            "loss: 0.992003  [  560/ 3200]\n",
            "loss: 0.790302  [  576/ 3200]\n",
            "loss: 0.993828  [  592/ 3200]\n",
            "loss: 0.919652  [  608/ 3200]\n",
            "loss: 0.899510  [  624/ 3200]\n",
            "loss: 0.784732  [  640/ 3200]\n",
            "loss: 0.998213  [  656/ 3200]\n",
            "loss: 0.926640  [  672/ 3200]\n",
            "loss: 1.096229  [  688/ 3200]\n",
            "loss: 1.276158  [  704/ 3200]\n",
            "loss: 0.900333  [  720/ 3200]\n",
            "loss: 1.097457  [  736/ 3200]\n",
            "loss: 1.205853  [  752/ 3200]\n",
            "loss: 0.978230  [  768/ 3200]\n",
            "loss: 0.833670  [  784/ 3200]\n",
            "loss: 1.130030  [  800/ 3200]\n",
            "loss: 1.283140  [  816/ 3200]\n",
            "loss: 1.018385  [  832/ 3200]\n",
            "loss: 1.036583  [  848/ 3200]\n",
            "loss: 1.049412  [  864/ 3200]\n",
            "loss: 0.948542  [  880/ 3200]\n",
            "loss: 0.892668  [  896/ 3200]\n",
            "loss: 0.719254  [  912/ 3200]\n",
            "loss: 0.670927  [  928/ 3200]\n",
            "loss: 0.789487  [  944/ 3200]\n",
            "loss: 1.319360  [  960/ 3200]\n",
            "loss: 0.984223  [  976/ 3200]\n",
            "loss: 0.831332  [  992/ 3200]\n",
            "loss: 0.784211  [ 1008/ 3200]\n",
            "loss: 0.869778  [ 1024/ 3200]\n",
            "loss: 1.126113  [ 1040/ 3200]\n",
            "loss: 0.829793  [ 1056/ 3200]\n",
            "loss: 0.882723  [ 1072/ 3200]\n",
            "loss: 0.832542  [ 1088/ 3200]\n",
            "loss: 1.049868  [ 1104/ 3200]\n",
            "loss: 1.003091  [ 1120/ 3200]\n",
            "loss: 0.760936  [ 1136/ 3200]\n",
            "loss: 1.050010  [ 1152/ 3200]\n",
            "loss: 1.094884  [ 1168/ 3200]\n",
            "loss: 0.997293  [ 1184/ 3200]\n",
            "loss: 0.987461  [ 1200/ 3200]\n",
            "loss: 1.026132  [ 1216/ 3200]\n",
            "loss: 1.120961  [ 1232/ 3200]\n",
            "loss: 0.829856  [ 1248/ 3200]\n",
            "loss: 0.683467  [ 1264/ 3200]\n",
            "loss: 0.887554  [ 1280/ 3200]\n",
            "loss: 1.162614  [ 1296/ 3200]\n",
            "loss: 1.192202  [ 1312/ 3200]\n",
            "loss: 1.066578  [ 1328/ 3200]\n",
            "loss: 0.923779  [ 1344/ 3200]\n",
            "loss: 1.178053  [ 1360/ 3200]\n",
            "loss: 1.149701  [ 1376/ 3200]\n",
            "loss: 0.842619  [ 1392/ 3200]\n",
            "loss: 0.748349  [ 1408/ 3200]\n",
            "loss: 1.108808  [ 1424/ 3200]\n",
            "loss: 1.059423  [ 1440/ 3200]\n",
            "loss: 1.016467  [ 1456/ 3200]\n",
            "loss: 0.749487  [ 1472/ 3200]\n",
            "loss: 0.815920  [ 1488/ 3200]\n",
            "loss: 1.166141  [ 1504/ 3200]\n",
            "loss: 0.922941  [ 1520/ 3200]\n",
            "loss: 1.223648  [ 1536/ 3200]\n",
            "loss: 1.096135  [ 1552/ 3200]\n",
            "loss: 0.825798  [ 1568/ 3200]\n",
            "loss: 0.925608  [ 1584/ 3200]\n",
            "loss: 0.987167  [ 1600/ 3200]\n",
            "loss: 0.891274  [ 1616/ 3200]\n",
            "loss: 0.721256  [ 1632/ 3200]\n",
            "loss: 1.128358  [ 1648/ 3200]\n",
            "loss: 0.814184  [ 1664/ 3200]\n",
            "loss: 1.235873  [ 1680/ 3200]\n",
            "loss: 1.075544  [ 1696/ 3200]\n",
            "loss: 1.218903  [ 1712/ 3200]\n",
            "loss: 0.915204  [ 1728/ 3200]\n",
            "loss: 1.094069  [ 1744/ 3200]\n",
            "loss: 0.796560  [ 1760/ 3200]\n",
            "loss: 1.079910  [ 1776/ 3200]\n",
            "loss: 0.929558  [ 1792/ 3200]\n",
            "loss: 1.081001  [ 1808/ 3200]\n",
            "loss: 0.943147  [ 1824/ 3200]\n",
            "loss: 0.964609  [ 1840/ 3200]\n",
            "loss: 1.110982  [ 1856/ 3200]\n",
            "loss: 0.717272  [ 1872/ 3200]\n",
            "loss: 1.083132  [ 1888/ 3200]\n",
            "loss: 1.284888  [ 1904/ 3200]\n",
            "loss: 1.190428  [ 1920/ 3200]\n",
            "loss: 1.021818  [ 1936/ 3200]\n",
            "loss: 0.792013  [ 1952/ 3200]\n",
            "loss: 1.112454  [ 1968/ 3200]\n",
            "loss: 0.969430  [ 1984/ 3200]\n",
            "loss: 0.897750  [ 2000/ 3200]\n",
            "loss: 0.925227  [ 2016/ 3200]\n",
            "loss: 1.124426  [ 2032/ 3200]\n",
            "loss: 1.143739  [ 2048/ 3200]\n",
            "loss: 0.797350  [ 2064/ 3200]\n",
            "loss: 0.950680  [ 2080/ 3200]\n",
            "loss: 1.581735  [ 2096/ 3200]\n",
            "loss: 1.370321  [ 2112/ 3200]\n",
            "loss: 0.789657  [ 2128/ 3200]\n",
            "loss: 1.037530  [ 2144/ 3200]\n",
            "loss: 0.889711  [ 2160/ 3200]\n",
            "loss: 0.930929  [ 2176/ 3200]\n",
            "loss: 1.160715  [ 2192/ 3200]\n",
            "loss: 1.061948  [ 2208/ 3200]\n",
            "loss: 1.045552  [ 2224/ 3200]\n",
            "loss: 1.047289  [ 2240/ 3200]\n",
            "loss: 0.865177  [ 2256/ 3200]\n",
            "loss: 1.320765  [ 2272/ 3200]\n",
            "loss: 0.853773  [ 2288/ 3200]\n",
            "loss: 1.182062  [ 2304/ 3200]\n",
            "loss: 1.116685  [ 2320/ 3200]\n",
            "loss: 0.972054  [ 2336/ 3200]\n",
            "loss: 0.810284  [ 2352/ 3200]\n",
            "loss: 1.118658  [ 2368/ 3200]\n",
            "loss: 1.193349  [ 2384/ 3200]\n",
            "loss: 0.817174  [ 2400/ 3200]\n",
            "loss: 1.154167  [ 2416/ 3200]\n",
            "loss: 0.913173  [ 2432/ 3200]\n",
            "loss: 1.116545  [ 2448/ 3200]\n",
            "loss: 1.125774  [ 2464/ 3200]\n",
            "loss: 0.921176  [ 2480/ 3200]\n",
            "loss: 1.390494  [ 2496/ 3200]\n",
            "loss: 1.167108  [ 2512/ 3200]\n",
            "loss: 1.079810  [ 2528/ 3200]\n",
            "loss: 0.880858  [ 2544/ 3200]\n",
            "loss: 0.895150  [ 2560/ 3200]\n",
            "loss: 0.838954  [ 2576/ 3200]\n",
            "loss: 1.047793  [ 2592/ 3200]\n",
            "loss: 1.029078  [ 2608/ 3200]\n",
            "loss: 1.000985  [ 2624/ 3200]\n",
            "loss: 0.873269  [ 2640/ 3200]\n",
            "loss: 1.106389  [ 2656/ 3200]\n",
            "loss: 1.052037  [ 2672/ 3200]\n",
            "loss: 0.873526  [ 2688/ 3200]\n",
            "loss: 0.843554  [ 2704/ 3200]\n",
            "loss: 0.955396  [ 2720/ 3200]\n",
            "loss: 1.050429  [ 2736/ 3200]\n",
            "loss: 0.938230  [ 2752/ 3200]\n",
            "loss: 0.955827  [ 2768/ 3200]\n",
            "loss: 1.010404  [ 2784/ 3200]\n",
            "loss: 0.944199  [ 2800/ 3200]\n",
            "loss: 1.059053  [ 2816/ 3200]\n",
            "loss: 1.151618  [ 2832/ 3200]\n",
            "loss: 0.964984  [ 2848/ 3200]\n",
            "loss: 1.205392  [ 2864/ 3200]\n",
            "loss: 0.902041  [ 2880/ 3200]\n",
            "loss: 0.825272  [ 2896/ 3200]\n",
            "loss: 0.805136  [ 2912/ 3200]\n",
            "loss: 0.931984  [ 2928/ 3200]\n",
            "loss: 0.948295  [ 2944/ 3200]\n",
            "loss: 0.941066  [ 2960/ 3200]\n",
            "loss: 1.102302  [ 2976/ 3200]\n",
            "loss: 0.779755  [ 2992/ 3200]\n",
            "loss: 1.129120  [ 3008/ 3200]\n",
            "loss: 1.158708  [ 3024/ 3200]\n",
            "loss: 0.879109  [ 3040/ 3200]\n",
            "loss: 1.073669  [ 3056/ 3200]\n",
            "loss: 1.224658  [ 3072/ 3200]\n",
            "loss: 1.014650  [ 3088/ 3200]\n",
            "loss: 0.905752  [ 3104/ 3200]\n",
            "loss: 0.983173  [ 3120/ 3200]\n",
            "loss: 1.117408  [ 3136/ 3200]\n",
            "loss: 0.969655  [ 3152/ 3200]\n",
            "loss: 1.181574  [ 3168/ 3200]\n",
            "loss: 0.810363  [ 3184/ 3200]\n",
            "Avg Accuracy: 63.375000%, Avg loss: 0.946641\n",
            "F1 score is: 0.5749443000555039\n",
            "Confusion Matrix:\n",
            "[[189   5   1   5]\n",
            " [ 71  35  14  80]\n",
            " [ 31  24 123  22]\n",
            " [ 14  15  11 160]]\n",
            "current epoch: 20\n",
            "\n",
            "loss: 1.031593  [    0/ 3200]\n",
            "loss: 0.913093  [   16/ 3200]\n",
            "loss: 0.937000  [   32/ 3200]\n",
            "loss: 1.086322  [   48/ 3200]\n",
            "loss: 0.929793  [   64/ 3200]\n",
            "loss: 0.998533  [   80/ 3200]\n",
            "loss: 0.953179  [   96/ 3200]\n",
            "loss: 1.117137  [  112/ 3200]\n",
            "loss: 1.031829  [  128/ 3200]\n",
            "loss: 1.072486  [  144/ 3200]\n",
            "loss: 1.048367  [  160/ 3200]\n",
            "loss: 1.091526  [  176/ 3200]\n",
            "loss: 0.915684  [  192/ 3200]\n",
            "loss: 0.933187  [  208/ 3200]\n",
            "loss: 0.986380  [  224/ 3200]\n",
            "loss: 0.944346  [  240/ 3200]\n",
            "loss: 0.928252  [  256/ 3200]\n",
            "loss: 0.883078  [  272/ 3200]\n",
            "loss: 0.843488  [  288/ 3200]\n",
            "loss: 1.129460  [  304/ 3200]\n",
            "loss: 1.067565  [  320/ 3200]\n",
            "loss: 1.021642  [  336/ 3200]\n",
            "loss: 1.326077  [  352/ 3200]\n",
            "loss: 0.898835  [  368/ 3200]\n",
            "loss: 1.326632  [  384/ 3200]\n",
            "loss: 1.083086  [  400/ 3200]\n",
            "loss: 0.838426  [  416/ 3200]\n",
            "loss: 0.822205  [  432/ 3200]\n",
            "loss: 1.130225  [  448/ 3200]\n",
            "loss: 1.019904  [  464/ 3200]\n",
            "loss: 1.085751  [  480/ 3200]\n",
            "loss: 0.959204  [  496/ 3200]\n",
            "loss: 1.299200  [  512/ 3200]\n",
            "loss: 0.856950  [  528/ 3200]\n",
            "loss: 1.217937  [  544/ 3200]\n",
            "loss: 1.147399  [  560/ 3200]\n",
            "loss: 0.954328  [  576/ 3200]\n",
            "loss: 1.256731  [  592/ 3200]\n",
            "loss: 1.161547  [  608/ 3200]\n",
            "loss: 1.022548  [  624/ 3200]\n",
            "loss: 1.211312  [  640/ 3200]\n",
            "loss: 0.806396  [  656/ 3200]\n",
            "loss: 1.265419  [  672/ 3200]\n",
            "loss: 1.215552  [  688/ 3200]\n",
            "loss: 0.813353  [  704/ 3200]\n",
            "loss: 1.022609  [  720/ 3200]\n",
            "loss: 0.945861  [  736/ 3200]\n",
            "loss: 0.908729  [  752/ 3200]\n",
            "loss: 1.076552  [  768/ 3200]\n",
            "loss: 0.802612  [  784/ 3200]\n",
            "loss: 1.012319  [  800/ 3200]\n",
            "loss: 1.094318  [  816/ 3200]\n",
            "loss: 1.150053  [  832/ 3200]\n",
            "loss: 0.861704  [  848/ 3200]\n",
            "loss: 0.913396  [  864/ 3200]\n",
            "loss: 0.913863  [  880/ 3200]\n",
            "loss: 1.020266  [  896/ 3200]\n",
            "loss: 0.971828  [  912/ 3200]\n",
            "loss: 1.259138  [  928/ 3200]\n",
            "loss: 0.836418  [  944/ 3200]\n",
            "loss: 0.908796  [  960/ 3200]\n",
            "loss: 1.506447  [  976/ 3200]\n",
            "loss: 1.039389  [  992/ 3200]\n",
            "loss: 1.150303  [ 1008/ 3200]\n",
            "loss: 1.228381  [ 1024/ 3200]\n",
            "loss: 0.941583  [ 1040/ 3200]\n",
            "loss: 1.031860  [ 1056/ 3200]\n",
            "loss: 1.106484  [ 1072/ 3200]\n",
            "loss: 1.166873  [ 1088/ 3200]\n",
            "loss: 0.976260  [ 1104/ 3200]\n",
            "loss: 1.251681  [ 1120/ 3200]\n",
            "loss: 0.980820  [ 1136/ 3200]\n",
            "loss: 0.912276  [ 1152/ 3200]\n",
            "loss: 0.864244  [ 1168/ 3200]\n",
            "loss: 1.088549  [ 1184/ 3200]\n",
            "loss: 0.850745  [ 1200/ 3200]\n",
            "loss: 0.889592  [ 1216/ 3200]\n",
            "loss: 0.932677  [ 1232/ 3200]\n",
            "loss: 0.822406  [ 1248/ 3200]\n",
            "loss: 0.921188  [ 1264/ 3200]\n",
            "loss: 0.719447  [ 1280/ 3200]\n",
            "loss: 1.095200  [ 1296/ 3200]\n",
            "loss: 0.931299  [ 1312/ 3200]\n",
            "loss: 1.012416  [ 1328/ 3200]\n",
            "loss: 1.136246  [ 1344/ 3200]\n",
            "loss: 0.900286  [ 1360/ 3200]\n",
            "loss: 0.802583  [ 1376/ 3200]\n",
            "loss: 0.961084  [ 1392/ 3200]\n",
            "loss: 1.076063  [ 1408/ 3200]\n",
            "loss: 0.816395  [ 1424/ 3200]\n",
            "loss: 1.066784  [ 1440/ 3200]\n",
            "loss: 0.884740  [ 1456/ 3200]\n",
            "loss: 1.119285  [ 1472/ 3200]\n",
            "loss: 1.155970  [ 1488/ 3200]\n",
            "loss: 1.208247  [ 1504/ 3200]\n",
            "loss: 1.015796  [ 1520/ 3200]\n",
            "loss: 0.862247  [ 1536/ 3200]\n",
            "loss: 1.228085  [ 1552/ 3200]\n",
            "loss: 0.842422  [ 1568/ 3200]\n",
            "loss: 1.018982  [ 1584/ 3200]\n",
            "loss: 0.809778  [ 1600/ 3200]\n",
            "loss: 1.049504  [ 1616/ 3200]\n",
            "loss: 0.879566  [ 1632/ 3200]\n",
            "loss: 0.970740  [ 1648/ 3200]\n",
            "loss: 0.942163  [ 1664/ 3200]\n",
            "loss: 1.127893  [ 1680/ 3200]\n",
            "loss: 0.840320  [ 1696/ 3200]\n",
            "loss: 1.168414  [ 1712/ 3200]\n",
            "loss: 0.911912  [ 1728/ 3200]\n",
            "loss: 1.363258  [ 1744/ 3200]\n",
            "loss: 0.813406  [ 1760/ 3200]\n",
            "loss: 0.948535  [ 1776/ 3200]\n",
            "loss: 0.916840  [ 1792/ 3200]\n",
            "loss: 0.782634  [ 1808/ 3200]\n",
            "loss: 1.033119  [ 1824/ 3200]\n",
            "loss: 0.744153  [ 1840/ 3200]\n",
            "loss: 0.953534  [ 1856/ 3200]\n",
            "loss: 1.123082  [ 1872/ 3200]\n",
            "loss: 1.011213  [ 1888/ 3200]\n",
            "loss: 1.106675  [ 1904/ 3200]\n",
            "loss: 0.866383  [ 1920/ 3200]\n",
            "loss: 0.906282  [ 1936/ 3200]\n",
            "loss: 1.015337  [ 1952/ 3200]\n",
            "loss: 0.914456  [ 1968/ 3200]\n",
            "loss: 1.028936  [ 1984/ 3200]\n",
            "loss: 1.035588  [ 2000/ 3200]\n",
            "loss: 0.698094  [ 2016/ 3200]\n",
            "loss: 1.042525  [ 2032/ 3200]\n",
            "loss: 1.021908  [ 2048/ 3200]\n",
            "loss: 0.856993  [ 2064/ 3200]\n",
            "loss: 1.138221  [ 2080/ 3200]\n",
            "loss: 1.084159  [ 2096/ 3200]\n",
            "loss: 0.983356  [ 2112/ 3200]\n",
            "loss: 0.939688  [ 2128/ 3200]\n",
            "loss: 0.920126  [ 2144/ 3200]\n",
            "loss: 1.011426  [ 2160/ 3200]\n",
            "loss: 0.920972  [ 2176/ 3200]\n",
            "loss: 0.739025  [ 2192/ 3200]\n",
            "loss: 0.809045  [ 2208/ 3200]\n",
            "loss: 0.757510  [ 2224/ 3200]\n",
            "loss: 0.965423  [ 2240/ 3200]\n",
            "loss: 0.744085  [ 2256/ 3200]\n",
            "loss: 1.136511  [ 2272/ 3200]\n",
            "loss: 0.865731  [ 2288/ 3200]\n",
            "loss: 0.780273  [ 2304/ 3200]\n",
            "loss: 1.052132  [ 2320/ 3200]\n",
            "loss: 1.050443  [ 2336/ 3200]\n",
            "loss: 0.715469  [ 2352/ 3200]\n",
            "loss: 1.234244  [ 2368/ 3200]\n",
            "loss: 1.081690  [ 2384/ 3200]\n",
            "loss: 0.925908  [ 2400/ 3200]\n",
            "loss: 0.921035  [ 2416/ 3200]\n",
            "loss: 0.943925  [ 2432/ 3200]\n",
            "loss: 0.786943  [ 2448/ 3200]\n",
            "loss: 1.233415  [ 2464/ 3200]\n",
            "loss: 0.948193  [ 2480/ 3200]\n",
            "loss: 1.046963  [ 2496/ 3200]\n",
            "loss: 0.939684  [ 2512/ 3200]\n",
            "loss: 0.843873  [ 2528/ 3200]\n",
            "loss: 1.101758  [ 2544/ 3200]\n",
            "loss: 0.951703  [ 2560/ 3200]\n",
            "loss: 1.029012  [ 2576/ 3200]\n",
            "loss: 0.908051  [ 2592/ 3200]\n",
            "loss: 0.824058  [ 2608/ 3200]\n",
            "loss: 0.645780  [ 2624/ 3200]\n",
            "loss: 0.942262  [ 2640/ 3200]\n",
            "loss: 0.849806  [ 2656/ 3200]\n",
            "loss: 1.065438  [ 2672/ 3200]\n",
            "loss: 0.833910  [ 2688/ 3200]\n",
            "loss: 0.833296  [ 2704/ 3200]\n",
            "loss: 0.736908  [ 2720/ 3200]\n",
            "loss: 0.842426  [ 2736/ 3200]\n",
            "loss: 1.080450  [ 2752/ 3200]\n",
            "loss: 0.966645  [ 2768/ 3200]\n",
            "loss: 0.996273  [ 2784/ 3200]\n",
            "loss: 0.707033  [ 2800/ 3200]\n",
            "loss: 1.071867  [ 2816/ 3200]\n",
            "loss: 0.831431  [ 2832/ 3200]\n",
            "loss: 0.799975  [ 2848/ 3200]\n",
            "loss: 0.786350  [ 2864/ 3200]\n",
            "loss: 0.861716  [ 2880/ 3200]\n",
            "loss: 1.113125  [ 2896/ 3200]\n",
            "loss: 0.822075  [ 2912/ 3200]\n",
            "loss: 0.984145  [ 2928/ 3200]\n",
            "loss: 0.891801  [ 2944/ 3200]\n",
            "loss: 0.568831  [ 2960/ 3200]\n",
            "loss: 0.865784  [ 2976/ 3200]\n",
            "loss: 0.962112  [ 2992/ 3200]\n",
            "loss: 0.882284  [ 3008/ 3200]\n",
            "loss: 1.030360  [ 3024/ 3200]\n",
            "loss: 1.134788  [ 3040/ 3200]\n",
            "loss: 0.913758  [ 3056/ 3200]\n",
            "loss: 0.893860  [ 3072/ 3200]\n",
            "loss: 0.809896  [ 3088/ 3200]\n",
            "loss: 1.266994  [ 3104/ 3200]\n",
            "loss: 1.080811  [ 3120/ 3200]\n",
            "loss: 1.113039  [ 3136/ 3200]\n",
            "loss: 0.857625  [ 3152/ 3200]\n",
            "loss: 0.933182  [ 3168/ 3200]\n",
            "loss: 0.916329  [ 3184/ 3200]\n",
            "Avg Accuracy: 62.625000%, Avg loss: 0.935584\n",
            "F1 score is: 0.5993214559555053\n",
            "Confusion Matrix:\n",
            "[[124  70   2   4]\n",
            " [ 25 116  20  39]\n",
            " [  6  55 132   7]\n",
            " [  1  54  16 129]]\n",
            "current epoch: 21\n",
            "\n",
            "loss: 0.761486  [    0/ 3200]\n",
            "loss: 1.042173  [   16/ 3200]\n",
            "loss: 1.007613  [   32/ 3200]\n",
            "loss: 1.017465  [   48/ 3200]\n",
            "loss: 0.899811  [   64/ 3200]\n",
            "loss: 0.599431  [   80/ 3200]\n",
            "loss: 0.831721  [   96/ 3200]\n",
            "loss: 1.093680  [  112/ 3200]\n",
            "loss: 1.008699  [  128/ 3200]\n",
            "loss: 0.728282  [  144/ 3200]\n",
            "loss: 0.700305  [  160/ 3200]\n",
            "loss: 1.245401  [  176/ 3200]\n",
            "loss: 0.947729  [  192/ 3200]\n",
            "loss: 1.256335  [  208/ 3200]\n",
            "loss: 0.826969  [  224/ 3200]\n",
            "loss: 0.810147  [  240/ 3200]\n",
            "loss: 1.332062  [  256/ 3200]\n",
            "loss: 0.772350  [  272/ 3200]\n",
            "loss: 0.655864  [  288/ 3200]\n",
            "loss: 0.972138  [  304/ 3200]\n",
            "loss: 0.932930  [  320/ 3200]\n",
            "loss: 0.950015  [  336/ 3200]\n",
            "loss: 0.682787  [  352/ 3200]\n",
            "loss: 0.964390  [  368/ 3200]\n",
            "loss: 1.142701  [  384/ 3200]\n",
            "loss: 1.030371  [  400/ 3200]\n",
            "loss: 0.953696  [  416/ 3200]\n",
            "loss: 0.687870  [  432/ 3200]\n",
            "loss: 0.893651  [  448/ 3200]\n",
            "loss: 1.028491  [  464/ 3200]\n",
            "loss: 1.091456  [  480/ 3200]\n",
            "loss: 0.892150  [  496/ 3200]\n",
            "loss: 1.316446  [  512/ 3200]\n",
            "loss: 0.816699  [  528/ 3200]\n",
            "loss: 1.036015  [  544/ 3200]\n",
            "loss: 0.931985  [  560/ 3200]\n",
            "loss: 0.989865  [  576/ 3200]\n",
            "loss: 1.113671  [  592/ 3200]\n",
            "loss: 0.943534  [  608/ 3200]\n",
            "loss: 0.995041  [  624/ 3200]\n",
            "loss: 0.902530  [  640/ 3200]\n",
            "loss: 0.970765  [  656/ 3200]\n",
            "loss: 0.815442  [  672/ 3200]\n",
            "loss: 0.977145  [  688/ 3200]\n",
            "loss: 1.154767  [  704/ 3200]\n",
            "loss: 1.088435  [  720/ 3200]\n",
            "loss: 1.089376  [  736/ 3200]\n",
            "loss: 0.970828  [  752/ 3200]\n",
            "loss: 0.665442  [  768/ 3200]\n",
            "loss: 0.985700  [  784/ 3200]\n",
            "loss: 1.089592  [  800/ 3200]\n",
            "loss: 0.844921  [  816/ 3200]\n",
            "loss: 1.129005  [  832/ 3200]\n",
            "loss: 1.415027  [  848/ 3200]\n",
            "loss: 0.757656  [  864/ 3200]\n",
            "loss: 0.929606  [  880/ 3200]\n",
            "loss: 0.997958  [  896/ 3200]\n",
            "loss: 0.807910  [  912/ 3200]\n",
            "loss: 0.916642  [  928/ 3200]\n",
            "loss: 0.982041  [  944/ 3200]\n",
            "loss: 0.961869  [  960/ 3200]\n",
            "loss: 1.142794  [  976/ 3200]\n",
            "loss: 0.853044  [  992/ 3200]\n",
            "loss: 0.998556  [ 1008/ 3200]\n",
            "loss: 0.755442  [ 1024/ 3200]\n",
            "loss: 0.809318  [ 1040/ 3200]\n",
            "loss: 1.707821  [ 1056/ 3200]\n",
            "loss: 0.872522  [ 1072/ 3200]\n",
            "loss: 1.025461  [ 1088/ 3200]\n",
            "loss: 0.901556  [ 1104/ 3200]\n",
            "loss: 1.164509  [ 1120/ 3200]\n",
            "loss: 0.580551  [ 1136/ 3200]\n",
            "loss: 1.093639  [ 1152/ 3200]\n",
            "loss: 1.116938  [ 1168/ 3200]\n",
            "loss: 1.192786  [ 1184/ 3200]\n",
            "loss: 0.810700  [ 1200/ 3200]\n",
            "loss: 1.424736  [ 1216/ 3200]\n",
            "loss: 0.960162  [ 1232/ 3200]\n",
            "loss: 0.787122  [ 1248/ 3200]\n",
            "loss: 0.967984  [ 1264/ 3200]\n",
            "loss: 1.016790  [ 1280/ 3200]\n",
            "loss: 0.971509  [ 1296/ 3200]\n",
            "loss: 0.902559  [ 1312/ 3200]\n",
            "loss: 1.032341  [ 1328/ 3200]\n",
            "loss: 1.073839  [ 1344/ 3200]\n",
            "loss: 1.084159  [ 1360/ 3200]\n",
            "loss: 0.874607  [ 1376/ 3200]\n",
            "loss: 1.083912  [ 1392/ 3200]\n",
            "loss: 1.037105  [ 1408/ 3200]\n",
            "loss: 0.717140  [ 1424/ 3200]\n",
            "loss: 0.670925  [ 1440/ 3200]\n",
            "loss: 1.193519  [ 1456/ 3200]\n",
            "loss: 1.327809  [ 1472/ 3200]\n",
            "loss: 0.959326  [ 1488/ 3200]\n",
            "loss: 0.879712  [ 1504/ 3200]\n",
            "loss: 0.687708  [ 1520/ 3200]\n",
            "loss: 1.108862  [ 1536/ 3200]\n",
            "loss: 0.860464  [ 1552/ 3200]\n",
            "loss: 1.005860  [ 1568/ 3200]\n",
            "loss: 0.917563  [ 1584/ 3200]\n",
            "loss: 1.221075  [ 1600/ 3200]\n",
            "loss: 0.711037  [ 1616/ 3200]\n",
            "loss: 1.234558  [ 1632/ 3200]\n",
            "loss: 0.878861  [ 1648/ 3200]\n",
            "loss: 0.948150  [ 1664/ 3200]\n",
            "loss: 0.905428  [ 1680/ 3200]\n",
            "loss: 1.099510  [ 1696/ 3200]\n",
            "loss: 0.710298  [ 1712/ 3200]\n",
            "loss: 0.832195  [ 1728/ 3200]\n",
            "loss: 1.146799  [ 1744/ 3200]\n",
            "loss: 0.894485  [ 1760/ 3200]\n",
            "loss: 1.139796  [ 1776/ 3200]\n",
            "loss: 1.135194  [ 1792/ 3200]\n",
            "loss: 0.747687  [ 1808/ 3200]\n",
            "loss: 0.814915  [ 1824/ 3200]\n",
            "loss: 1.163351  [ 1840/ 3200]\n",
            "loss: 0.834893  [ 1856/ 3200]\n",
            "loss: 0.957359  [ 1872/ 3200]\n",
            "loss: 0.978393  [ 1888/ 3200]\n",
            "loss: 1.056186  [ 1904/ 3200]\n",
            "loss: 1.066241  [ 1920/ 3200]\n",
            "loss: 1.528651  [ 1936/ 3200]\n",
            "loss: 1.290162  [ 1952/ 3200]\n",
            "loss: 0.810664  [ 1968/ 3200]\n",
            "loss: 1.129160  [ 1984/ 3200]\n",
            "loss: 1.001578  [ 2000/ 3200]\n",
            "loss: 1.223154  [ 2016/ 3200]\n",
            "loss: 1.396717  [ 2032/ 3200]\n",
            "loss: 0.868190  [ 2048/ 3200]\n",
            "loss: 0.846466  [ 2064/ 3200]\n",
            "loss: 0.976537  [ 2080/ 3200]\n",
            "loss: 0.979733  [ 2096/ 3200]\n",
            "loss: 1.174672  [ 2112/ 3200]\n",
            "loss: 1.213443  [ 2128/ 3200]\n",
            "loss: 1.098488  [ 2144/ 3200]\n",
            "loss: 1.115147  [ 2160/ 3200]\n",
            "loss: 0.814020  [ 2176/ 3200]\n",
            "loss: 0.793738  [ 2192/ 3200]\n",
            "loss: 0.700806  [ 2208/ 3200]\n",
            "loss: 1.048448  [ 2224/ 3200]\n",
            "loss: 0.964304  [ 2240/ 3200]\n",
            "loss: 0.940387  [ 2256/ 3200]\n",
            "loss: 1.206699  [ 2272/ 3200]\n",
            "loss: 0.794492  [ 2288/ 3200]\n",
            "loss: 0.789662  [ 2304/ 3200]\n",
            "loss: 0.870072  [ 2320/ 3200]\n",
            "loss: 1.111311  [ 2336/ 3200]\n",
            "loss: 1.191605  [ 2352/ 3200]\n",
            "loss: 1.010478  [ 2368/ 3200]\n",
            "loss: 0.942876  [ 2384/ 3200]\n",
            "loss: 0.858664  [ 2400/ 3200]\n",
            "loss: 0.728890  [ 2416/ 3200]\n",
            "loss: 0.808603  [ 2432/ 3200]\n",
            "loss: 1.167530  [ 2448/ 3200]\n",
            "loss: 1.082639  [ 2464/ 3200]\n",
            "loss: 1.051048  [ 2480/ 3200]\n",
            "loss: 0.830706  [ 2496/ 3200]\n",
            "loss: 1.021094  [ 2512/ 3200]\n",
            "loss: 1.004061  [ 2528/ 3200]\n",
            "loss: 1.036883  [ 2544/ 3200]\n",
            "loss: 0.863576  [ 2560/ 3200]\n",
            "loss: 1.014951  [ 2576/ 3200]\n",
            "loss: 1.145452  [ 2592/ 3200]\n",
            "loss: 1.049953  [ 2608/ 3200]\n",
            "loss: 0.796164  [ 2624/ 3200]\n",
            "loss: 0.856623  [ 2640/ 3200]\n",
            "loss: 0.943639  [ 2656/ 3200]\n",
            "loss: 1.080441  [ 2672/ 3200]\n",
            "loss: 0.770261  [ 2688/ 3200]\n",
            "loss: 1.070989  [ 2704/ 3200]\n",
            "loss: 1.081582  [ 2720/ 3200]\n",
            "loss: 0.777655  [ 2736/ 3200]\n",
            "loss: 0.767751  [ 2752/ 3200]\n",
            "loss: 1.363717  [ 2768/ 3200]\n",
            "loss: 0.949816  [ 2784/ 3200]\n",
            "loss: 1.296699  [ 2800/ 3200]\n",
            "loss: 0.773394  [ 2816/ 3200]\n",
            "loss: 0.943166  [ 2832/ 3200]\n",
            "loss: 0.987632  [ 2848/ 3200]\n",
            "loss: 1.104319  [ 2864/ 3200]\n",
            "loss: 0.899572  [ 2880/ 3200]\n",
            "loss: 0.546846  [ 2896/ 3200]\n",
            "loss: 0.829129  [ 2912/ 3200]\n",
            "loss: 0.900015  [ 2928/ 3200]\n",
            "loss: 0.783903  [ 2944/ 3200]\n",
            "loss: 0.727035  [ 2960/ 3200]\n",
            "loss: 1.006611  [ 2976/ 3200]\n",
            "loss: 0.813940  [ 2992/ 3200]\n",
            "loss: 0.959488  [ 3008/ 3200]\n",
            "loss: 1.027839  [ 3024/ 3200]\n",
            "loss: 0.876216  [ 3040/ 3200]\n",
            "loss: 0.803533  [ 3056/ 3200]\n",
            "loss: 0.985039  [ 3072/ 3200]\n",
            "loss: 0.766867  [ 3088/ 3200]\n",
            "loss: 1.039950  [ 3104/ 3200]\n",
            "loss: 1.063869  [ 3120/ 3200]\n",
            "loss: 1.272622  [ 3136/ 3200]\n",
            "loss: 0.930713  [ 3152/ 3200]\n",
            "loss: 1.164516  [ 3168/ 3200]\n",
            "loss: 1.316319  [ 3184/ 3200]\n",
            "Avg Accuracy: 53.875000%, Avg loss: 0.965470\n",
            "F1 score is: 0.4798160123825073\n",
            "Confusion Matrix:\n",
            "[[131  62   0   7]\n",
            " [ 27  57   3 113]\n",
            " [  6  92  64  38]\n",
            " [  1  19   1 179]]\n",
            "current epoch: 22\n",
            "\n",
            "loss: 1.092501  [    0/ 3200]\n",
            "loss: 0.962550  [   16/ 3200]\n",
            "loss: 0.903237  [   32/ 3200]\n",
            "loss: 0.858995  [   48/ 3200]\n",
            "loss: 1.149937  [   64/ 3200]\n",
            "loss: 0.762539  [   80/ 3200]\n",
            "loss: 1.080763  [   96/ 3200]\n",
            "loss: 1.259013  [  112/ 3200]\n",
            "loss: 1.098649  [  128/ 3200]\n",
            "loss: 0.962632  [  144/ 3200]\n",
            "loss: 0.973729  [  160/ 3200]\n",
            "loss: 0.663797  [  176/ 3200]\n",
            "loss: 0.973663  [  192/ 3200]\n",
            "loss: 1.265031  [  208/ 3200]\n",
            "loss: 1.058184  [  224/ 3200]\n",
            "loss: 0.655849  [  240/ 3200]\n",
            "loss: 0.809053  [  256/ 3200]\n",
            "loss: 0.906134  [  272/ 3200]\n",
            "loss: 1.082681  [  288/ 3200]\n",
            "loss: 0.825517  [  304/ 3200]\n",
            "loss: 0.861657  [  320/ 3200]\n",
            "loss: 1.328690  [  336/ 3200]\n",
            "loss: 0.911008  [  352/ 3200]\n",
            "loss: 0.699687  [  368/ 3200]\n",
            "loss: 1.020874  [  384/ 3200]\n",
            "loss: 0.965572  [  400/ 3200]\n",
            "loss: 0.971977  [  416/ 3200]\n",
            "loss: 0.787664  [  432/ 3200]\n",
            "loss: 1.149561  [  448/ 3200]\n",
            "loss: 1.001492  [  464/ 3200]\n",
            "loss: 1.047829  [  480/ 3200]\n",
            "loss: 0.937306  [  496/ 3200]\n",
            "loss: 0.825079  [  512/ 3200]\n",
            "loss: 1.109428  [  528/ 3200]\n",
            "loss: 0.766147  [  544/ 3200]\n",
            "loss: 1.073616  [  560/ 3200]\n",
            "loss: 0.872116  [  576/ 3200]\n",
            "loss: 0.869512  [  592/ 3200]\n",
            "loss: 1.076881  [  608/ 3200]\n",
            "loss: 0.885131  [  624/ 3200]\n",
            "loss: 1.116479  [  640/ 3200]\n",
            "loss: 0.656083  [  656/ 3200]\n",
            "loss: 0.707327  [  672/ 3200]\n",
            "loss: 1.075863  [  688/ 3200]\n",
            "loss: 0.804801  [  704/ 3200]\n",
            "loss: 0.914342  [  720/ 3200]\n",
            "loss: 1.011825  [  736/ 3200]\n",
            "loss: 0.962030  [  752/ 3200]\n",
            "loss: 0.960590  [  768/ 3200]\n",
            "loss: 0.958394  [  784/ 3200]\n",
            "loss: 0.800951  [  800/ 3200]\n",
            "loss: 1.058761  [  816/ 3200]\n",
            "loss: 1.169027  [  832/ 3200]\n",
            "loss: 1.077065  [  848/ 3200]\n",
            "loss: 1.044624  [  864/ 3200]\n",
            "loss: 1.052685  [  880/ 3200]\n",
            "loss: 1.054225  [  896/ 3200]\n",
            "loss: 0.995560  [  912/ 3200]\n",
            "loss: 0.940932  [  928/ 3200]\n",
            "loss: 0.902741  [  944/ 3200]\n",
            "loss: 1.099689  [  960/ 3200]\n",
            "loss: 0.677157  [  976/ 3200]\n",
            "loss: 1.032861  [  992/ 3200]\n",
            "loss: 1.018914  [ 1008/ 3200]\n",
            "loss: 1.264035  [ 1024/ 3200]\n",
            "loss: 0.836266  [ 1040/ 3200]\n",
            "loss: 0.922164  [ 1056/ 3200]\n",
            "loss: 0.968350  [ 1072/ 3200]\n",
            "loss: 1.055449  [ 1088/ 3200]\n",
            "loss: 1.032350  [ 1104/ 3200]\n",
            "loss: 1.012627  [ 1120/ 3200]\n",
            "loss: 1.000849  [ 1136/ 3200]\n",
            "loss: 0.752396  [ 1152/ 3200]\n",
            "loss: 1.161208  [ 1168/ 3200]\n",
            "loss: 0.936724  [ 1184/ 3200]\n",
            "loss: 1.060749  [ 1200/ 3200]\n",
            "loss: 1.459196  [ 1216/ 3200]\n",
            "loss: 0.842193  [ 1232/ 3200]\n",
            "loss: 0.975696  [ 1248/ 3200]\n",
            "loss: 1.074508  [ 1264/ 3200]\n",
            "loss: 1.013488  [ 1280/ 3200]\n",
            "loss: 0.938399  [ 1296/ 3200]\n",
            "loss: 1.009515  [ 1312/ 3200]\n",
            "loss: 0.617989  [ 1328/ 3200]\n",
            "loss: 0.827955  [ 1344/ 3200]\n",
            "loss: 0.946459  [ 1360/ 3200]\n",
            "loss: 0.831447  [ 1376/ 3200]\n",
            "loss: 0.841482  [ 1392/ 3200]\n",
            "loss: 0.875568  [ 1408/ 3200]\n",
            "loss: 0.868291  [ 1424/ 3200]\n",
            "loss: 0.997863  [ 1440/ 3200]\n",
            "loss: 0.724412  [ 1456/ 3200]\n",
            "loss: 1.056591  [ 1472/ 3200]\n",
            "loss: 0.947130  [ 1488/ 3200]\n",
            "loss: 1.020783  [ 1504/ 3200]\n",
            "loss: 1.009099  [ 1520/ 3200]\n",
            "loss: 0.831106  [ 1536/ 3200]\n",
            "loss: 0.957843  [ 1552/ 3200]\n",
            "loss: 1.115618  [ 1568/ 3200]\n",
            "loss: 1.283031  [ 1584/ 3200]\n",
            "loss: 0.886366  [ 1600/ 3200]\n",
            "loss: 0.903426  [ 1616/ 3200]\n",
            "loss: 1.197992  [ 1632/ 3200]\n",
            "loss: 0.984907  [ 1648/ 3200]\n",
            "loss: 1.153817  [ 1664/ 3200]\n",
            "loss: 0.731162  [ 1680/ 3200]\n",
            "loss: 0.838134  [ 1696/ 3200]\n",
            "loss: 0.822596  [ 1712/ 3200]\n",
            "loss: 0.911305  [ 1728/ 3200]\n",
            "loss: 0.787229  [ 1744/ 3200]\n",
            "loss: 1.236650  [ 1760/ 3200]\n",
            "loss: 0.842269  [ 1776/ 3200]\n",
            "loss: 0.849237  [ 1792/ 3200]\n",
            "loss: 0.864104  [ 1808/ 3200]\n",
            "loss: 0.771594  [ 1824/ 3200]\n",
            "loss: 0.823712  [ 1840/ 3200]\n",
            "loss: 1.039917  [ 1856/ 3200]\n",
            "loss: 0.967916  [ 1872/ 3200]\n",
            "loss: 0.895310  [ 1888/ 3200]\n",
            "loss: 0.966184  [ 1904/ 3200]\n",
            "loss: 1.201777  [ 1920/ 3200]\n",
            "loss: 1.089052  [ 1936/ 3200]\n",
            "loss: 1.081336  [ 1952/ 3200]\n",
            "loss: 1.035525  [ 1968/ 3200]\n",
            "loss: 1.236601  [ 1984/ 3200]\n",
            "loss: 0.937431  [ 2000/ 3200]\n",
            "loss: 1.106266  [ 2016/ 3200]\n",
            "loss: 0.893412  [ 2032/ 3200]\n",
            "loss: 1.197310  [ 2048/ 3200]\n",
            "loss: 0.804027  [ 2064/ 3200]\n",
            "loss: 0.950752  [ 2080/ 3200]\n",
            "loss: 0.807614  [ 2096/ 3200]\n",
            "loss: 0.966808  [ 2112/ 3200]\n",
            "loss: 0.792594  [ 2128/ 3200]\n",
            "loss: 0.692496  [ 2144/ 3200]\n",
            "loss: 0.803105  [ 2160/ 3200]\n",
            "loss: 0.981629  [ 2176/ 3200]\n",
            "loss: 0.955861  [ 2192/ 3200]\n",
            "loss: 0.860940  [ 2208/ 3200]\n",
            "loss: 0.942065  [ 2224/ 3200]\n",
            "loss: 0.966061  [ 2240/ 3200]\n",
            "loss: 0.641134  [ 2256/ 3200]\n",
            "loss: 1.041181  [ 2272/ 3200]\n",
            "loss: 1.090747  [ 2288/ 3200]\n",
            "loss: 1.374140  [ 2304/ 3200]\n",
            "loss: 1.167443  [ 2320/ 3200]\n",
            "loss: 1.177639  [ 2336/ 3200]\n",
            "loss: 1.183525  [ 2352/ 3200]\n",
            "loss: 0.932426  [ 2368/ 3200]\n",
            "loss: 0.860385  [ 2384/ 3200]\n",
            "loss: 0.771122  [ 2400/ 3200]\n",
            "loss: 0.914414  [ 2416/ 3200]\n",
            "loss: 1.103599  [ 2432/ 3200]\n",
            "loss: 0.758179  [ 2448/ 3200]\n",
            "loss: 1.026574  [ 2464/ 3200]\n",
            "loss: 0.856076  [ 2480/ 3200]\n",
            "loss: 0.980570  [ 2496/ 3200]\n",
            "loss: 1.221064  [ 2512/ 3200]\n",
            "loss: 1.036014  [ 2528/ 3200]\n",
            "loss: 1.315053  [ 2544/ 3200]\n",
            "loss: 1.172859  [ 2560/ 3200]\n",
            "loss: 0.921600  [ 2576/ 3200]\n",
            "loss: 0.723335  [ 2592/ 3200]\n",
            "loss: 1.285039  [ 2608/ 3200]\n",
            "loss: 0.932814  [ 2624/ 3200]\n",
            "loss: 0.902973  [ 2640/ 3200]\n",
            "loss: 0.894597  [ 2656/ 3200]\n",
            "loss: 0.705406  [ 2672/ 3200]\n",
            "loss: 0.741404  [ 2688/ 3200]\n",
            "loss: 0.569295  [ 2704/ 3200]\n",
            "loss: 1.048870  [ 2720/ 3200]\n",
            "loss: 1.065359  [ 2736/ 3200]\n",
            "loss: 0.833342  [ 2752/ 3200]\n",
            "loss: 1.035540  [ 2768/ 3200]\n",
            "loss: 0.993935  [ 2784/ 3200]\n",
            "loss: 0.902874  [ 2800/ 3200]\n",
            "loss: 0.628900  [ 2816/ 3200]\n",
            "loss: 1.017227  [ 2832/ 3200]\n",
            "loss: 0.754594  [ 2848/ 3200]\n",
            "loss: 0.881049  [ 2864/ 3200]\n",
            "loss: 1.121995  [ 2880/ 3200]\n",
            "loss: 0.884925  [ 2896/ 3200]\n",
            "loss: 1.272523  [ 2912/ 3200]\n",
            "loss: 0.861378  [ 2928/ 3200]\n",
            "loss: 0.902744  [ 2944/ 3200]\n",
            "loss: 1.033003  [ 2960/ 3200]\n",
            "loss: 0.995758  [ 2976/ 3200]\n",
            "loss: 0.808480  [ 2992/ 3200]\n",
            "loss: 1.177110  [ 3008/ 3200]\n",
            "loss: 0.956923  [ 3024/ 3200]\n",
            "loss: 1.306453  [ 3040/ 3200]\n",
            "loss: 0.898940  [ 3056/ 3200]\n",
            "loss: 1.058803  [ 3072/ 3200]\n",
            "loss: 0.786321  [ 3088/ 3200]\n",
            "loss: 0.647997  [ 3104/ 3200]\n",
            "loss: 1.042222  [ 3120/ 3200]\n",
            "loss: 1.134725  [ 3136/ 3200]\n",
            "loss: 0.655616  [ 3152/ 3200]\n",
            "loss: 1.280785  [ 3168/ 3200]\n",
            "loss: 0.956077  [ 3184/ 3200]\n",
            "Avg Accuracy: 52.875000%, Avg loss: 1.031783\n",
            "F1 score is: 0.4633187049627304\n",
            "Confusion Matrix:\n",
            "[[191   5   0   4]\n",
            " [ 85 107   0   8]\n",
            " [ 46 106  46   2]\n",
            " [ 24  96   1  79]]\n",
            "current epoch: 23\n",
            "\n",
            "loss: 1.341246  [    0/ 3200]\n",
            "loss: 0.911786  [   16/ 3200]\n",
            "loss: 1.246854  [   32/ 3200]\n",
            "loss: 1.048471  [   48/ 3200]\n",
            "loss: 0.852182  [   64/ 3200]\n",
            "loss: 1.095604  [   80/ 3200]\n",
            "loss: 1.249502  [   96/ 3200]\n",
            "loss: 0.910711  [  112/ 3200]\n",
            "loss: 0.805469  [  128/ 3200]\n",
            "loss: 0.721421  [  144/ 3200]\n",
            "loss: 1.076252  [  160/ 3200]\n",
            "loss: 0.795411  [  176/ 3200]\n",
            "loss: 0.905361  [  192/ 3200]\n",
            "loss: 0.924882  [  208/ 3200]\n",
            "loss: 0.734469  [  224/ 3200]\n",
            "loss: 0.731805  [  240/ 3200]\n",
            "loss: 0.927521  [  256/ 3200]\n",
            "loss: 1.219090  [  272/ 3200]\n",
            "loss: 0.935753  [  288/ 3200]\n",
            "loss: 0.929061  [  304/ 3200]\n",
            "loss: 0.996021  [  320/ 3200]\n",
            "loss: 1.269298  [  336/ 3200]\n",
            "loss: 1.090788  [  352/ 3200]\n",
            "loss: 1.212326  [  368/ 3200]\n",
            "loss: 1.184971  [  384/ 3200]\n",
            "loss: 1.113229  [  400/ 3200]\n",
            "loss: 0.871685  [  416/ 3200]\n",
            "loss: 0.870279  [  432/ 3200]\n",
            "loss: 1.090806  [  448/ 3200]\n",
            "loss: 0.999548  [  464/ 3200]\n",
            "loss: 1.160696  [  480/ 3200]\n",
            "loss: 0.991826  [  496/ 3200]\n",
            "loss: 1.485081  [  512/ 3200]\n",
            "loss: 1.059050  [  528/ 3200]\n",
            "loss: 1.006446  [  544/ 3200]\n",
            "loss: 1.109699  [  560/ 3200]\n",
            "loss: 0.940795  [  576/ 3200]\n",
            "loss: 0.768434  [  592/ 3200]\n",
            "loss: 1.022071  [  608/ 3200]\n",
            "loss: 0.692353  [  624/ 3200]\n",
            "loss: 0.797454  [  640/ 3200]\n",
            "loss: 0.815145  [  656/ 3200]\n",
            "loss: 0.873926  [  672/ 3200]\n",
            "loss: 0.893565  [  688/ 3200]\n",
            "loss: 0.921200  [  704/ 3200]\n",
            "loss: 1.036589  [  720/ 3200]\n",
            "loss: 1.052263  [  736/ 3200]\n",
            "loss: 0.942111  [  752/ 3200]\n",
            "loss: 0.720143  [  768/ 3200]\n",
            "loss: 0.810803  [  784/ 3200]\n",
            "loss: 0.991976  [  800/ 3200]\n",
            "loss: 0.964032  [  816/ 3200]\n",
            "loss: 1.254599  [  832/ 3200]\n",
            "loss: 0.993224  [  848/ 3200]\n",
            "loss: 0.944894  [  864/ 3200]\n",
            "loss: 0.903501  [  880/ 3200]\n",
            "loss: 0.951384  [  896/ 3200]\n",
            "loss: 0.928199  [  912/ 3200]\n",
            "loss: 0.846105  [  928/ 3200]\n",
            "loss: 0.838383  [  944/ 3200]\n",
            "loss: 1.064562  [  960/ 3200]\n",
            "loss: 1.052052  [  976/ 3200]\n",
            "loss: 0.850954  [  992/ 3200]\n",
            "loss: 0.870800  [ 1008/ 3200]\n",
            "loss: 0.991751  [ 1024/ 3200]\n",
            "loss: 0.781974  [ 1040/ 3200]\n",
            "loss: 0.788788  [ 1056/ 3200]\n",
            "loss: 0.895209  [ 1072/ 3200]\n",
            "loss: 1.089699  [ 1088/ 3200]\n",
            "loss: 0.691998  [ 1104/ 3200]\n",
            "loss: 0.782782  [ 1120/ 3200]\n",
            "loss: 0.947614  [ 1136/ 3200]\n",
            "loss: 0.902168  [ 1152/ 3200]\n",
            "loss: 0.732753  [ 1168/ 3200]\n",
            "loss: 0.975577  [ 1184/ 3200]\n",
            "loss: 0.756670  [ 1200/ 3200]\n",
            "loss: 0.994446  [ 1216/ 3200]\n",
            "loss: 1.074048  [ 1232/ 3200]\n",
            "loss: 0.884024  [ 1248/ 3200]\n",
            "loss: 0.779987  [ 1264/ 3200]\n",
            "loss: 0.910458  [ 1280/ 3200]\n",
            "loss: 1.011611  [ 1296/ 3200]\n",
            "loss: 0.983889  [ 1312/ 3200]\n",
            "loss: 0.807847  [ 1328/ 3200]\n",
            "loss: 0.971049  [ 1344/ 3200]\n",
            "loss: 0.643420  [ 1360/ 3200]\n",
            "loss: 1.433722  [ 1376/ 3200]\n",
            "loss: 0.806341  [ 1392/ 3200]\n",
            "loss: 1.237700  [ 1408/ 3200]\n",
            "loss: 0.968791  [ 1424/ 3200]\n",
            "loss: 0.941269  [ 1440/ 3200]\n",
            "loss: 1.042137  [ 1456/ 3200]\n",
            "loss: 0.998157  [ 1472/ 3200]\n",
            "loss: 0.838056  [ 1488/ 3200]\n",
            "loss: 1.003244  [ 1504/ 3200]\n",
            "loss: 1.129611  [ 1520/ 3200]\n",
            "loss: 1.020354  [ 1536/ 3200]\n",
            "loss: 0.855343  [ 1552/ 3200]\n",
            "loss: 1.147427  [ 1568/ 3200]\n",
            "loss: 0.631523  [ 1584/ 3200]\n",
            "loss: 0.895375  [ 1600/ 3200]\n",
            "loss: 0.846281  [ 1616/ 3200]\n",
            "loss: 1.134676  [ 1632/ 3200]\n",
            "loss: 0.950265  [ 1648/ 3200]\n",
            "loss: 0.961748  [ 1664/ 3200]\n",
            "loss: 0.759614  [ 1680/ 3200]\n",
            "loss: 1.074085  [ 1696/ 3200]\n",
            "loss: 0.741721  [ 1712/ 3200]\n",
            "loss: 1.162991  [ 1728/ 3200]\n",
            "loss: 0.660570  [ 1744/ 3200]\n",
            "loss: 0.843729  [ 1760/ 3200]\n",
            "loss: 1.196642  [ 1776/ 3200]\n",
            "loss: 0.792558  [ 1792/ 3200]\n",
            "loss: 0.878676  [ 1808/ 3200]\n",
            "loss: 0.913774  [ 1824/ 3200]\n",
            "loss: 0.722801  [ 1840/ 3200]\n",
            "loss: 0.927048  [ 1856/ 3200]\n",
            "loss: 0.792042  [ 1872/ 3200]\n",
            "loss: 0.704279  [ 1888/ 3200]\n",
            "loss: 0.758587  [ 1904/ 3200]\n",
            "loss: 1.123051  [ 1920/ 3200]\n",
            "loss: 0.812426  [ 1936/ 3200]\n",
            "loss: 1.123301  [ 1952/ 3200]\n",
            "loss: 1.187663  [ 1968/ 3200]\n",
            "loss: 0.800490  [ 1984/ 3200]\n",
            "loss: 0.685865  [ 2000/ 3200]\n",
            "loss: 1.115715  [ 2016/ 3200]\n",
            "loss: 0.699188  [ 2032/ 3200]\n",
            "loss: 0.759317  [ 2048/ 3200]\n",
            "loss: 0.970568  [ 2064/ 3200]\n",
            "loss: 1.249849  [ 2080/ 3200]\n",
            "loss: 0.872243  [ 2096/ 3200]\n",
            "loss: 1.388648  [ 2112/ 3200]\n",
            "loss: 0.956498  [ 2128/ 3200]\n",
            "loss: 0.783881  [ 2144/ 3200]\n",
            "loss: 0.955462  [ 2160/ 3200]\n",
            "loss: 0.963800  [ 2176/ 3200]\n",
            "loss: 0.935842  [ 2192/ 3200]\n",
            "loss: 1.219534  [ 2208/ 3200]\n",
            "loss: 0.912214  [ 2224/ 3200]\n",
            "loss: 0.827836  [ 2240/ 3200]\n",
            "loss: 0.973883  [ 2256/ 3200]\n",
            "loss: 1.110298  [ 2272/ 3200]\n",
            "loss: 0.980705  [ 2288/ 3200]\n",
            "loss: 0.915452  [ 2304/ 3200]\n",
            "loss: 0.861529  [ 2320/ 3200]\n",
            "loss: 0.809464  [ 2336/ 3200]\n",
            "loss: 0.897740  [ 2352/ 3200]\n",
            "loss: 0.985486  [ 2368/ 3200]\n",
            "loss: 1.277778  [ 2384/ 3200]\n",
            "loss: 0.927504  [ 2400/ 3200]\n",
            "loss: 0.932300  [ 2416/ 3200]\n",
            "loss: 1.246028  [ 2432/ 3200]\n",
            "loss: 1.160929  [ 2448/ 3200]\n",
            "loss: 1.203890  [ 2464/ 3200]\n",
            "loss: 0.824684  [ 2480/ 3200]\n",
            "loss: 0.892789  [ 2496/ 3200]\n",
            "loss: 0.804821  [ 2512/ 3200]\n",
            "loss: 1.134944  [ 2528/ 3200]\n",
            "loss: 0.819890  [ 2544/ 3200]\n",
            "loss: 0.754270  [ 2560/ 3200]\n",
            "loss: 0.860239  [ 2576/ 3200]\n",
            "loss: 1.125124  [ 2592/ 3200]\n",
            "loss: 0.903797  [ 2608/ 3200]\n",
            "loss: 0.833529  [ 2624/ 3200]\n",
            "loss: 0.845404  [ 2640/ 3200]\n",
            "loss: 1.076490  [ 2656/ 3200]\n",
            "loss: 1.172345  [ 2672/ 3200]\n",
            "loss: 0.798309  [ 2688/ 3200]\n",
            "loss: 1.239618  [ 2704/ 3200]\n",
            "loss: 0.852736  [ 2720/ 3200]\n",
            "loss: 1.328004  [ 2736/ 3200]\n",
            "loss: 0.760498  [ 2752/ 3200]\n",
            "loss: 0.980965  [ 2768/ 3200]\n",
            "loss: 0.892989  [ 2784/ 3200]\n",
            "loss: 1.169557  [ 2800/ 3200]\n",
            "loss: 0.803910  [ 2816/ 3200]\n",
            "loss: 1.166127  [ 2832/ 3200]\n",
            "loss: 1.297352  [ 2848/ 3200]\n",
            "loss: 1.009958  [ 2864/ 3200]\n",
            "loss: 1.103303  [ 2880/ 3200]\n",
            "loss: 1.059089  [ 2896/ 3200]\n",
            "loss: 0.814326  [ 2912/ 3200]\n",
            "loss: 0.904814  [ 2928/ 3200]\n",
            "loss: 0.895145  [ 2944/ 3200]\n",
            "loss: 0.703982  [ 2960/ 3200]\n",
            "loss: 1.150980  [ 2976/ 3200]\n",
            "loss: 0.996572  [ 2992/ 3200]\n",
            "loss: 1.005211  [ 3008/ 3200]\n",
            "loss: 0.769333  [ 3024/ 3200]\n",
            "loss: 0.975612  [ 3040/ 3200]\n",
            "loss: 0.697872  [ 3056/ 3200]\n",
            "loss: 0.691190  [ 3072/ 3200]\n",
            "loss: 0.800745  [ 3088/ 3200]\n",
            "loss: 1.530768  [ 3104/ 3200]\n",
            "loss: 1.488546  [ 3120/ 3200]\n",
            "loss: 0.842441  [ 3136/ 3200]\n",
            "loss: 1.066089  [ 3152/ 3200]\n",
            "loss: 0.745876  [ 3168/ 3200]\n",
            "loss: 0.784372  [ 3184/ 3200]\n",
            "Avg Accuracy: 66.500000%, Avg loss: 0.897151\n",
            "F1 score is: 0.6180130827426911\n",
            "Confusion Matrix:\n",
            "[[165  23   7   5]\n",
            " [ 34  56  26  84]\n",
            " [ 11  20 150  19]\n",
            " [  2  25  12 161]]\n",
            "current epoch: 24\n",
            "\n",
            "loss: 0.988887  [    0/ 3200]\n",
            "loss: 0.872341  [   16/ 3200]\n",
            "loss: 1.114236  [   32/ 3200]\n",
            "loss: 0.995075  [   48/ 3200]\n",
            "loss: 0.833793  [   64/ 3200]\n",
            "loss: 0.603022  [   80/ 3200]\n",
            "loss: 0.706011  [   96/ 3200]\n",
            "loss: 0.924562  [  112/ 3200]\n",
            "loss: 0.922575  [  128/ 3200]\n",
            "loss: 1.213256  [  144/ 3200]\n",
            "loss: 0.982565  [  160/ 3200]\n",
            "loss: 0.901504  [  176/ 3200]\n",
            "loss: 0.988131  [  192/ 3200]\n",
            "loss: 0.976475  [  208/ 3200]\n",
            "loss: 1.194776  [  224/ 3200]\n",
            "loss: 0.806884  [  240/ 3200]\n",
            "loss: 0.846088  [  256/ 3200]\n",
            "loss: 0.874589  [  272/ 3200]\n",
            "loss: 0.903034  [  288/ 3200]\n",
            "loss: 0.923929  [  304/ 3200]\n",
            "loss: 0.874463  [  320/ 3200]\n",
            "loss: 1.374901  [  336/ 3200]\n",
            "loss: 1.261869  [  352/ 3200]\n",
            "loss: 0.884774  [  368/ 3200]\n",
            "loss: 0.810604  [  384/ 3200]\n",
            "loss: 0.908948  [  400/ 3200]\n",
            "loss: 0.821337  [  416/ 3200]\n",
            "loss: 1.262534  [  432/ 3200]\n",
            "loss: 1.030542  [  448/ 3200]\n",
            "loss: 0.738509  [  464/ 3200]\n",
            "loss: 0.861525  [  480/ 3200]\n",
            "loss: 0.845428  [  496/ 3200]\n",
            "loss: 1.133880  [  512/ 3200]\n",
            "loss: 1.030245  [  528/ 3200]\n",
            "loss: 0.903271  [  544/ 3200]\n",
            "loss: 0.764479  [  560/ 3200]\n",
            "loss: 0.924425  [  576/ 3200]\n",
            "loss: 1.009024  [  592/ 3200]\n",
            "loss: 0.813666  [  608/ 3200]\n",
            "loss: 1.119577  [  624/ 3200]\n",
            "loss: 1.012801  [  640/ 3200]\n",
            "loss: 1.120831  [  656/ 3200]\n",
            "loss: 1.104796  [  672/ 3200]\n",
            "loss: 0.927826  [  688/ 3200]\n",
            "loss: 1.109483  [  704/ 3200]\n",
            "loss: 0.853886  [  720/ 3200]\n",
            "loss: 0.941490  [  736/ 3200]\n",
            "loss: 1.225785  [  752/ 3200]\n",
            "loss: 0.575402  [  768/ 3200]\n",
            "loss: 1.038831  [  784/ 3200]\n",
            "loss: 0.882003  [  800/ 3200]\n",
            "loss: 0.935425  [  816/ 3200]\n",
            "loss: 0.836601  [  832/ 3200]\n",
            "loss: 1.074357  [  848/ 3200]\n",
            "loss: 0.888250  [  864/ 3200]\n",
            "loss: 0.746273  [  880/ 3200]\n",
            "loss: 1.052949  [  896/ 3200]\n",
            "loss: 0.965634  [  912/ 3200]\n",
            "loss: 1.029710  [  928/ 3200]\n",
            "loss: 0.986926  [  944/ 3200]\n",
            "loss: 0.989563  [  960/ 3200]\n",
            "loss: 0.853495  [  976/ 3200]\n",
            "loss: 0.899966  [  992/ 3200]\n",
            "loss: 1.027052  [ 1008/ 3200]\n",
            "loss: 0.944996  [ 1024/ 3200]\n",
            "loss: 1.275682  [ 1040/ 3200]\n",
            "loss: 0.966894  [ 1056/ 3200]\n",
            "loss: 0.950181  [ 1072/ 3200]\n",
            "loss: 0.824773  [ 1088/ 3200]\n",
            "loss: 1.021338  [ 1104/ 3200]\n",
            "loss: 1.045581  [ 1120/ 3200]\n",
            "loss: 0.742754  [ 1136/ 3200]\n",
            "loss: 1.007200  [ 1152/ 3200]\n",
            "loss: 0.974966  [ 1168/ 3200]\n",
            "loss: 0.645053  [ 1184/ 3200]\n",
            "loss: 0.734597  [ 1200/ 3200]\n",
            "loss: 0.855141  [ 1216/ 3200]\n",
            "loss: 0.925188  [ 1232/ 3200]\n",
            "loss: 0.890147  [ 1248/ 3200]\n",
            "loss: 1.067901  [ 1264/ 3200]\n",
            "loss: 0.754686  [ 1280/ 3200]\n",
            "loss: 1.112659  [ 1296/ 3200]\n",
            "loss: 0.850765  [ 1312/ 3200]\n",
            "loss: 1.149481  [ 1328/ 3200]\n",
            "loss: 1.008697  [ 1344/ 3200]\n",
            "loss: 1.092927  [ 1360/ 3200]\n",
            "loss: 1.133726  [ 1376/ 3200]\n",
            "loss: 0.904256  [ 1392/ 3200]\n",
            "loss: 1.259563  [ 1408/ 3200]\n",
            "loss: 1.108433  [ 1424/ 3200]\n",
            "loss: 1.031343  [ 1440/ 3200]\n",
            "loss: 0.983254  [ 1456/ 3200]\n",
            "loss: 0.972338  [ 1472/ 3200]\n",
            "loss: 1.103621  [ 1488/ 3200]\n",
            "loss: 0.809211  [ 1504/ 3200]\n",
            "loss: 0.987067  [ 1520/ 3200]\n",
            "loss: 1.132078  [ 1536/ 3200]\n",
            "loss: 0.930951  [ 1552/ 3200]\n",
            "loss: 0.780706  [ 1568/ 3200]\n",
            "loss: 0.841078  [ 1584/ 3200]\n",
            "loss: 0.911247  [ 1600/ 3200]\n",
            "loss: 0.948481  [ 1616/ 3200]\n",
            "loss: 1.028349  [ 1632/ 3200]\n",
            "loss: 0.914571  [ 1648/ 3200]\n",
            "loss: 0.777059  [ 1664/ 3200]\n",
            "loss: 0.896429  [ 1680/ 3200]\n",
            "loss: 1.047426  [ 1696/ 3200]\n",
            "loss: 0.954065  [ 1712/ 3200]\n",
            "loss: 0.890518  [ 1728/ 3200]\n",
            "loss: 1.086248  [ 1744/ 3200]\n",
            "loss: 1.252835  [ 1760/ 3200]\n",
            "loss: 1.316636  [ 1776/ 3200]\n",
            "loss: 0.929170  [ 1792/ 3200]\n",
            "loss: 0.714399  [ 1808/ 3200]\n",
            "loss: 1.116303  [ 1824/ 3200]\n",
            "loss: 0.811197  [ 1840/ 3200]\n",
            "loss: 0.818422  [ 1856/ 3200]\n",
            "loss: 0.847048  [ 1872/ 3200]\n",
            "loss: 0.781296  [ 1888/ 3200]\n",
            "loss: 0.947345  [ 1904/ 3200]\n",
            "loss: 1.084933  [ 1920/ 3200]\n",
            "loss: 0.847483  [ 1936/ 3200]\n",
            "loss: 1.077075  [ 1952/ 3200]\n",
            "loss: 0.837790  [ 1968/ 3200]\n",
            "loss: 1.017512  [ 1984/ 3200]\n",
            "loss: 0.952064  [ 2000/ 3200]\n",
            "loss: 1.083441  [ 2016/ 3200]\n",
            "loss: 0.711821  [ 2032/ 3200]\n",
            "loss: 0.939556  [ 2048/ 3200]\n",
            "loss: 1.171245  [ 2064/ 3200]\n",
            "loss: 1.075969  [ 2080/ 3200]\n",
            "loss: 1.168952  [ 2096/ 3200]\n",
            "loss: 0.782413  [ 2112/ 3200]\n",
            "loss: 0.912851  [ 2128/ 3200]\n",
            "loss: 0.936879  [ 2144/ 3200]\n",
            "loss: 0.911557  [ 2160/ 3200]\n",
            "loss: 1.092448  [ 2176/ 3200]\n",
            "loss: 0.970176  [ 2192/ 3200]\n",
            "loss: 0.899159  [ 2208/ 3200]\n",
            "loss: 0.821958  [ 2224/ 3200]\n",
            "loss: 0.944198  [ 2240/ 3200]\n",
            "loss: 0.865908  [ 2256/ 3200]\n",
            "loss: 1.166753  [ 2272/ 3200]\n",
            "loss: 0.930506  [ 2288/ 3200]\n",
            "loss: 0.969597  [ 2304/ 3200]\n",
            "loss: 0.859520  [ 2320/ 3200]\n",
            "loss: 1.040412  [ 2336/ 3200]\n",
            "loss: 0.964172  [ 2352/ 3200]\n",
            "loss: 0.909555  [ 2368/ 3200]\n",
            "loss: 0.695635  [ 2384/ 3200]\n",
            "loss: 0.780018  [ 2400/ 3200]\n",
            "loss: 0.976905  [ 2416/ 3200]\n",
            "loss: 0.694971  [ 2432/ 3200]\n",
            "loss: 1.329612  [ 2448/ 3200]\n",
            "loss: 1.288353  [ 2464/ 3200]\n",
            "loss: 0.808671  [ 2480/ 3200]\n",
            "loss: 0.650287  [ 2496/ 3200]\n",
            "loss: 0.971954  [ 2512/ 3200]\n",
            "loss: 0.824839  [ 2528/ 3200]\n",
            "loss: 0.845568  [ 2544/ 3200]\n",
            "loss: 1.334484  [ 2560/ 3200]\n",
            "loss: 0.861995  [ 2576/ 3200]\n",
            "loss: 0.944717  [ 2592/ 3200]\n",
            "loss: 0.718604  [ 2608/ 3200]\n",
            "loss: 1.119120  [ 2624/ 3200]\n",
            "loss: 0.850096  [ 2640/ 3200]\n",
            "loss: 0.871177  [ 2656/ 3200]\n",
            "loss: 1.375533  [ 2672/ 3200]\n",
            "loss: 1.082890  [ 2688/ 3200]\n",
            "loss: 0.993989  [ 2704/ 3200]\n",
            "loss: 0.954023  [ 2720/ 3200]\n",
            "loss: 0.986122  [ 2736/ 3200]\n",
            "loss: 0.773127  [ 2752/ 3200]\n",
            "loss: 1.005182  [ 2768/ 3200]\n",
            "loss: 0.930059  [ 2784/ 3200]\n",
            "loss: 1.177147  [ 2800/ 3200]\n",
            "loss: 1.707191  [ 2816/ 3200]\n",
            "loss: 1.170767  [ 2832/ 3200]\n",
            "loss: 0.703478  [ 2848/ 3200]\n",
            "loss: 0.559714  [ 2864/ 3200]\n",
            "loss: 0.890609  [ 2880/ 3200]\n",
            "loss: 0.648774  [ 2896/ 3200]\n",
            "loss: 0.722341  [ 2912/ 3200]\n",
            "loss: 0.996912  [ 2928/ 3200]\n",
            "loss: 1.532884  [ 2944/ 3200]\n",
            "loss: 0.818234  [ 2960/ 3200]\n",
            "loss: 0.581656  [ 2976/ 3200]\n",
            "loss: 0.702984  [ 2992/ 3200]\n",
            "loss: 0.724715  [ 3008/ 3200]\n",
            "loss: 0.762159  [ 3024/ 3200]\n",
            "loss: 0.928514  [ 3040/ 3200]\n",
            "loss: 1.101857  [ 3056/ 3200]\n",
            "loss: 1.221882  [ 3072/ 3200]\n",
            "loss: 0.765567  [ 3088/ 3200]\n",
            "loss: 0.969010  [ 3104/ 3200]\n",
            "loss: 0.715838  [ 3120/ 3200]\n",
            "loss: 0.724445  [ 3136/ 3200]\n",
            "loss: 1.124241  [ 3152/ 3200]\n",
            "loss: 0.940419  [ 3168/ 3200]\n",
            "loss: 0.907833  [ 3184/ 3200]\n",
            "Avg Accuracy: 63.875000%, Avg loss: 0.897303\n",
            "F1 score is: 0.5625949305295944\n",
            "Confusion Matrix:\n",
            "[[142  25  27   6]\n",
            " [ 29  38  43  90]\n",
            " [  8   6 170  16]\n",
            " [  2  13  24 161]]\n",
            "current epoch: 25\n",
            "\n",
            "loss: 0.737226  [    0/ 3200]\n",
            "loss: 0.808783  [   16/ 3200]\n",
            "loss: 1.264177  [   32/ 3200]\n",
            "loss: 0.803518  [   48/ 3200]\n",
            "loss: 0.860286  [   64/ 3200]\n",
            "loss: 0.952540  [   80/ 3200]\n",
            "loss: 1.073047  [   96/ 3200]\n",
            "loss: 0.929798  [  112/ 3200]\n",
            "loss: 0.630082  [  128/ 3200]\n",
            "loss: 0.839840  [  144/ 3200]\n",
            "loss: 0.674593  [  160/ 3200]\n",
            "loss: 1.003760  [  176/ 3200]\n",
            "loss: 0.866388  [  192/ 3200]\n",
            "loss: 1.149863  [  208/ 3200]\n",
            "loss: 0.689132  [  224/ 3200]\n",
            "loss: 0.941415  [  240/ 3200]\n",
            "loss: 0.636638  [  256/ 3200]\n",
            "loss: 0.972027  [  272/ 3200]\n",
            "loss: 0.849398  [  288/ 3200]\n",
            "loss: 0.792657  [  304/ 3200]\n",
            "loss: 0.910501  [  320/ 3200]\n",
            "loss: 1.023820  [  336/ 3200]\n",
            "loss: 0.862038  [  352/ 3200]\n",
            "loss: 0.768623  [  368/ 3200]\n",
            "loss: 1.066771  [  384/ 3200]\n",
            "loss: 0.984703  [  400/ 3200]\n",
            "loss: 0.900859  [  416/ 3200]\n",
            "loss: 1.002394  [  432/ 3200]\n",
            "loss: 0.712962  [  448/ 3200]\n",
            "loss: 1.145554  [  464/ 3200]\n",
            "loss: 0.709112  [  480/ 3200]\n",
            "loss: 0.910562  [  496/ 3200]\n",
            "loss: 0.865770  [  512/ 3200]\n",
            "loss: 1.228200  [  528/ 3200]\n",
            "loss: 1.275857  [  544/ 3200]\n",
            "loss: 0.902447  [  560/ 3200]\n",
            "loss: 0.972285  [  576/ 3200]\n",
            "loss: 0.917145  [  592/ 3200]\n",
            "loss: 0.884629  [  608/ 3200]\n",
            "loss: 1.324091  [  624/ 3200]\n",
            "loss: 1.100873  [  640/ 3200]\n",
            "loss: 1.132841  [  656/ 3200]\n",
            "loss: 0.872462  [  672/ 3200]\n",
            "loss: 0.467990  [  688/ 3200]\n",
            "loss: 1.168355  [  704/ 3200]\n",
            "loss: 1.012606  [  720/ 3200]\n",
            "loss: 0.830900  [  736/ 3200]\n",
            "loss: 1.001977  [  752/ 3200]\n",
            "loss: 1.124599  [  768/ 3200]\n",
            "loss: 0.938655  [  784/ 3200]\n",
            "loss: 0.931776  [  800/ 3200]\n",
            "loss: 0.915237  [  816/ 3200]\n",
            "loss: 1.215847  [  832/ 3200]\n",
            "loss: 0.697846  [  848/ 3200]\n",
            "loss: 1.100609  [  864/ 3200]\n",
            "loss: 0.969266  [  880/ 3200]\n",
            "loss: 0.942571  [  896/ 3200]\n",
            "loss: 0.767689  [  912/ 3200]\n",
            "loss: 0.989684  [  928/ 3200]\n",
            "loss: 0.971325  [  944/ 3200]\n",
            "loss: 0.919348  [  960/ 3200]\n",
            "loss: 0.842646  [  976/ 3200]\n",
            "loss: 0.931570  [  992/ 3200]\n",
            "loss: 0.936354  [ 1008/ 3200]\n",
            "loss: 0.548509  [ 1024/ 3200]\n",
            "loss: 1.148666  [ 1040/ 3200]\n",
            "loss: 0.904807  [ 1056/ 3200]\n",
            "loss: 0.788547  [ 1072/ 3200]\n",
            "loss: 0.958562  [ 1088/ 3200]\n",
            "loss: 0.984161  [ 1104/ 3200]\n",
            "loss: 1.001655  [ 1120/ 3200]\n",
            "loss: 0.779188  [ 1136/ 3200]\n",
            "loss: 0.889359  [ 1152/ 3200]\n",
            "loss: 0.953309  [ 1168/ 3200]\n",
            "loss: 1.147768  [ 1184/ 3200]\n",
            "loss: 1.103548  [ 1200/ 3200]\n",
            "loss: 0.863754  [ 1216/ 3200]\n",
            "loss: 0.861239  [ 1232/ 3200]\n",
            "loss: 0.967082  [ 1248/ 3200]\n",
            "loss: 0.754222  [ 1264/ 3200]\n",
            "loss: 1.044791  [ 1280/ 3200]\n",
            "loss: 0.566961  [ 1296/ 3200]\n",
            "loss: 0.807611  [ 1312/ 3200]\n",
            "loss: 1.157627  [ 1328/ 3200]\n",
            "loss: 1.254701  [ 1344/ 3200]\n",
            "loss: 1.160569  [ 1360/ 3200]\n",
            "loss: 1.048252  [ 1376/ 3200]\n",
            "loss: 1.294395  [ 1392/ 3200]\n",
            "loss: 0.945000  [ 1408/ 3200]\n",
            "loss: 1.013166  [ 1424/ 3200]\n",
            "loss: 1.032029  [ 1440/ 3200]\n",
            "loss: 1.054035  [ 1456/ 3200]\n",
            "loss: 0.947346  [ 1472/ 3200]\n",
            "loss: 0.873953  [ 1488/ 3200]\n",
            "loss: 1.121801  [ 1504/ 3200]\n",
            "loss: 1.265217  [ 1520/ 3200]\n",
            "loss: 1.133126  [ 1536/ 3200]\n",
            "loss: 0.998740  [ 1552/ 3200]\n",
            "loss: 0.900981  [ 1568/ 3200]\n",
            "loss: 0.922304  [ 1584/ 3200]\n",
            "loss: 0.955840  [ 1600/ 3200]\n",
            "loss: 0.936034  [ 1616/ 3200]\n",
            "loss: 0.849154  [ 1632/ 3200]\n",
            "loss: 0.805223  [ 1648/ 3200]\n",
            "loss: 0.997004  [ 1664/ 3200]\n",
            "loss: 1.204543  [ 1680/ 3200]\n",
            "loss: 0.947593  [ 1696/ 3200]\n",
            "loss: 0.777925  [ 1712/ 3200]\n",
            "loss: 0.708327  [ 1728/ 3200]\n",
            "loss: 1.345000  [ 1744/ 3200]\n",
            "loss: 1.322358  [ 1760/ 3200]\n",
            "loss: 0.845864  [ 1776/ 3200]\n",
            "loss: 1.083755  [ 1792/ 3200]\n",
            "loss: 0.907201  [ 1808/ 3200]\n",
            "loss: 0.725413  [ 1824/ 3200]\n",
            "loss: 1.189259  [ 1840/ 3200]\n",
            "loss: 0.988511  [ 1856/ 3200]\n",
            "loss: 0.887549  [ 1872/ 3200]\n",
            "loss: 0.618103  [ 1888/ 3200]\n",
            "loss: 0.868301  [ 1904/ 3200]\n",
            "loss: 1.075070  [ 1920/ 3200]\n",
            "loss: 0.917840  [ 1936/ 3200]\n",
            "loss: 0.852117  [ 1952/ 3200]\n",
            "loss: 1.277080  [ 1968/ 3200]\n",
            "loss: 0.933519  [ 1984/ 3200]\n",
            "loss: 0.765851  [ 2000/ 3200]\n",
            "loss: 0.888829  [ 2016/ 3200]\n",
            "loss: 1.090047  [ 2032/ 3200]\n",
            "loss: 1.026571  [ 2048/ 3200]\n",
            "loss: 1.156390  [ 2064/ 3200]\n",
            "loss: 1.057928  [ 2080/ 3200]\n",
            "loss: 1.123394  [ 2096/ 3200]\n",
            "loss: 0.848300  [ 2112/ 3200]\n",
            "loss: 1.143831  [ 2128/ 3200]\n",
            "loss: 0.762049  [ 2144/ 3200]\n",
            "loss: 0.657261  [ 2160/ 3200]\n",
            "loss: 0.595569  [ 2176/ 3200]\n",
            "loss: 0.787158  [ 2192/ 3200]\n",
            "loss: 0.994641  [ 2208/ 3200]\n",
            "loss: 0.959423  [ 2224/ 3200]\n",
            "loss: 0.985435  [ 2240/ 3200]\n",
            "loss: 0.862503  [ 2256/ 3200]\n",
            "loss: 0.663429  [ 2272/ 3200]\n",
            "loss: 0.754547  [ 2288/ 3200]\n",
            "loss: 1.291457  [ 2304/ 3200]\n",
            "loss: 1.174203  [ 2320/ 3200]\n",
            "loss: 0.768281  [ 2336/ 3200]\n",
            "loss: 0.862230  [ 2352/ 3200]\n",
            "loss: 0.956601  [ 2368/ 3200]\n",
            "loss: 0.855491  [ 2384/ 3200]\n",
            "loss: 1.074753  [ 2400/ 3200]\n",
            "loss: 0.801349  [ 2416/ 3200]\n",
            "loss: 0.891551  [ 2432/ 3200]\n",
            "loss: 1.560929  [ 2448/ 3200]\n",
            "loss: 0.891688  [ 2464/ 3200]\n",
            "loss: 0.740629  [ 2480/ 3200]\n",
            "loss: 0.993313  [ 2496/ 3200]\n",
            "loss: 1.346519  [ 2512/ 3200]\n",
            "loss: 1.223969  [ 2528/ 3200]\n",
            "loss: 1.048761  [ 2544/ 3200]\n",
            "loss: 1.001897  [ 2560/ 3200]\n",
            "loss: 1.027749  [ 2576/ 3200]\n",
            "loss: 0.991654  [ 2592/ 3200]\n",
            "loss: 0.856234  [ 2608/ 3200]\n",
            "loss: 1.066343  [ 2624/ 3200]\n",
            "loss: 0.938416  [ 2640/ 3200]\n",
            "loss: 0.831941  [ 2656/ 3200]\n",
            "loss: 1.099086  [ 2672/ 3200]\n",
            "loss: 0.830892  [ 2688/ 3200]\n",
            "loss: 1.396517  [ 2704/ 3200]\n",
            "loss: 1.082535  [ 2720/ 3200]\n",
            "loss: 0.782555  [ 2736/ 3200]\n",
            "loss: 1.109726  [ 2752/ 3200]\n",
            "loss: 0.925328  [ 2768/ 3200]\n",
            "loss: 0.783077  [ 2784/ 3200]\n",
            "loss: 0.900780  [ 2800/ 3200]\n",
            "loss: 0.833966  [ 2816/ 3200]\n",
            "loss: 1.260899  [ 2832/ 3200]\n",
            "loss: 0.865649  [ 2848/ 3200]\n",
            "loss: 0.882371  [ 2864/ 3200]\n",
            "loss: 1.034878  [ 2880/ 3200]\n",
            "loss: 1.078654  [ 2896/ 3200]\n",
            "loss: 1.059360  [ 2912/ 3200]\n",
            "loss: 0.830496  [ 2928/ 3200]\n",
            "loss: 0.854469  [ 2944/ 3200]\n",
            "loss: 0.849878  [ 2960/ 3200]\n",
            "loss: 1.160112  [ 2976/ 3200]\n",
            "loss: 1.052858  [ 2992/ 3200]\n",
            "loss: 0.676927  [ 3008/ 3200]\n",
            "loss: 1.019993  [ 3024/ 3200]\n",
            "loss: 0.827575  [ 3040/ 3200]\n",
            "loss: 1.234361  [ 3056/ 3200]\n",
            "loss: 1.053722  [ 3072/ 3200]\n",
            "loss: 0.788882  [ 3088/ 3200]\n",
            "loss: 0.761574  [ 3104/ 3200]\n",
            "loss: 1.106508  [ 3120/ 3200]\n",
            "loss: 0.731473  [ 3136/ 3200]\n",
            "loss: 0.778466  [ 3152/ 3200]\n",
            "loss: 0.858934  [ 3168/ 3200]\n",
            "loss: 0.858823  [ 3184/ 3200]\n",
            "Avg Accuracy: 64.000000%, Avg loss: 0.922502\n",
            "F1 score is: 0.5825973016023636\n",
            "Confusion Matrix:\n",
            "[[191   4   2   3]\n",
            " [ 82  67  26  25]\n",
            " [ 42  20 133   5]\n",
            " [ 21  38  20 121]]\n",
            "current epoch: 26\n",
            "\n",
            "loss: 1.035902  [    0/ 3200]\n",
            "loss: 0.729527  [   16/ 3200]\n",
            "loss: 1.056011  [   32/ 3200]\n",
            "loss: 0.866799  [   48/ 3200]\n",
            "loss: 0.935509  [   64/ 3200]\n",
            "loss: 0.788560  [   80/ 3200]\n",
            "loss: 0.928340  [   96/ 3200]\n",
            "loss: 1.034352  [  112/ 3200]\n",
            "loss: 0.881482  [  128/ 3200]\n",
            "loss: 0.807082  [  144/ 3200]\n",
            "loss: 0.938171  [  160/ 3200]\n",
            "loss: 0.718102  [  176/ 3200]\n",
            "loss: 0.727077  [  192/ 3200]\n",
            "loss: 0.959341  [  208/ 3200]\n",
            "loss: 0.917312  [  224/ 3200]\n",
            "loss: 0.877767  [  240/ 3200]\n",
            "loss: 0.793581  [  256/ 3200]\n",
            "loss: 0.522424  [  272/ 3200]\n",
            "loss: 1.024275  [  288/ 3200]\n",
            "loss: 1.059189  [  304/ 3200]\n",
            "loss: 0.972379  [  320/ 3200]\n",
            "loss: 0.849921  [  336/ 3200]\n",
            "loss: 0.705081  [  352/ 3200]\n",
            "loss: 0.774757  [  368/ 3200]\n",
            "loss: 0.998496  [  384/ 3200]\n",
            "loss: 1.081717  [  400/ 3200]\n",
            "loss: 0.747109  [  416/ 3200]\n",
            "loss: 1.099388  [  432/ 3200]\n",
            "loss: 1.018492  [  448/ 3200]\n",
            "loss: 0.865749  [  464/ 3200]\n",
            "loss: 1.273232  [  480/ 3200]\n",
            "loss: 1.136156  [  496/ 3200]\n",
            "loss: 0.964035  [  512/ 3200]\n",
            "loss: 0.939827  [  528/ 3200]\n",
            "loss: 0.770160  [  544/ 3200]\n",
            "loss: 0.975992  [  560/ 3200]\n",
            "loss: 1.001199  [  576/ 3200]\n",
            "loss: 0.848887  [  592/ 3200]\n",
            "loss: 1.314635  [  608/ 3200]\n",
            "loss: 0.986837  [  624/ 3200]\n",
            "loss: 0.823383  [  640/ 3200]\n",
            "loss: 0.931501  [  656/ 3200]\n",
            "loss: 1.053010  [  672/ 3200]\n",
            "loss: 0.953067  [  688/ 3200]\n",
            "loss: 0.996928  [  704/ 3200]\n",
            "loss: 1.259578  [  720/ 3200]\n",
            "loss: 0.542255  [  736/ 3200]\n",
            "loss: 0.782882  [  752/ 3200]\n",
            "loss: 1.087471  [  768/ 3200]\n",
            "loss: 0.807939  [  784/ 3200]\n",
            "loss: 1.337239  [  800/ 3200]\n",
            "loss: 1.481745  [  816/ 3200]\n",
            "loss: 1.065506  [  832/ 3200]\n",
            "loss: 0.789787  [  848/ 3200]\n",
            "loss: 0.728793  [  864/ 3200]\n",
            "loss: 1.156530  [  880/ 3200]\n",
            "loss: 0.824033  [  896/ 3200]\n",
            "loss: 1.216837  [  912/ 3200]\n",
            "loss: 0.806331  [  928/ 3200]\n",
            "loss: 0.994637  [  944/ 3200]\n",
            "loss: 0.818026  [  960/ 3200]\n",
            "loss: 0.630968  [  976/ 3200]\n",
            "loss: 0.738957  [  992/ 3200]\n",
            "loss: 0.883723  [ 1008/ 3200]\n",
            "loss: 1.102348  [ 1024/ 3200]\n",
            "loss: 1.005729  [ 1040/ 3200]\n",
            "loss: 0.647198  [ 1056/ 3200]\n",
            "loss: 0.508273  [ 1072/ 3200]\n",
            "loss: 1.163692  [ 1088/ 3200]\n",
            "loss: 0.969955  [ 1104/ 3200]\n",
            "loss: 0.687089  [ 1120/ 3200]\n",
            "loss: 0.917816  [ 1136/ 3200]\n",
            "loss: 0.853466  [ 1152/ 3200]\n",
            "loss: 1.252748  [ 1168/ 3200]\n",
            "loss: 1.025231  [ 1184/ 3200]\n",
            "loss: 0.934025  [ 1200/ 3200]\n",
            "loss: 0.714658  [ 1216/ 3200]\n",
            "loss: 1.071438  [ 1232/ 3200]\n",
            "loss: 0.634588  [ 1248/ 3200]\n",
            "loss: 1.077080  [ 1264/ 3200]\n",
            "loss: 0.767251  [ 1280/ 3200]\n",
            "loss: 0.852676  [ 1296/ 3200]\n",
            "loss: 0.890335  [ 1312/ 3200]\n",
            "loss: 1.108944  [ 1328/ 3200]\n",
            "loss: 0.885528  [ 1344/ 3200]\n",
            "loss: 0.721499  [ 1360/ 3200]\n",
            "loss: 0.971438  [ 1376/ 3200]\n",
            "loss: 0.955560  [ 1392/ 3200]\n",
            "loss: 0.967356  [ 1408/ 3200]\n",
            "loss: 0.697642  [ 1424/ 3200]\n",
            "loss: 0.762394  [ 1440/ 3200]\n",
            "loss: 1.054410  [ 1456/ 3200]\n",
            "loss: 0.741674  [ 1472/ 3200]\n",
            "loss: 1.081222  [ 1488/ 3200]\n",
            "loss: 0.964050  [ 1504/ 3200]\n",
            "loss: 0.766571  [ 1520/ 3200]\n",
            "loss: 1.001274  [ 1536/ 3200]\n",
            "loss: 1.138687  [ 1552/ 3200]\n",
            "loss: 1.021995  [ 1568/ 3200]\n",
            "loss: 0.948309  [ 1584/ 3200]\n",
            "loss: 1.048104  [ 1600/ 3200]\n",
            "loss: 0.712715  [ 1616/ 3200]\n",
            "loss: 0.608826  [ 1632/ 3200]\n",
            "loss: 0.871799  [ 1648/ 3200]\n",
            "loss: 1.050263  [ 1664/ 3200]\n",
            "loss: 1.026079  [ 1680/ 3200]\n",
            "loss: 0.953548  [ 1696/ 3200]\n",
            "loss: 0.797573  [ 1712/ 3200]\n",
            "loss: 0.779465  [ 1728/ 3200]\n",
            "loss: 0.791607  [ 1744/ 3200]\n",
            "loss: 1.063962  [ 1760/ 3200]\n",
            "loss: 0.881452  [ 1776/ 3200]\n",
            "loss: 1.259984  [ 1792/ 3200]\n",
            "loss: 0.630191  [ 1808/ 3200]\n",
            "loss: 1.242377  [ 1824/ 3200]\n",
            "loss: 1.010181  [ 1840/ 3200]\n",
            "loss: 0.845344  [ 1856/ 3200]\n",
            "loss: 1.027693  [ 1872/ 3200]\n",
            "loss: 0.926088  [ 1888/ 3200]\n",
            "loss: 0.744874  [ 1904/ 3200]\n",
            "loss: 1.075932  [ 1920/ 3200]\n",
            "loss: 1.348144  [ 1936/ 3200]\n",
            "loss: 1.513963  [ 1952/ 3200]\n",
            "loss: 0.938147  [ 1968/ 3200]\n",
            "loss: 1.001539  [ 1984/ 3200]\n",
            "loss: 0.845204  [ 2000/ 3200]\n",
            "loss: 0.741636  [ 2016/ 3200]\n",
            "loss: 1.259357  [ 2032/ 3200]\n",
            "loss: 1.126060  [ 2048/ 3200]\n",
            "loss: 0.738390  [ 2064/ 3200]\n",
            "loss: 0.942525  [ 2080/ 3200]\n",
            "loss: 1.076538  [ 2096/ 3200]\n",
            "loss: 1.265543  [ 2112/ 3200]\n",
            "loss: 0.792415  [ 2128/ 3200]\n",
            "loss: 1.296615  [ 2144/ 3200]\n",
            "loss: 1.078029  [ 2160/ 3200]\n",
            "loss: 0.775591  [ 2176/ 3200]\n",
            "loss: 1.318924  [ 2192/ 3200]\n",
            "loss: 1.052889  [ 2208/ 3200]\n",
            "loss: 1.292635  [ 2224/ 3200]\n",
            "loss: 1.023537  [ 2240/ 3200]\n",
            "loss: 0.931400  [ 2256/ 3200]\n",
            "loss: 0.958535  [ 2272/ 3200]\n",
            "loss: 1.148042  [ 2288/ 3200]\n",
            "loss: 0.774649  [ 2304/ 3200]\n",
            "loss: 1.002304  [ 2320/ 3200]\n",
            "loss: 0.819011  [ 2336/ 3200]\n",
            "loss: 0.751239  [ 2352/ 3200]\n",
            "loss: 0.759092  [ 2368/ 3200]\n",
            "loss: 1.012248  [ 2384/ 3200]\n",
            "loss: 1.287048  [ 2400/ 3200]\n",
            "loss: 1.002968  [ 2416/ 3200]\n",
            "loss: 0.878128  [ 2432/ 3200]\n",
            "loss: 0.990929  [ 2448/ 3200]\n",
            "loss: 1.079853  [ 2464/ 3200]\n",
            "loss: 1.116779  [ 2480/ 3200]\n",
            "loss: 0.898782  [ 2496/ 3200]\n",
            "loss: 1.032916  [ 2512/ 3200]\n",
            "loss: 0.963636  [ 2528/ 3200]\n",
            "loss: 1.132669  [ 2544/ 3200]\n",
            "loss: 0.955513  [ 2560/ 3200]\n",
            "loss: 0.735673  [ 2576/ 3200]\n",
            "loss: 1.283318  [ 2592/ 3200]\n",
            "loss: 1.134685  [ 2608/ 3200]\n",
            "loss: 1.207908  [ 2624/ 3200]\n",
            "loss: 1.003043  [ 2640/ 3200]\n",
            "loss: 0.818707  [ 2656/ 3200]\n",
            "loss: 0.789560  [ 2672/ 3200]\n",
            "loss: 0.900545  [ 2688/ 3200]\n",
            "loss: 0.729263  [ 2704/ 3200]\n",
            "loss: 0.934981  [ 2720/ 3200]\n",
            "loss: 1.003329  [ 2736/ 3200]\n",
            "loss: 0.978594  [ 2752/ 3200]\n",
            "loss: 1.278772  [ 2768/ 3200]\n",
            "loss: 1.146713  [ 2784/ 3200]\n",
            "loss: 0.850149  [ 2800/ 3200]\n",
            "loss: 1.190454  [ 2816/ 3200]\n",
            "loss: 0.734921  [ 2832/ 3200]\n",
            "loss: 0.682376  [ 2848/ 3200]\n",
            "loss: 0.769696  [ 2864/ 3200]\n",
            "loss: 1.006798  [ 2880/ 3200]\n",
            "loss: 0.972087  [ 2896/ 3200]\n",
            "loss: 0.699638  [ 2912/ 3200]\n",
            "loss: 0.756088  [ 2928/ 3200]\n",
            "loss: 1.032667  [ 2944/ 3200]\n",
            "loss: 1.087141  [ 2960/ 3200]\n",
            "loss: 0.854724  [ 2976/ 3200]\n",
            "loss: 1.056792  [ 2992/ 3200]\n",
            "loss: 0.955638  [ 3008/ 3200]\n",
            "loss: 1.131000  [ 3024/ 3200]\n",
            "loss: 1.216794  [ 3040/ 3200]\n",
            "loss: 0.804901  [ 3056/ 3200]\n",
            "loss: 0.913005  [ 3072/ 3200]\n",
            "loss: 0.928520  [ 3088/ 3200]\n",
            "loss: 1.251247  [ 3104/ 3200]\n",
            "loss: 1.225870  [ 3120/ 3200]\n",
            "loss: 0.947328  [ 3136/ 3200]\n",
            "loss: 0.923665  [ 3152/ 3200]\n",
            "loss: 0.939369  [ 3168/ 3200]\n",
            "loss: 1.150240  [ 3184/ 3200]\n",
            "Avg Accuracy: 68.000000%, Avg loss: 0.873127\n",
            "F1 score is: 0.6313887041807175\n",
            "Confusion Matrix:\n",
            "[[179   9   8   4]\n",
            " [ 52  67  31  50]\n",
            " [ 15  19 156  10]\n",
            " [  5  33  20 142]]\n",
            "current epoch: 27\n",
            "\n",
            "loss: 0.892343  [    0/ 3200]\n",
            "loss: 1.142554  [   16/ 3200]\n",
            "loss: 0.916987  [   32/ 3200]\n",
            "loss: 0.917534  [   48/ 3200]\n",
            "loss: 0.517379  [   64/ 3200]\n",
            "loss: 0.794639  [   80/ 3200]\n",
            "loss: 0.640596  [   96/ 3200]\n",
            "loss: 0.716338  [  112/ 3200]\n",
            "loss: 0.973276  [  128/ 3200]\n",
            "loss: 0.879312  [  144/ 3200]\n",
            "loss: 0.993747  [  160/ 3200]\n",
            "loss: 0.970792  [  176/ 3200]\n",
            "loss: 0.720216  [  192/ 3200]\n",
            "loss: 1.064495  [  208/ 3200]\n",
            "loss: 1.070680  [  224/ 3200]\n",
            "loss: 1.112757  [  240/ 3200]\n",
            "loss: 0.866594  [  256/ 3200]\n",
            "loss: 1.021810  [  272/ 3200]\n",
            "loss: 0.668341  [  288/ 3200]\n",
            "loss: 0.642622  [  304/ 3200]\n",
            "loss: 0.980868  [  320/ 3200]\n",
            "loss: 1.052934  [  336/ 3200]\n",
            "loss: 0.905285  [  352/ 3200]\n",
            "loss: 0.631368  [  368/ 3200]\n",
            "loss: 0.791225  [  384/ 3200]\n",
            "loss: 0.974806  [  400/ 3200]\n",
            "loss: 0.910429  [  416/ 3200]\n",
            "loss: 0.981308  [  432/ 3200]\n",
            "loss: 0.801675  [  448/ 3200]\n",
            "loss: 1.204987  [  464/ 3200]\n",
            "loss: 1.076557  [  480/ 3200]\n",
            "loss: 1.022148  [  496/ 3200]\n",
            "loss: 1.139289  [  512/ 3200]\n",
            "loss: 0.817633  [  528/ 3200]\n",
            "loss: 1.090217  [  544/ 3200]\n",
            "loss: 0.946633  [  560/ 3200]\n",
            "loss: 0.791831  [  576/ 3200]\n",
            "loss: 0.768337  [  592/ 3200]\n",
            "loss: 0.974362  [  608/ 3200]\n",
            "loss: 1.158048  [  624/ 3200]\n",
            "loss: 0.740190  [  640/ 3200]\n",
            "loss: 1.132320  [  656/ 3200]\n",
            "loss: 0.920737  [  672/ 3200]\n",
            "loss: 0.963943  [  688/ 3200]\n",
            "loss: 0.834529  [  704/ 3200]\n",
            "loss: 0.811197  [  720/ 3200]\n",
            "loss: 1.111487  [  736/ 3200]\n",
            "loss: 0.964032  [  752/ 3200]\n",
            "loss: 0.969651  [  768/ 3200]\n",
            "loss: 0.681965  [  784/ 3200]\n",
            "loss: 0.996790  [  800/ 3200]\n",
            "loss: 0.971616  [  816/ 3200]\n",
            "loss: 0.826799  [  832/ 3200]\n",
            "loss: 0.986450  [  848/ 3200]\n",
            "loss: 0.818911  [  864/ 3200]\n",
            "loss: 0.957136  [  880/ 3200]\n",
            "loss: 0.938195  [  896/ 3200]\n",
            "loss: 0.833119  [  912/ 3200]\n",
            "loss: 0.871165  [  928/ 3200]\n",
            "loss: 1.047465  [  944/ 3200]\n",
            "loss: 0.953561  [  960/ 3200]\n",
            "loss: 1.165528  [  976/ 3200]\n",
            "loss: 1.102662  [  992/ 3200]\n",
            "loss: 1.310763  [ 1008/ 3200]\n",
            "loss: 0.682938  [ 1024/ 3200]\n",
            "loss: 0.811218  [ 1040/ 3200]\n",
            "loss: 0.713750  [ 1056/ 3200]\n",
            "loss: 0.808881  [ 1072/ 3200]\n",
            "loss: 1.090168  [ 1088/ 3200]\n",
            "loss: 0.738942  [ 1104/ 3200]\n",
            "loss: 0.852635  [ 1120/ 3200]\n",
            "loss: 0.954612  [ 1136/ 3200]\n",
            "loss: 0.835761  [ 1152/ 3200]\n",
            "loss: 0.963499  [ 1168/ 3200]\n",
            "loss: 1.098477  [ 1184/ 3200]\n",
            "loss: 0.800274  [ 1200/ 3200]\n",
            "loss: 0.809065  [ 1216/ 3200]\n",
            "loss: 1.302257  [ 1232/ 3200]\n",
            "loss: 1.217080  [ 1248/ 3200]\n",
            "loss: 1.071881  [ 1264/ 3200]\n",
            "loss: 0.915168  [ 1280/ 3200]\n",
            "loss: 0.921865  [ 1296/ 3200]\n",
            "loss: 0.657383  [ 1312/ 3200]\n",
            "loss: 0.902310  [ 1328/ 3200]\n",
            "loss: 0.782706  [ 1344/ 3200]\n",
            "loss: 1.182500  [ 1360/ 3200]\n",
            "loss: 0.929134  [ 1376/ 3200]\n",
            "loss: 0.625280  [ 1392/ 3200]\n",
            "loss: 0.737236  [ 1408/ 3200]\n",
            "loss: 0.845286  [ 1424/ 3200]\n",
            "loss: 0.670424  [ 1440/ 3200]\n",
            "loss: 1.269777  [ 1456/ 3200]\n",
            "loss: 0.904284  [ 1472/ 3200]\n",
            "loss: 1.291567  [ 1488/ 3200]\n",
            "loss: 0.588659  [ 1504/ 3200]\n",
            "loss: 0.802233  [ 1520/ 3200]\n",
            "loss: 0.872648  [ 1536/ 3200]\n",
            "loss: 0.704211  [ 1552/ 3200]\n",
            "loss: 1.015107  [ 1568/ 3200]\n",
            "loss: 1.020663  [ 1584/ 3200]\n",
            "loss: 0.790294  [ 1600/ 3200]\n",
            "loss: 0.804365  [ 1616/ 3200]\n",
            "loss: 1.062101  [ 1632/ 3200]\n",
            "loss: 1.003861  [ 1648/ 3200]\n",
            "loss: 0.778120  [ 1664/ 3200]\n",
            "loss: 1.030123  [ 1680/ 3200]\n",
            "loss: 0.778845  [ 1696/ 3200]\n",
            "loss: 0.761039  [ 1712/ 3200]\n",
            "loss: 1.202916  [ 1728/ 3200]\n",
            "loss: 1.257291  [ 1744/ 3200]\n",
            "loss: 0.852256  [ 1760/ 3200]\n",
            "loss: 0.562216  [ 1776/ 3200]\n",
            "loss: 0.802064  [ 1792/ 3200]\n",
            "loss: 0.906470  [ 1808/ 3200]\n",
            "loss: 1.135005  [ 1824/ 3200]\n",
            "loss: 0.948745  [ 1840/ 3200]\n",
            "loss: 0.799462  [ 1856/ 3200]\n",
            "loss: 1.338781  [ 1872/ 3200]\n",
            "loss: 0.817596  [ 1888/ 3200]\n",
            "loss: 0.713714  [ 1904/ 3200]\n",
            "loss: 0.859564  [ 1920/ 3200]\n",
            "loss: 1.070735  [ 1936/ 3200]\n",
            "loss: 0.878477  [ 1952/ 3200]\n",
            "loss: 1.109790  [ 1968/ 3200]\n",
            "loss: 0.946275  [ 1984/ 3200]\n",
            "loss: 0.893929  [ 2000/ 3200]\n",
            "loss: 1.213478  [ 2016/ 3200]\n",
            "loss: 1.038618  [ 2032/ 3200]\n",
            "loss: 1.011164  [ 2048/ 3200]\n",
            "loss: 0.831422  [ 2064/ 3200]\n",
            "loss: 1.188443  [ 2080/ 3200]\n",
            "loss: 1.171789  [ 2096/ 3200]\n",
            "loss: 0.792364  [ 2112/ 3200]\n",
            "loss: 1.154119  [ 2128/ 3200]\n",
            "loss: 0.931781  [ 2144/ 3200]\n",
            "loss: 1.054489  [ 2160/ 3200]\n",
            "loss: 0.772628  [ 2176/ 3200]\n",
            "loss: 0.950726  [ 2192/ 3200]\n",
            "loss: 0.848397  [ 2208/ 3200]\n",
            "loss: 0.721055  [ 2224/ 3200]\n",
            "loss: 0.766007  [ 2240/ 3200]\n",
            "loss: 0.635283  [ 2256/ 3200]\n",
            "loss: 0.817823  [ 2272/ 3200]\n",
            "loss: 0.606388  [ 2288/ 3200]\n",
            "loss: 1.011364  [ 2304/ 3200]\n",
            "loss: 1.264008  [ 2320/ 3200]\n",
            "loss: 1.231596  [ 2336/ 3200]\n",
            "loss: 0.728237  [ 2352/ 3200]\n",
            "loss: 1.276517  [ 2368/ 3200]\n",
            "loss: 0.890649  [ 2384/ 3200]\n",
            "loss: 0.704530  [ 2400/ 3200]\n",
            "loss: 0.943400  [ 2416/ 3200]\n",
            "loss: 0.742534  [ 2432/ 3200]\n",
            "loss: 0.999014  [ 2448/ 3200]\n",
            "loss: 1.047205  [ 2464/ 3200]\n",
            "loss: 0.467467  [ 2480/ 3200]\n",
            "loss: 1.011955  [ 2496/ 3200]\n",
            "loss: 0.909531  [ 2512/ 3200]\n",
            "loss: 0.938829  [ 2528/ 3200]\n",
            "loss: 0.941207  [ 2544/ 3200]\n",
            "loss: 0.876262  [ 2560/ 3200]\n",
            "loss: 1.195712  [ 2576/ 3200]\n",
            "loss: 0.914387  [ 2592/ 3200]\n",
            "loss: 1.563983  [ 2608/ 3200]\n",
            "loss: 1.253733  [ 2624/ 3200]\n",
            "loss: 1.322961  [ 2640/ 3200]\n",
            "loss: 0.926568  [ 2656/ 3200]\n",
            "loss: 0.889005  [ 2672/ 3200]\n",
            "loss: 1.023194  [ 2688/ 3200]\n",
            "loss: 0.951227  [ 2704/ 3200]\n",
            "loss: 1.054630  [ 2720/ 3200]\n",
            "loss: 1.109336  [ 2736/ 3200]\n",
            "loss: 0.930417  [ 2752/ 3200]\n",
            "loss: 0.909458  [ 2768/ 3200]\n",
            "loss: 0.690709  [ 2784/ 3200]\n",
            "loss: 1.144443  [ 2800/ 3200]\n",
            "loss: 1.120757  [ 2816/ 3200]\n",
            "loss: 0.713123  [ 2832/ 3200]\n",
            "loss: 0.790547  [ 2848/ 3200]\n",
            "loss: 1.207492  [ 2864/ 3200]\n",
            "loss: 1.390087  [ 2880/ 3200]\n",
            "loss: 0.876248  [ 2896/ 3200]\n",
            "loss: 0.766863  [ 2912/ 3200]\n",
            "loss: 0.978122  [ 2928/ 3200]\n",
            "loss: 1.344336  [ 2944/ 3200]\n",
            "loss: 0.995291  [ 2960/ 3200]\n",
            "loss: 1.143486  [ 2976/ 3200]\n",
            "loss: 0.832040  [ 2992/ 3200]\n",
            "loss: 0.977774  [ 3008/ 3200]\n",
            "loss: 0.880191  [ 3024/ 3200]\n",
            "loss: 1.377138  [ 3040/ 3200]\n",
            "loss: 0.871701  [ 3056/ 3200]\n",
            "loss: 0.862268  [ 3072/ 3200]\n",
            "loss: 1.218739  [ 3088/ 3200]\n",
            "loss: 0.744968  [ 3104/ 3200]\n",
            "loss: 0.758557  [ 3120/ 3200]\n",
            "loss: 0.949967  [ 3136/ 3200]\n",
            "loss: 0.885143  [ 3152/ 3200]\n",
            "loss: 0.970663  [ 3168/ 3200]\n",
            "loss: 0.845058  [ 3184/ 3200]\n",
            "Avg Accuracy: 66.250000%, Avg loss: 0.873664\n",
            "F1 score is: 0.6240465718507767\n",
            "Confusion Matrix:\n",
            "[[144  23  30   3]\n",
            " [ 30  68  55  47]\n",
            " [  7   7 177   9]\n",
            " [  1  30  28 141]]\n",
            "current epoch: 28\n",
            "\n",
            "loss: 1.047265  [    0/ 3200]\n",
            "loss: 0.858038  [   16/ 3200]\n",
            "loss: 0.868124  [   32/ 3200]\n",
            "loss: 1.202791  [   48/ 3200]\n",
            "loss: 0.886798  [   64/ 3200]\n",
            "loss: 0.961166  [   80/ 3200]\n",
            "loss: 1.703722  [   96/ 3200]\n",
            "loss: 1.234363  [  112/ 3200]\n",
            "loss: 1.060413  [  128/ 3200]\n",
            "loss: 1.034155  [  144/ 3200]\n",
            "loss: 0.863906  [  160/ 3200]\n",
            "loss: 0.891961  [  176/ 3200]\n",
            "loss: 1.035398  [  192/ 3200]\n",
            "loss: 1.156053  [  208/ 3200]\n",
            "loss: 0.836111  [  224/ 3200]\n",
            "loss: 0.920339  [  240/ 3200]\n",
            "loss: 1.294672  [  256/ 3200]\n",
            "loss: 0.939063  [  272/ 3200]\n",
            "loss: 0.864869  [  288/ 3200]\n",
            "loss: 1.076099  [  304/ 3200]\n",
            "loss: 0.911037  [  320/ 3200]\n",
            "loss: 0.657372  [  336/ 3200]\n",
            "loss: 0.820611  [  352/ 3200]\n",
            "loss: 0.730309  [  368/ 3200]\n",
            "loss: 0.812811  [  384/ 3200]\n",
            "loss: 0.917888  [  400/ 3200]\n",
            "loss: 0.654486  [  416/ 3200]\n",
            "loss: 1.511476  [  432/ 3200]\n",
            "loss: 0.792987  [  448/ 3200]\n",
            "loss: 1.129947  [  464/ 3200]\n",
            "loss: 1.062365  [  480/ 3200]\n",
            "loss: 1.031834  [  496/ 3200]\n",
            "loss: 0.683529  [  512/ 3200]\n",
            "loss: 0.618543  [  528/ 3200]\n",
            "loss: 1.047109  [  544/ 3200]\n",
            "loss: 1.320294  [  560/ 3200]\n",
            "loss: 0.961140  [  576/ 3200]\n",
            "loss: 0.877497  [  592/ 3200]\n",
            "loss: 0.889894  [  608/ 3200]\n",
            "loss: 0.884157  [  624/ 3200]\n",
            "loss: 0.964287  [  640/ 3200]\n",
            "loss: 0.950398  [  656/ 3200]\n",
            "loss: 0.886172  [  672/ 3200]\n",
            "loss: 0.943734  [  688/ 3200]\n",
            "loss: 1.040499  [  704/ 3200]\n",
            "loss: 0.574904  [  720/ 3200]\n",
            "loss: 0.635157  [  736/ 3200]\n",
            "loss: 0.724652  [  752/ 3200]\n",
            "loss: 1.081186  [  768/ 3200]\n",
            "loss: 0.726469  [  784/ 3200]\n",
            "loss: 0.950764  [  800/ 3200]\n",
            "loss: 0.843288  [  816/ 3200]\n",
            "loss: 1.181865  [  832/ 3200]\n",
            "loss: 1.069036  [  848/ 3200]\n",
            "loss: 0.967538  [  864/ 3200]\n",
            "loss: 0.624602  [  880/ 3200]\n",
            "loss: 1.090600  [  896/ 3200]\n",
            "loss: 0.990115  [  912/ 3200]\n",
            "loss: 0.670241  [  928/ 3200]\n",
            "loss: 0.875407  [  944/ 3200]\n",
            "loss: 0.963461  [  960/ 3200]\n",
            "loss: 0.721842  [  976/ 3200]\n",
            "loss: 1.031447  [  992/ 3200]\n",
            "loss: 1.164722  [ 1008/ 3200]\n",
            "loss: 0.753708  [ 1024/ 3200]\n",
            "loss: 0.965529  [ 1040/ 3200]\n",
            "loss: 0.876797  [ 1056/ 3200]\n",
            "loss: 1.105368  [ 1072/ 3200]\n",
            "loss: 1.222474  [ 1088/ 3200]\n",
            "loss: 1.140345  [ 1104/ 3200]\n",
            "loss: 1.010222  [ 1120/ 3200]\n",
            "loss: 1.032388  [ 1136/ 3200]\n",
            "loss: 1.115361  [ 1152/ 3200]\n",
            "loss: 0.564392  [ 1168/ 3200]\n",
            "loss: 0.936234  [ 1184/ 3200]\n",
            "loss: 0.984361  [ 1200/ 3200]\n",
            "loss: 1.015043  [ 1216/ 3200]\n",
            "loss: 0.699784  [ 1232/ 3200]\n",
            "loss: 0.998712  [ 1248/ 3200]\n",
            "loss: 0.875945  [ 1264/ 3200]\n",
            "loss: 0.668894  [ 1280/ 3200]\n",
            "loss: 0.920093  [ 1296/ 3200]\n",
            "loss: 1.060413  [ 1312/ 3200]\n",
            "loss: 0.692495  [ 1328/ 3200]\n",
            "loss: 1.020522  [ 1344/ 3200]\n",
            "loss: 0.913136  [ 1360/ 3200]\n",
            "loss: 0.849585  [ 1376/ 3200]\n",
            "loss: 0.880537  [ 1392/ 3200]\n",
            "loss: 0.949520  [ 1408/ 3200]\n",
            "loss: 0.923007  [ 1424/ 3200]\n",
            "loss: 0.698010  [ 1440/ 3200]\n",
            "loss: 0.787502  [ 1456/ 3200]\n",
            "loss: 0.909118  [ 1472/ 3200]\n",
            "loss: 1.053196  [ 1488/ 3200]\n",
            "loss: 1.272018  [ 1504/ 3200]\n",
            "loss: 0.719015  [ 1520/ 3200]\n",
            "loss: 1.003828  [ 1536/ 3200]\n",
            "loss: 0.704468  [ 1552/ 3200]\n",
            "loss: 0.948037  [ 1568/ 3200]\n",
            "loss: 0.901553  [ 1584/ 3200]\n",
            "loss: 0.834205  [ 1600/ 3200]\n",
            "loss: 0.837785  [ 1616/ 3200]\n",
            "loss: 0.920512  [ 1632/ 3200]\n",
            "loss: 0.998852  [ 1648/ 3200]\n",
            "loss: 1.683938  [ 1664/ 3200]\n",
            "loss: 1.197940  [ 1680/ 3200]\n",
            "loss: 0.984144  [ 1696/ 3200]\n",
            "loss: 0.684404  [ 1712/ 3200]\n",
            "loss: 1.028649  [ 1728/ 3200]\n",
            "loss: 0.833710  [ 1744/ 3200]\n",
            "loss: 0.971684  [ 1760/ 3200]\n",
            "loss: 0.832670  [ 1776/ 3200]\n",
            "loss: 1.045147  [ 1792/ 3200]\n",
            "loss: 0.673102  [ 1808/ 3200]\n",
            "loss: 0.789469  [ 1824/ 3200]\n",
            "loss: 0.873143  [ 1840/ 3200]\n",
            "loss: 1.154294  [ 1856/ 3200]\n",
            "loss: 1.118861  [ 1872/ 3200]\n",
            "loss: 0.832026  [ 1888/ 3200]\n",
            "loss: 0.610287  [ 1904/ 3200]\n",
            "loss: 1.231768  [ 1920/ 3200]\n",
            "loss: 0.807513  [ 1936/ 3200]\n",
            "loss: 1.106398  [ 1952/ 3200]\n",
            "loss: 0.910144  [ 1968/ 3200]\n",
            "loss: 0.888678  [ 1984/ 3200]\n",
            "loss: 1.305675  [ 2000/ 3200]\n",
            "loss: 0.806541  [ 2016/ 3200]\n",
            "loss: 0.716028  [ 2032/ 3200]\n",
            "loss: 0.879516  [ 2048/ 3200]\n",
            "loss: 1.045211  [ 2064/ 3200]\n",
            "loss: 0.782150  [ 2080/ 3200]\n",
            "loss: 1.027097  [ 2096/ 3200]\n",
            "loss: 0.842727  [ 2112/ 3200]\n",
            "loss: 0.728955  [ 2128/ 3200]\n",
            "loss: 0.769746  [ 2144/ 3200]\n",
            "loss: 0.786091  [ 2160/ 3200]\n",
            "loss: 0.846055  [ 2176/ 3200]\n",
            "loss: 1.114292  [ 2192/ 3200]\n",
            "loss: 1.414336  [ 2208/ 3200]\n",
            "loss: 0.966060  [ 2224/ 3200]\n",
            "loss: 0.975846  [ 2240/ 3200]\n",
            "loss: 0.928146  [ 2256/ 3200]\n",
            "loss: 0.805613  [ 2272/ 3200]\n",
            "loss: 1.024336  [ 2288/ 3200]\n",
            "loss: 1.013284  [ 2304/ 3200]\n",
            "loss: 0.967085  [ 2320/ 3200]\n",
            "loss: 1.167964  [ 2336/ 3200]\n",
            "loss: 0.870127  [ 2352/ 3200]\n",
            "loss: 0.878793  [ 2368/ 3200]\n",
            "loss: 0.565521  [ 2384/ 3200]\n",
            "loss: 1.128536  [ 2400/ 3200]\n",
            "loss: 0.565483  [ 2416/ 3200]\n",
            "loss: 0.817344  [ 2432/ 3200]\n",
            "loss: 0.984021  [ 2448/ 3200]\n",
            "loss: 1.102397  [ 2464/ 3200]\n",
            "loss: 0.901707  [ 2480/ 3200]\n",
            "loss: 1.212746  [ 2496/ 3200]\n",
            "loss: 0.850298  [ 2512/ 3200]\n",
            "loss: 0.963232  [ 2528/ 3200]\n",
            "loss: 0.880610  [ 2544/ 3200]\n",
            "loss: 0.841211  [ 2560/ 3200]\n",
            "loss: 0.848613  [ 2576/ 3200]\n",
            "loss: 0.698343  [ 2592/ 3200]\n",
            "loss: 0.910428  [ 2608/ 3200]\n",
            "loss: 0.842065  [ 2624/ 3200]\n",
            "loss: 0.967486  [ 2640/ 3200]\n",
            "loss: 0.978107  [ 2656/ 3200]\n",
            "loss: 0.821803  [ 2672/ 3200]\n",
            "loss: 0.871608  [ 2688/ 3200]\n",
            "loss: 1.112872  [ 2704/ 3200]\n",
            "loss: 1.090307  [ 2720/ 3200]\n",
            "loss: 0.775278  [ 2736/ 3200]\n",
            "loss: 0.817025  [ 2752/ 3200]\n",
            "loss: 0.678724  [ 2768/ 3200]\n",
            "loss: 0.901167  [ 2784/ 3200]\n",
            "loss: 0.890522  [ 2800/ 3200]\n",
            "loss: 1.072388  [ 2816/ 3200]\n",
            "loss: 0.963829  [ 2832/ 3200]\n",
            "loss: 1.067971  [ 2848/ 3200]\n",
            "loss: 0.869505  [ 2864/ 3200]\n",
            "loss: 0.832075  [ 2880/ 3200]\n",
            "loss: 0.891016  [ 2896/ 3200]\n",
            "loss: 0.786671  [ 2912/ 3200]\n",
            "loss: 1.211364  [ 2928/ 3200]\n",
            "loss: 0.789684  [ 2944/ 3200]\n",
            "loss: 1.013936  [ 2960/ 3200]\n",
            "loss: 1.027645  [ 2976/ 3200]\n",
            "loss: 0.807176  [ 2992/ 3200]\n",
            "loss: 1.177039  [ 3008/ 3200]\n",
            "loss: 0.995347  [ 3024/ 3200]\n",
            "loss: 0.801620  [ 3040/ 3200]\n",
            "loss: 0.618662  [ 3056/ 3200]\n",
            "loss: 0.917740  [ 3072/ 3200]\n",
            "loss: 0.789278  [ 3088/ 3200]\n",
            "loss: 1.154122  [ 3104/ 3200]\n",
            "loss: 1.063307  [ 3120/ 3200]\n",
            "loss: 0.771397  [ 3136/ 3200]\n",
            "loss: 0.787555  [ 3152/ 3200]\n",
            "loss: 1.027622  [ 3168/ 3200]\n",
            "loss: 1.121470  [ 3184/ 3200]\n",
            "Avg Accuracy: 61.750000%, Avg loss: 0.929204\n",
            "F1 score is: 0.5733372861146927\n",
            "Confusion Matrix:\n",
            "[[ 97  86  13   4]\n",
            " [ 16  98  26  60]\n",
            " [  1  34 150  15]\n",
            " [  0  37  14 149]]\n",
            "current epoch: 29\n",
            "\n",
            "loss: 1.114707  [    0/ 3200]\n",
            "loss: 0.843672  [   16/ 3200]\n",
            "loss: 0.841841  [   32/ 3200]\n",
            "loss: 1.010814  [   48/ 3200]\n",
            "loss: 0.855745  [   64/ 3200]\n",
            "loss: 0.860974  [   80/ 3200]\n",
            "loss: 0.913255  [   96/ 3200]\n",
            "loss: 1.081751  [  112/ 3200]\n",
            "loss: 0.974867  [  128/ 3200]\n",
            "loss: 0.999706  [  144/ 3200]\n",
            "loss: 0.671482  [  160/ 3200]\n",
            "loss: 1.001265  [  176/ 3200]\n",
            "loss: 0.856305  [  192/ 3200]\n",
            "loss: 0.695416  [  208/ 3200]\n",
            "loss: 1.012891  [  224/ 3200]\n",
            "loss: 0.924847  [  240/ 3200]\n",
            "loss: 0.909302  [  256/ 3200]\n",
            "loss: 0.868404  [  272/ 3200]\n",
            "loss: 1.390693  [  288/ 3200]\n",
            "loss: 0.994049  [  304/ 3200]\n",
            "loss: 1.099846  [  320/ 3200]\n",
            "loss: 1.064890  [  336/ 3200]\n",
            "loss: 0.950993  [  352/ 3200]\n",
            "loss: 1.050542  [  368/ 3200]\n",
            "loss: 0.902380  [  384/ 3200]\n",
            "loss: 1.151385  [  400/ 3200]\n",
            "loss: 0.954015  [  416/ 3200]\n",
            "loss: 0.928986  [  432/ 3200]\n",
            "loss: 0.897156  [  448/ 3200]\n",
            "loss: 0.875360  [  464/ 3200]\n",
            "loss: 1.031405  [  480/ 3200]\n",
            "loss: 0.775141  [  496/ 3200]\n",
            "loss: 0.917402  [  512/ 3200]\n",
            "loss: 0.889749  [  528/ 3200]\n",
            "loss: 1.147482  [  544/ 3200]\n",
            "loss: 1.286579  [  560/ 3200]\n",
            "loss: 1.046539  [  576/ 3200]\n",
            "loss: 1.338428  [  592/ 3200]\n",
            "loss: 0.903867  [  608/ 3200]\n",
            "loss: 0.563320  [  624/ 3200]\n",
            "loss: 1.037768  [  640/ 3200]\n",
            "loss: 1.092068  [  656/ 3200]\n",
            "loss: 1.240698  [  672/ 3200]\n",
            "loss: 0.976250  [  688/ 3200]\n",
            "loss: 1.420391  [  704/ 3200]\n",
            "loss: 0.801556  [  720/ 3200]\n",
            "loss: 1.004515  [  736/ 3200]\n",
            "loss: 0.993777  [  752/ 3200]\n",
            "loss: 0.650371  [  768/ 3200]\n",
            "loss: 0.885252  [  784/ 3200]\n",
            "loss: 0.958733  [  800/ 3200]\n",
            "loss: 1.308056  [  816/ 3200]\n",
            "loss: 0.684529  [  832/ 3200]\n",
            "loss: 0.840707  [  848/ 3200]\n",
            "loss: 0.892243  [  864/ 3200]\n",
            "loss: 0.727237  [  880/ 3200]\n",
            "loss: 0.889912  [  896/ 3200]\n",
            "loss: 1.202772  [  912/ 3200]\n",
            "loss: 1.178433  [  928/ 3200]\n",
            "loss: 0.803860  [  944/ 3200]\n",
            "loss: 0.809145  [  960/ 3200]\n",
            "loss: 0.968574  [  976/ 3200]\n",
            "loss: 0.841156  [  992/ 3200]\n",
            "loss: 0.819674  [ 1008/ 3200]\n",
            "loss: 0.908184  [ 1024/ 3200]\n",
            "loss: 1.025827  [ 1040/ 3200]\n",
            "loss: 0.913827  [ 1056/ 3200]\n",
            "loss: 0.826507  [ 1072/ 3200]\n",
            "loss: 0.826880  [ 1088/ 3200]\n",
            "loss: 0.969562  [ 1104/ 3200]\n",
            "loss: 0.922909  [ 1120/ 3200]\n",
            "loss: 1.063264  [ 1136/ 3200]\n",
            "loss: 1.197496  [ 1152/ 3200]\n",
            "loss: 0.927122  [ 1168/ 3200]\n",
            "loss: 1.017150  [ 1184/ 3200]\n",
            "loss: 0.923597  [ 1200/ 3200]\n",
            "loss: 0.818181  [ 1216/ 3200]\n",
            "loss: 1.053455  [ 1232/ 3200]\n",
            "loss: 1.094198  [ 1248/ 3200]\n",
            "loss: 0.802671  [ 1264/ 3200]\n",
            "loss: 0.669553  [ 1280/ 3200]\n",
            "loss: 0.877555  [ 1296/ 3200]\n",
            "loss: 0.901739  [ 1312/ 3200]\n",
            "loss: 0.677010  [ 1328/ 3200]\n",
            "loss: 0.761451  [ 1344/ 3200]\n",
            "loss: 0.898002  [ 1360/ 3200]\n",
            "loss: 1.026544  [ 1376/ 3200]\n",
            "loss: 0.750138  [ 1392/ 3200]\n",
            "loss: 0.951481  [ 1408/ 3200]\n",
            "loss: 0.956174  [ 1424/ 3200]\n",
            "loss: 0.886342  [ 1440/ 3200]\n",
            "loss: 1.117381  [ 1456/ 3200]\n",
            "loss: 0.604017  [ 1472/ 3200]\n",
            "loss: 0.808112  [ 1488/ 3200]\n",
            "loss: 1.028183  [ 1504/ 3200]\n",
            "loss: 0.919866  [ 1520/ 3200]\n",
            "loss: 1.214605  [ 1536/ 3200]\n",
            "loss: 0.950023  [ 1552/ 3200]\n",
            "loss: 0.830695  [ 1568/ 3200]\n",
            "loss: 1.306959  [ 1584/ 3200]\n",
            "loss: 1.240190  [ 1600/ 3200]\n",
            "loss: 0.799862  [ 1616/ 3200]\n",
            "loss: 0.835918  [ 1632/ 3200]\n",
            "loss: 0.957780  [ 1648/ 3200]\n",
            "loss: 0.991312  [ 1664/ 3200]\n",
            "loss: 1.321958  [ 1680/ 3200]\n",
            "loss: 0.782049  [ 1696/ 3200]\n",
            "loss: 0.814134  [ 1712/ 3200]\n",
            "loss: 0.910413  [ 1728/ 3200]\n",
            "loss: 0.835612  [ 1744/ 3200]\n",
            "loss: 0.857455  [ 1760/ 3200]\n",
            "loss: 0.941498  [ 1776/ 3200]\n",
            "loss: 1.000170  [ 1792/ 3200]\n",
            "loss: 0.967651  [ 1808/ 3200]\n",
            "loss: 1.482349  [ 1824/ 3200]\n",
            "loss: 0.988257  [ 1840/ 3200]\n",
            "loss: 0.653753  [ 1856/ 3200]\n",
            "loss: 0.976413  [ 1872/ 3200]\n",
            "loss: 0.766279  [ 1888/ 3200]\n",
            "loss: 0.843920  [ 1904/ 3200]\n",
            "loss: 1.249120  [ 1920/ 3200]\n",
            "loss: 0.813728  [ 1936/ 3200]\n",
            "loss: 1.121060  [ 1952/ 3200]\n",
            "loss: 1.204460  [ 1968/ 3200]\n",
            "loss: 0.791327  [ 1984/ 3200]\n",
            "loss: 0.919917  [ 2000/ 3200]\n",
            "loss: 1.299261  [ 2016/ 3200]\n",
            "loss: 0.906875  [ 2032/ 3200]\n",
            "loss: 0.706738  [ 2048/ 3200]\n",
            "loss: 0.919638  [ 2064/ 3200]\n",
            "loss: 0.912012  [ 2080/ 3200]\n",
            "loss: 0.800442  [ 2096/ 3200]\n",
            "loss: 0.791785  [ 2112/ 3200]\n",
            "loss: 1.152595  [ 2128/ 3200]\n",
            "loss: 0.856000  [ 2144/ 3200]\n",
            "loss: 1.024697  [ 2160/ 3200]\n",
            "loss: 0.979352  [ 2176/ 3200]\n",
            "loss: 1.010863  [ 2192/ 3200]\n",
            "loss: 0.708188  [ 2208/ 3200]\n",
            "loss: 1.117867  [ 2224/ 3200]\n",
            "loss: 1.145047  [ 2240/ 3200]\n",
            "loss: 0.733156  [ 2256/ 3200]\n",
            "loss: 0.729307  [ 2272/ 3200]\n",
            "loss: 0.911588  [ 2288/ 3200]\n",
            "loss: 0.961931  [ 2304/ 3200]\n",
            "loss: 0.965813  [ 2320/ 3200]\n",
            "loss: 1.243350  [ 2336/ 3200]\n",
            "loss: 1.098065  [ 2352/ 3200]\n",
            "loss: 1.011172  [ 2368/ 3200]\n",
            "loss: 0.982596  [ 2384/ 3200]\n",
            "loss: 0.659290  [ 2400/ 3200]\n",
            "loss: 0.760320  [ 2416/ 3200]\n",
            "loss: 0.911541  [ 2432/ 3200]\n",
            "loss: 0.839731  [ 2448/ 3200]\n",
            "loss: 1.098696  [ 2464/ 3200]\n",
            "loss: 0.836157  [ 2480/ 3200]\n",
            "loss: 0.924635  [ 2496/ 3200]\n",
            "loss: 0.890112  [ 2512/ 3200]\n",
            "loss: 0.794478  [ 2528/ 3200]\n",
            "loss: 0.652369  [ 2544/ 3200]\n",
            "loss: 0.736900  [ 2560/ 3200]\n",
            "loss: 1.047850  [ 2576/ 3200]\n",
            "loss: 1.129565  [ 2592/ 3200]\n",
            "loss: 0.698781  [ 2608/ 3200]\n",
            "loss: 0.862110  [ 2624/ 3200]\n",
            "loss: 0.942431  [ 2640/ 3200]\n",
            "loss: 0.707604  [ 2656/ 3200]\n",
            "loss: 0.625917  [ 2672/ 3200]\n",
            "loss: 1.176019  [ 2688/ 3200]\n",
            "loss: 0.775389  [ 2704/ 3200]\n",
            "loss: 0.687126  [ 2720/ 3200]\n",
            "loss: 1.078063  [ 2736/ 3200]\n",
            "loss: 0.998666  [ 2752/ 3200]\n",
            "loss: 0.743771  [ 2768/ 3200]\n",
            "loss: 1.118849  [ 2784/ 3200]\n",
            "loss: 0.819307  [ 2800/ 3200]\n",
            "loss: 0.928666  [ 2816/ 3200]\n",
            "loss: 0.915402  [ 2832/ 3200]\n",
            "loss: 1.055198  [ 2848/ 3200]\n",
            "loss: 1.027015  [ 2864/ 3200]\n",
            "loss: 0.765047  [ 2880/ 3200]\n",
            "loss: 0.902996  [ 2896/ 3200]\n",
            "loss: 0.864847  [ 2912/ 3200]\n",
            "loss: 0.815557  [ 2928/ 3200]\n",
            "loss: 0.735387  [ 2944/ 3200]\n",
            "loss: 1.454580  [ 2960/ 3200]\n",
            "loss: 0.941261  [ 2976/ 3200]\n",
            "loss: 0.941794  [ 2992/ 3200]\n",
            "loss: 0.902534  [ 3008/ 3200]\n",
            "loss: 0.766105  [ 3024/ 3200]\n",
            "loss: 1.155644  [ 3040/ 3200]\n",
            "loss: 0.769886  [ 3056/ 3200]\n",
            "loss: 0.591173  [ 3072/ 3200]\n",
            "loss: 1.101957  [ 3088/ 3200]\n",
            "loss: 1.234009  [ 3104/ 3200]\n",
            "loss: 1.077557  [ 3120/ 3200]\n",
            "loss: 0.764144  [ 3136/ 3200]\n",
            "loss: 0.595747  [ 3152/ 3200]\n",
            "loss: 0.848034  [ 3168/ 3200]\n",
            "loss: 1.003809  [ 3184/ 3200]\n",
            "Avg Accuracy: 61.625000%, Avg loss: 0.934364\n",
            "F1 score is: 0.5326865869760513\n",
            "Confusion Matrix:\n",
            "[[191   1   5   3]\n",
            " [ 91  36  59  14]\n",
            " [ 34   3 160   3]\n",
            " [ 27  24  43 106]]\n",
            "best epoch is: 26\n",
            "Avg Accuracy: 56.540698%, Avg loss: 1.014891\n",
            "F1 score is: 0.30274493862376656\n",
            "Confusion Matrix:\n",
            "[[269   7  16   5]\n",
            " [ 83  49 152  40]\n",
            " [ 69  16 289  25]\n",
            " [ 56  58  71 171]]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(1.0148908757972857,\n",
              " 0.30274493862376656,\n",
              " 0.565406976744186,\n",
              " tensor([[269,   7,  16,   5],\n",
              "         [ 83,  49, 152,  40],\n",
              "         [ 69,  16, 289,  25],\n",
              "         [ 56,  58,  71, 171]]))"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def train_and_evaluate(num_epochs, optimizer,train_dataloader,valid_dataloader,cost_func,model,device):\n",
        "  bestmodel = None\n",
        "  bestf1 = -1\n",
        "  bestepoch = -1\n",
        "  size = len(train_dataloader.dataset)\n",
        "  model.to(device)\n",
        "  for epoch in range(num_epochs):\n",
        "    print(f\"current epoch: {epoch}\\n\")\n",
        "    for batch, (X,y) in enumerate(train_dataloader):\n",
        "      X = X.to(device)\n",
        "      y = y.to(device)\n",
        "      pred = model(X.float())\n",
        "      loss = cost_func(pred,y)\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      loss, current = loss.item(), batch * len(X)\n",
        "      print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "    tl,f1, acc, conf = evaluate(valid_dataloader, cost_func, model,device)\n",
        "    if f1 > bestf1:\n",
        "      bestmodel = model\n",
        "      bestf1 = f1\n",
        "      bestepoch = epoch\n",
        "\n",
        "  print(f\"best epoch is: {bestepoch}\")\n",
        "  return bestmodel\n",
        "\n",
        "\n",
        "\n",
        "device = \"cpu\"\n",
        "model = NeuralNetwork().to(device)\n",
        "cost_func = nn.CrossEntropyLoss()\n",
        "learning_rate = 0.002\n",
        "num_epochs = 30\n",
        "batch_size = 16\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
        "bestmodel = train_and_evaluate(num_epochs, optimizer,train_dataloader,val_dataloader,cost_func,model,device)\n",
        "evaluate(test_dataloader, cost_func, bestmodel,device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We observe that the best result is obtained in epoch 26, not the last epoch. There are many reasons for this, one could be that in later epochs our model overfits the data. Another reason could be that we encounter a local minimum of the loss function at epoch 26."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nj3lFXVOdyrx"
      },
      "source": [
        "# Part 2 Convolutional Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1C1xLofd9WM"
      },
      "source": [
        "## Step 1 Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "OxNIEiloeGr8",
        "outputId": "f167d840-9d61-4853-af5c-d978eab31162"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 4 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "meltraindata = np.load('/content/drive/MyDrive/Machine_Learning/music_genre_data_di/train/melgrams/X.npy')\n",
        "meltrainlabels = np.load('/content/drive/MyDrive/Machine_Learning/music_genre_data_di/train/melgrams/labels.npy')\n",
        "\n",
        "meltestdata = np.load('/content/drive/MyDrive/Machine_Learning/music_genre_data_di/test/melgrams/X.npy')\n",
        "meltestlabels = np.load('/content/drive/MyDrive/Machine_Learning/music_genre_data_di/test/melgrams/labels.npy')\n",
        "\n",
        "melvaldata = np.load('/content/drive/MyDrive/Machine_Learning/music_genre_data_di/val/melgrams/X.npy')\n",
        "melvallabels = np.load('/content/drive/MyDrive/Machine_Learning/music_genre_data_di/val/melgrams/labels.npy')\n",
        "\n",
        "\n",
        "training_data = dataset(meltraindata, meltrainlabels, labels_map, torch.tensor)\n",
        "val_data = dataset(melvaldata, melvallabels, labels_map, torch.tensor)\n",
        "test_data = dataset(meltestdata, meltestlabels, labels_map, torch.tensor)\n",
        "\n",
        "train_dataloader = DataLoader(training_data, batch_size=16,shuffle=True )\n",
        "val_dataloader =  DataLoader(val_data, batch_size=16,shuffle=True )\n",
        "test_dataloader =  DataLoader(test_data, batch_size=16,shuffle=False)\n",
        "\n",
        "\n",
        "X, Y = training_data[range(0,len(training_data))]\n",
        "Y = torch.tensor(Y)\n",
        "indices_by_label = {label.item(): (Y == label).nonzero(as_tuple=True)[0].tolist() for label in torch.unique(Y)}\n",
        "# Select a random index for each label\n",
        "selected_indices = {label: random.choice(indices) for label, indices in indices_by_label.items()}\n",
        "# Retrieve the corresponding x values\n",
        "selected_x = {label: X[idx] for label, idx in selected_indices.items()}\n",
        "\n",
        "opp_map = {\n",
        "    0:\"classical\",\n",
        "    1:\"blues\",\n",
        "    2:\"rock_metal_hardrock\",\n",
        "    3:\"hiphop\",\n",
        "}\n",
        "\n",
        "for i,key in enumerate(selected_x.keys()):\n",
        "  img = selected_x[key]\n",
        "  plt.subplot(4,1,i+1)\n",
        "  plt.title(opp_map[key])\n",
        "  plt.axis(\"off\")\n",
        "  plt.imshow(img.squeeze())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAWiCZn7fWWw"
      },
      "source": [
        "## Step 2 Convolutional Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ucp_-1sKfcHa",
        "outputId": "9d3b728a-e371-402e-daa0-c19ed6e4c813"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LeNet(\n",
            "  (conv1): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (conv2): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (conv3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (conv4): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (linear1): Linear(in_features=71680, out_features=1024, bias=True)\n",
            "  (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "  (linear3): Linear(in_features=256, out_features=32, bias=True)\n",
            "  (linear4): Linear(in_features=32, out_features=4, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "device = 'cpu'\n",
        "\n",
        "class LeNet(nn.Module):\n",
        "  def __init__(self, padding=0, pooling=None, activation_function=None):\n",
        "      super(LeNet, self).__init__()\n",
        "      channels, height, width = (1,21, 128) #Mel Spectrograms input dimensions\n",
        "      self.activation_function = activation_function\n",
        "      self.pooling = pooling\n",
        "      self._conv_shape = None\n",
        "\n",
        "      self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5, stride=1, padding=padding)\n",
        "      self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=padding)\n",
        "      self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5, stride=1, padding=padding)\n",
        "      self.conv4 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=5, stride=1, padding=padding)\n",
        "\n",
        "      self._conv_layer_output_shape((channels, height, width)) # Calculate Convolutional Layer output\n",
        "\n",
        "      self.linear1 = nn.Linear(self._conv_shape, 1024)\n",
        "      self.linear2 = nn.Linear(1024, 256)\n",
        "      self.linear3 = nn.Linear(256,32)\n",
        "      self.linear4 = nn.Linear(32,4)\n",
        "\n",
        "\n",
        "\n",
        "  def _conv_layer_output_shape(self, shape):\n",
        "    x = torch.rand(1, *shape) # dummy\n",
        "    x = self._convolutional_layers(x)\n",
        "    self._conv_shape = x.numel()\n",
        "\n",
        "  def _convolutional_layers(self,x):\n",
        "    x = self.conv1(x)\n",
        "    if self.activation_function:\n",
        "      x = self.activation_function(x)\n",
        "    if self.pooling:\n",
        "      x = self.pooling(x)\n",
        "\n",
        "    x = self.conv2(x)\n",
        "    if self.activation_function:\n",
        "      x = self.activation_function(x)\n",
        "    if self.pooling:\n",
        "      x = self.pooling(x)\n",
        "\n",
        "    x = self.conv3(x)\n",
        "    if self.activation_function:\n",
        "      x = self.activation_function(x)\n",
        "    if self.pooling:\n",
        "      x = self.pooling(x)\n",
        "\n",
        "    x = self.conv4(x)\n",
        "    if self.activation_function:\n",
        "      x = self.activation_function(x)\n",
        "    if self.pooling:\n",
        "      x = self.pooling(x)\n",
        "    return x\n",
        "\n",
        "  def _linear_layers(self,x):\n",
        "    x = self.linear1(x)\n",
        "    if self.activation_function:\n",
        "      x = self.activation_function(x)\n",
        "    x = self.linear2(x)\n",
        "    if self.activation_function:\n",
        "      x = self.activation_function(x)\n",
        "    x = self.linear3(x)\n",
        "    if self.activation_function:\n",
        "      x = self.activation_function(x)\n",
        "    x = self.linear4(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self._convolutional_layers(x)\n",
        "    x = torch.flatten(x,1)\n",
        "    x = self._linear_layers(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "model = LeNet().to(device)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPS-oJY-5uSu"
      },
      "source": [
        "## Step 3 Training and Evaluation of our Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQhgQu0159rk",
        "outputId": "fb6a3800-129c-49b0-d0f4-a744b5ea5730"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "current epoch: 0\n",
            "\n",
            "Avg Accuracy: 47.750000%, Avg loss: 1.253927\n",
            "F1 score is: 0.38573354110121727\n",
            "Confusion Matrix:\n",
            "[[145   0  22  33]\n",
            " [ 55   0  65  80]\n",
            " [ 15   0  82 103]\n",
            " [ 12   0  33 155]]\n",
            "current epoch: 1\n",
            "\n",
            "Avg Accuracy: 45.250000%, Avg loss: 1.147988\n",
            "F1 score is: 0.3766078811883926\n",
            "Confusion Matrix:\n",
            "[[148  48   0   4]\n",
            " [ 52 128   2  18]\n",
            " [ 12 139   6  43]\n",
            " [ 16 103   1  80]]\n",
            "current epoch: 2\n",
            "\n",
            "Avg Accuracy: 40.500000%, Avg loss: 1.195562\n",
            "F1 score is: 0.33615219861268997\n",
            "Confusion Matrix:\n",
            "[[ 72 119   1   8]\n",
            " [ 10 176   4  10]\n",
            " [  1 166  22  11]\n",
            " [  1 142   3  54]]\n",
            "current epoch: 3\n",
            "\n",
            "Avg Accuracy: 53.250000%, Avg loss: 1.106028\n",
            "F1 score is: 0.4479525700211525\n",
            "Confusion Matrix:\n",
            "[[172  15  12   1]\n",
            " [ 70  59  64   7]\n",
            " [ 21  20 157   2]\n",
            " [ 33  17 112  38]]\n",
            "current epoch: 4\n",
            "\n",
            "Avg Accuracy: 51.375000%, Avg loss: 1.060820\n",
            "F1 score is: 0.44291103422641753\n",
            "Confusion Matrix:\n",
            "[[127  58   1  14]\n",
            " [ 30 136   3  31]\n",
            " [  3 115  13  69]\n",
            " [  7  57   1 135]]\n",
            "current epoch: 5\n",
            "\n",
            "Avg Accuracy: 58.000000%, Avg loss: 1.021038\n",
            "F1 score is: 0.5359918999671937\n",
            "Confusion Matrix:\n",
            "[[153  32   6   9]\n",
            " [ 55  88  18  39]\n",
            " [ 12  44  76  68]\n",
            " [ 19  20  14 147]]\n",
            "current epoch: 6\n",
            "\n",
            "Avg Accuracy: 58.125000%, Avg loss: 1.011042\n",
            "F1 score is: 0.5188909900188446\n",
            "Confusion Matrix:\n",
            "[[173  15   6   6]\n",
            " [ 69  60  28  43]\n",
            " [ 20  27  92  61]\n",
            " [ 28  13  19 140]]\n",
            "current epoch: 7\n",
            "\n",
            "Avg Accuracy: 55.500000%, Avg loss: 1.026394\n",
            "F1 score is: 0.4978403049707413\n",
            "Confusion Matrix:\n",
            "[[163  27   8   2]\n",
            " [ 62  89  43   6]\n",
            " [ 14  37 140   9]\n",
            " [ 30  36  82  52]]\n",
            "current epoch: 8\n",
            "\n",
            "Avg Accuracy: 50.750000%, Avg loss: 1.061783\n",
            "F1 score is: 0.4769261917471886\n",
            "Confusion Matrix:\n",
            "[[115  71  10   4]\n",
            " [ 25 149  19   7]\n",
            " [  2 103  90   5]\n",
            " [  5 104  39  52]]\n",
            "current epoch: 9\n",
            "\n",
            "Avg Accuracy: 61.500000%, Avg loss: 0.963198\n",
            "F1 score is: 0.5777849739789963\n",
            "Confusion Matrix:\n",
            "[[142  37  10  11]\n",
            " [ 38  88  35  39]\n",
            " [  6  36 111  47]\n",
            " [  8  14  27 151]]\n",
            "current epoch: 10\n",
            "\n",
            "Avg Accuracy: 59.750000%, Avg loss: 0.950343\n",
            "F1 score is: 0.5537724661827087\n",
            "Confusion Matrix:\n",
            "[[148  36   6  10]\n",
            " [ 41 101  18  40]\n",
            " [  8  59  75  58]\n",
            " [ 15  19  12 154]]\n",
            "current epoch: 11\n",
            "\n",
            "Avg Accuracy: 61.250000%, Avg loss: 0.957990\n",
            "F1 score is: 0.5854952275753021\n",
            "Confusion Matrix:\n",
            "[[130  48   9  13]\n",
            " [ 29 107  24  40]\n",
            " [  2  54  94  50]\n",
            " [  5  19  17 159]]\n",
            "current epoch: 12\n",
            "\n",
            "Avg Accuracy: 60.625000%, Avg loss: 0.953386\n",
            "F1 score is: 0.562589972615242\n",
            "Confusion Matrix:\n",
            "[[135  32  10  23]\n",
            " [ 29  74  36  61]\n",
            " [  4  20 106  70]\n",
            " [  6   6  18 170]]\n",
            "current epoch: 13\n",
            "\n",
            "Avg Accuracy: 59.250000%, Avg loss: 0.935474\n",
            "F1 score is: 0.5620677012205124\n",
            "Confusion Matrix:\n",
            "[[160  29   4   7]\n",
            " [ 57  90  28  25]\n",
            " [ 11  51 103  35]\n",
            " [ 25  27  27 121]]\n",
            "current epoch: 14\n",
            "\n",
            "Avg Accuracy: 57.250000%, Avg loss: 0.978073\n",
            "F1 score is: 0.5125193804502487\n",
            "Confusion Matrix:\n",
            "[[167  21   8   4]\n",
            " [ 60  76  53  11]\n",
            " [ 12  24 154  10]\n",
            " [ 30  22  87  61]]\n",
            "current epoch: 15\n",
            "\n",
            "Avg Accuracy: 62.250000%, Avg loss: 0.926092\n",
            "F1 score is: 0.5773976111412048\n",
            "Confusion Matrix:\n",
            "[[138  41  10  11]\n",
            " [ 31 102  24  43]\n",
            " [  4  52  97  47]\n",
            " [  6  16  17 161]]\n",
            "current epoch: 16\n",
            "\n",
            "Avg Accuracy: 60.250000%, Avg loss: 0.940161\n",
            "F1 score is: 0.5530713164806366\n",
            "Confusion Matrix:\n",
            "[[138  36  16  10]\n",
            " [ 32  75  60  33]\n",
            " [  4  19 157  20]\n",
            " [  7   8  73 112]]\n",
            "current epoch: 17\n",
            "\n",
            "Avg Accuracy: 62.125000%, Avg loss: 0.909152\n",
            "F1 score is: 0.5854592603445054\n",
            "Confusion Matrix:\n",
            "[[151  32   8   9]\n",
            " [ 40  88  35  37]\n",
            " [  7  38 122  33]\n",
            " [ 14  15  35 136]]\n",
            "current epoch: 18\n",
            "\n",
            "Avg Accuracy: 61.000000%, Avg loss: 0.917468\n",
            "F1 score is: 0.5795768219232559\n",
            "Confusion Matrix:\n",
            "[[146  40   7   7]\n",
            " [ 36 106  32  26]\n",
            " [  5  46 125  24]\n",
            " [ 11  37  41 111]]\n",
            "current epoch: 19\n",
            "\n",
            "Avg Accuracy: 59.000000%, Avg loss: 0.951833\n",
            "F1 score is: 0.5576374149322509\n",
            "Confusion Matrix:\n",
            "[[143  36  16   5]\n",
            " [ 35  91  58  16]\n",
            " [  5  27 158  10]\n",
            " [ 14  23  83  80]]\n",
            "current epoch: 20\n",
            "\n",
            "Avg Accuracy: 62.125000%, Avg loss: 0.920467\n",
            "F1 score is: 0.5822106641530991\n",
            "Confusion Matrix:\n",
            "[[135  46   7  12]\n",
            " [ 30 107  23  40]\n",
            " [  4  60  97  39]\n",
            " [  6  21  15 158]]\n",
            "current epoch: 21\n",
            "\n",
            "Avg Accuracy: 62.375000%, Avg loss: 0.905555\n",
            "F1 score is: 0.5761935520172119\n",
            "Confusion Matrix:\n",
            "[[148  28   8  16]\n",
            " [ 38  71  41  50]\n",
            " [  5  29 121  45]\n",
            " [  8  12  21 159]]\n",
            "current epoch: 22\n",
            "\n",
            "Avg Accuracy: 62.250000%, Avg loss: 0.906963\n",
            "F1 score is: 0.5854119998216629\n",
            "Confusion Matrix:\n",
            "[[150  30  11   9]\n",
            " [ 39  79  52  30]\n",
            " [  5  31 144  20]\n",
            " [ 14  13  48 125]]\n",
            "current epoch: 23\n",
            "\n",
            "Avg Accuracy: 60.875000%, Avg loss: 0.918764\n",
            "F1 score is: 0.5616076385974884\n",
            "Confusion Matrix:\n",
            "[[148  27  17   8]\n",
            " [ 39  62  67  32]\n",
            " [  4  19 159  18]\n",
            " [ 14   7  61 118]]\n",
            "current epoch: 24\n",
            "\n",
            "Avg Accuracy: 61.375000%, Avg loss: 0.922370\n",
            "F1 score is: 0.5572175550460815\n",
            "Confusion Matrix:\n",
            "[[148  28  16   8]\n",
            " [ 39  71  62  28]\n",
            " [  4  22 158  16]\n",
            " [ 13  12  61 114]]\n",
            "current epoch: 25\n",
            "\n",
            "Avg Accuracy: 61.375000%, Avg loss: 0.901766\n",
            "F1 score is: 0.5684706932306289\n",
            "Confusion Matrix:\n",
            "[[145  33  13   9]\n",
            " [ 37  80  50  33]\n",
            " [  5  34 138  23]\n",
            " [ 13  16  43 128]]\n",
            "current epoch: 26\n",
            "\n",
            "Avg Accuracy: 61.875000%, Avg loss: 0.908972\n",
            "F1 score is: 0.5836250513792038\n",
            "Confusion Matrix:\n",
            "[[156  32   6   6]\n",
            " [ 40 109  27  24]\n",
            " [  8  58 108  26]\n",
            " [ 19  29  30 122]]\n",
            "current epoch: 27\n",
            "\n",
            "Avg Accuracy: 60.500000%, Avg loss: 0.907708\n",
            "F1 score is: 0.5617945462465286\n",
            "Confusion Matrix:\n",
            "[[144  30  11  15]\n",
            " [ 37  64  53  46]\n",
            " [  5  27 138  30]\n",
            " [ 12  11  39 138]]\n",
            "current epoch: 28\n",
            "\n",
            "Avg Accuracy: 63.250000%, Avg loss: 0.893073\n",
            "F1 score is: 0.5878684139251709\n",
            "Confusion Matrix:\n",
            "[[154  32   5   9]\n",
            " [ 39  98  29  34]\n",
            " [  9  51 112  28]\n",
            " [ 18  17  23 142]]\n",
            "current epoch: 29\n",
            "\n",
            "Avg Accuracy: 62.625000%, Avg loss: 0.902103\n",
            "F1 score is: 0.5887675648927688\n",
            "Confusion Matrix:\n",
            "[[144  34  10  12]\n",
            " [ 33  90  43  34]\n",
            " [  6  34 134  26]\n",
            " [ 16  16  35 133]]\n",
            "best epoch is: 29\n",
            "\n",
            "Total Time for Training in CPU: 4293.326610088348\n",
            "Avg Accuracy: 56.758721%, Avg loss: 1.080199\n",
            "F1 score is: 0.2610119650963434\n",
            "Confusion Matrix:\n",
            "[[200  57  19  21]\n",
            " [ 32 125  96  71]\n",
            " [ 36  85 202  76]\n",
            " [ 27  31  44 254]]\n",
            "current epoch: 0\n",
            "\n",
            "Avg Accuracy: 62.875000%, Avg loss: 0.909322\n",
            "F1 score is: 0.5985443782806397\n",
            "Confusion Matrix:\n",
            "[[149  37   8   6]\n",
            " [ 36 114  31  19]\n",
            " [  7  51 121  21]\n",
            " [ 18  30  33 119]]\n",
            "current epoch: 1\n",
            "\n",
            "Avg Accuracy: 62.500000%, Avg loss: 0.918266\n",
            "F1 score is: 0.5994337558746338\n",
            "Confusion Matrix:\n",
            "[[146  36  11   7]\n",
            " [ 34 100  47  19]\n",
            " [  5  36 142  17]\n",
            " [ 16  27  45 112]]\n",
            "current epoch: 2\n",
            "\n",
            "Avg Accuracy: 62.250000%, Avg loss: 0.908573\n",
            "F1 score is: 0.5762072896957398\n",
            "Confusion Matrix:\n",
            "[[148  34  11   7]\n",
            " [ 35  94  46  25]\n",
            " [  7  37 134  22]\n",
            " [ 16  24  38 122]]\n",
            "current epoch: 3\n",
            "\n",
            "Avg Accuracy: 62.500000%, Avg loss: 0.919473\n",
            "F1 score is: 0.5937238138914108\n",
            "Confusion Matrix:\n",
            "[[150  34  12   4]\n",
            " [ 37  95  47  21]\n",
            " [  7  37 140  16]\n",
            " [ 17  26  42 115]]\n",
            "current epoch: 4\n",
            "\n",
            "Avg Accuracy: 61.375000%, Avg loss: 0.917905\n",
            "F1 score is: 0.5855778521299362\n",
            "Confusion Matrix:\n",
            "[[132  49  10   9]\n",
            " [ 28 109  34  29]\n",
            " [  4  54 117  25]\n",
            " [  8  28  31 133]]\n",
            "current epoch: 5\n",
            "\n",
            "Avg Accuracy: 62.500000%, Avg loss: 0.918522\n",
            "F1 score is: 0.5901324707269668\n",
            "Confusion Matrix:\n",
            "[[152  34  10   4]\n",
            " [ 37 101  43  19]\n",
            " [  7  43 130  20]\n",
            " [ 18  27  38 117]]\n",
            "current epoch: 6\n",
            "\n",
            "Avg Accuracy: 61.250000%, Avg loss: 0.930029\n",
            "F1 score is: 0.5828696155548095\n",
            "Confusion Matrix:\n",
            "[[140  45   9   6]\n",
            " [ 32 121  29  18]\n",
            " [  4  64 113  19]\n",
            " [ 15  41  28 116]]\n",
            "current epoch: 7\n",
            "\n",
            "Avg Accuracy: 62.000000%, Avg loss: 0.915994\n",
            "F1 score is: 0.5763032042980194\n",
            "Confusion Matrix:\n",
            "[[148  33  12   7]\n",
            " [ 35  90  48  27]\n",
            " [  6  36 134  24]\n",
            " [ 17  23  36 124]]\n",
            "current epoch: 8\n",
            "\n",
            "Avg Accuracy: 60.750000%, Avg loss: 0.919850\n",
            "F1 score is: 0.5609079575538636\n",
            "Confusion Matrix:\n",
            "[[146  25  15  14]\n",
            " [ 35  60  56  49]\n",
            " [  6  26 138  30]\n",
            " [ 12  12  34 142]]\n",
            "current epoch: 9\n",
            "\n",
            "Avg Accuracy: 62.000000%, Avg loss: 0.926881\n",
            "F1 score is: 0.5772702550888061\n",
            "Confusion Matrix:\n",
            "[[148  31  14   7]\n",
            " [ 33  84  52  31]\n",
            " [  6  34 137  23]\n",
            " [ 13  21  39 127]]\n",
            "current epoch: 10\n",
            "\n",
            "Avg Accuracy: 62.375000%, Avg loss: 0.928626\n",
            "F1 score is: 0.5745150560140609\n",
            "Confusion Matrix:\n",
            "[[154  28  12   6]\n",
            " [ 40  81  55  24]\n",
            " [  7  34 140  19]\n",
            " [ 18  21  37 124]]\n",
            "current epoch: 11\n",
            "\n",
            "Avg Accuracy: 62.125000%, Avg loss: 0.931787\n",
            "F1 score is: 0.5766368424892425\n",
            "Confusion Matrix:\n",
            "[[150  34  11   5]\n",
            " [ 35  97  49  19]\n",
            " [  6  38 139  17]\n",
            " [ 16  28  45 111]]\n",
            "current epoch: 12\n",
            "\n",
            "Avg Accuracy: 62.500000%, Avg loss: 0.914242\n",
            "F1 score is: 0.5817094093561173\n",
            "Confusion Matrix:\n",
            "[[150  32  11   7]\n",
            " [ 36  85  43  36]\n",
            " [  7  37 129  27]\n",
            " [ 15  20  29 136]]\n",
            "current epoch: 13\n",
            "\n",
            "Avg Accuracy: 61.875000%, Avg loss: 0.931058\n",
            "F1 score is: 0.5736909490823746\n",
            "Confusion Matrix:\n",
            "[[155  24  15   6]\n",
            " [ 40  76  57  27]\n",
            " [  7  29 144  20]\n",
            " [ 17  20  43 120]]\n",
            "current epoch: 14\n",
            "\n",
            "Avg Accuracy: 62.125000%, Avg loss: 0.932025\n",
            "F1 score is: 0.5883792781829834\n",
            "Confusion Matrix:\n",
            "[[156  24  14   6]\n",
            " [ 42  83  53  22]\n",
            " [  7  34 141  18]\n",
            " [ 18  21  44 117]]\n",
            "current epoch: 15\n",
            "\n",
            "Avg Accuracy: 61.625000%, Avg loss: 0.934472\n",
            "F1 score is: 0.5833753022551537\n",
            "Confusion Matrix:\n",
            "[[145  33  16   6]\n",
            " [ 34  85  55  26]\n",
            " [  6  35 140  19]\n",
            " [ 12  22  43 123]]\n",
            "current epoch: 16\n",
            "\n",
            "Avg Accuracy: 61.875000%, Avg loss: 0.940111\n",
            "F1 score is: 0.587836646437645\n",
            "Confusion Matrix:\n",
            "[[151  29  14   6]\n",
            " [ 37  90  53  20]\n",
            " [  7  36 138  19]\n",
            " [ 15  25  44 116]]\n",
            "current epoch: 17\n",
            "\n",
            "Avg Accuracy: 62.125000%, Avg loss: 0.926734\n",
            "F1 score is: 0.5927492755651474\n",
            "Confusion Matrix:\n",
            "[[152  32  10   6]\n",
            " [ 41  91  42  26]\n",
            " [  8  46 121  25]\n",
            " [ 16  24  27 133]]\n",
            "current epoch: 18\n",
            "\n",
            "Avg Accuracy: 61.625000%, Avg loss: 0.935833\n",
            "F1 score is: 0.5784330600500107\n",
            "Confusion Matrix:\n",
            "[[140  38  15   7]\n",
            " [ 33  87  49  31]\n",
            " [  5  37 133  25]\n",
            " [ 11  22  34 133]]\n",
            "current epoch: 19\n",
            "\n",
            "Avg Accuracy: 61.375000%, Avg loss: 0.936568\n",
            "F1 score is: 0.5754067146778107\n",
            "Confusion Matrix:\n",
            "[[152  28  14   6]\n",
            " [ 40  77  52  31]\n",
            " [  7  37 135  21]\n",
            " [ 14  21  38 127]]\n",
            "current epoch: 20\n",
            "\n",
            "Avg Accuracy: 61.500000%, Avg loss: 0.943030\n",
            "F1 score is: 0.5834107893705368\n",
            "Confusion Matrix:\n",
            "[[138  36  19   7]\n",
            " [ 33  80  50  37]\n",
            " [  5  34 132  29]\n",
            " [ 11  19  28 142]]\n",
            "current epoch: 21\n",
            "\n",
            "Avg Accuracy: 61.875000%, Avg loss: 0.948675\n",
            "F1 score is: 0.5779917275905609\n",
            "Confusion Matrix:\n",
            "[[153  27  14   6]\n",
            " [ 41  90  47  22]\n",
            " [  8  41 132  19]\n",
            " [ 17  27  36 120]]\n",
            "current epoch: 22\n",
            "\n",
            "Avg Accuracy: 61.500000%, Avg loss: 0.952463\n",
            "F1 score is: 0.5570171064138413\n",
            "Confusion Matrix:\n",
            "[[154  29  11   6]\n",
            " [ 41  76  43  40]\n",
            " [  9  39 124  28]\n",
            " [ 18  20  24 138]]\n",
            "current epoch: 23\n",
            "\n",
            "Avg Accuracy: 61.375000%, Avg loss: 0.970937\n",
            "F1 score is: 0.5758756357431412\n",
            "Confusion Matrix:\n",
            "[[151  25  18   6]\n",
            " [ 41  78  58  23]\n",
            " [  7  30 148  15]\n",
            " [ 17  21  48 114]]\n",
            "current epoch: 24\n",
            "\n",
            "Avg Accuracy: 60.250000%, Avg loss: 1.004232\n",
            "F1 score is: 0.5612447619438171\n",
            "Confusion Matrix:\n",
            "[[151  25  18   6]\n",
            " [ 43  83  58  16]\n",
            " [  7  31 154   8]\n",
            " [ 17  26  63  94]]\n",
            "current epoch: 25\n",
            "\n",
            "Avg Accuracy: 61.500000%, Avg loss: 0.961473\n",
            "F1 score is: 0.5635354465246201\n",
            "Confusion Matrix:\n",
            "[[148  29  17   6]\n",
            " [ 40  72  53  35]\n",
            " [  7  29 136  28]\n",
            " [ 14  19  31 136]]\n",
            "current epoch: 26\n",
            "\n",
            "Avg Accuracy: 61.250000%, Avg loss: 0.962191\n",
            "F1 score is: 0.5876951152086258\n",
            "Confusion Matrix:\n",
            "[[149  30  15   6]\n",
            " [ 39  95  42  24]\n",
            " [  8  47 122  23]\n",
            " [ 15  32  29 124]]\n",
            "current epoch: 27\n",
            "\n",
            "Avg Accuracy: 60.375000%, Avg loss: 0.975839\n",
            "F1 score is: 0.5697280460596085\n",
            "Confusion Matrix:\n",
            "[[149  29  16   6]\n",
            " [ 41  87  50  22]\n",
            " [  8  41 133  18]\n",
            " [ 15  28  43 114]]\n",
            "current epoch: 28\n",
            "\n",
            "Avg Accuracy: 60.875000%, Avg loss: 0.966985\n",
            "F1 score is: 0.5835439485311508\n",
            "Confusion Matrix:\n",
            "[[144  33  17   6]\n",
            " [ 37  99  41  23]\n",
            " [  7  50 124  19]\n",
            " [ 14  35  31 120]]\n",
            "current epoch: 29\n",
            "\n",
            "Avg Accuracy: 63.000000%, Avg loss: 0.960533\n",
            "F1 score is: 0.6023683333396912\n",
            "Confusion Matrix:\n",
            "[[146  32  16   6]\n",
            " [ 37  99  40  24]\n",
            " [  6  47 124  23]\n",
            " [ 14  28  23 135]]\n",
            "best epoch is: 29\n",
            "\n",
            "Total Time for Training in GPU: 120.61348843574524\n",
            "Avg Accuracy: 55.087209%, Avg loss: 1.196429\n",
            "F1 score is: 0.236123317679347\n",
            "Confusion Matrix:\n",
            "[[197  60  16  24]\n",
            " [ 33 140  84  67]\n",
            " [ 36 112 181  70]\n",
            " [ 24  52  40 240]]\n"
          ]
        }
      ],
      "source": [
        "def trainCNN(num_epochs, optimizer, dataloader,cost_func,model, device):\n",
        "  model.to(device)\n",
        "  model.train()\n",
        "  size = len(dataloader.dataset)\n",
        "  for epoch in range(num_epochs):\n",
        "    print(f\"current epoch: {epoch}\\n\")\n",
        "    for batch, (X,y) in enumerate(dataloader):\n",
        "      X = X.to(device)\n",
        "      y = y.to(device)\n",
        "      X = torch.unsqueeze(X,1)\n",
        "      pred = model(X.float())\n",
        "      loss = cost_func(pred,y)\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      loss, current = loss.item(), batch * len(X)\n",
        "      print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "  return model\n",
        "\n",
        "def evaluateCNN(dataloader, cost_func, model, device):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    test_loss = 0.0\n",
        "    test_f1 = 0.0\n",
        "    test_acc = 0.0\n",
        "    correct = 0\n",
        "    size = len(dataloader.dataset)\n",
        "    accuracy = Accuracy(task=\"multiclass\", num_classes=4).to(device)\n",
        "    f1 = F1Score(task=\"multiclass\", num_classes=4, average=\"macro\").to(device)\n",
        "    confmatrix = ConfusionMatrix(task=\"multiclass\", num_classes=4).to(device)\n",
        "\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch, (X, y) in enumerate(dataloader):\n",
        "            X = X.to(device).float()\n",
        "            y = y.to(device)\n",
        "            X = torch.unsqueeze(X,1)\n",
        "            pred = model(X)\n",
        "            all_preds.append(pred)\n",
        "            all_labels.append(y)\n",
        "\n",
        "            test_loss += cost_func(pred, y).item() * X.size(0)\n",
        "            test_acc += accuracy(pred, y).item() * X.size(0)\n",
        "            test_f1 += f1(pred, y).item() * X.size(0)\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    test_loss /= size\n",
        "    test_acc /= size\n",
        "    test_f1 /= size\n",
        "\n",
        "    all_preds = torch.cat(all_preds)\n",
        "    all_labels = torch.cat(all_labels)\n",
        "    confusion_matrix = confmatrix(all_preds.argmax(dim=1), all_labels)\n",
        "\n",
        "    print(f\"Avg Accuracy: {100*test_acc:>8f}%, Avg loss: {test_loss:>8f}\")\n",
        "    print(f\"F1 score is: {test_f1}\")\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrix.cpu().numpy())\n",
        "\n",
        "    return test_loss, test_f1, test_acc, confusion_matrix\n",
        "\n",
        "def train_and_evaluateCNN(num_epochs, optimizer,train_dataloader,valid_dataloader,cost_func,model,device):\n",
        "  bestmodel = None\n",
        "  bestf1 = -1\n",
        "  bestepoch = -1\n",
        "  size = len(train_dataloader.dataset)\n",
        "  model.to(device)\n",
        "  for epoch in range(num_epochs):\n",
        "    print(f\"current epoch: {epoch}\\n\")\n",
        "    for batch, (X,y) in enumerate(train_dataloader):\n",
        "      X = X.to(device)\n",
        "      y = y.to(device)\n",
        "      X = torch.unsqueeze(X,1)\n",
        "      pred = model(X.float())\n",
        "      loss = cost_func(pred,y)\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      loss, current = loss.item(), batch * len(X)\n",
        "      #Disable printing for this step, to save time\n",
        "      #print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "    tl,f1, acc, conf = evaluateCNN(valid_dataloader, cost_func, model,device)\n",
        "    if f1 > bestf1:\n",
        "      bestmodel = model\n",
        "      bestf1 = f1\n",
        "      bestepoch = epoch\n",
        "\n",
        "  print(f\"best epoch is: {bestepoch}\")\n",
        "  return bestmodel\n",
        "\n",
        "\n",
        "device = \"cpu\"\n",
        "\n",
        "dev_names = {\n",
        "    \"cpu\" : \"CPU\",\n",
        "    \"cuda\" : \"GPU\"\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "model = LeNet().to(device)\n",
        "cost_func = nn.CrossEntropyLoss()\n",
        "learning_rate = 0.0001  # Start with a lower learning rate\n",
        "#when using learning rate 0.002 we encountered the exploding gradients problem...\n",
        "num_epochs = 30\n",
        "batch_size = 16\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
        "for dev, name in dev_names.items():\n",
        "  start_time = time.time()\n",
        "  model = train_and_evaluateCNN(num_epochs, optimizer, train_dataloader,val_dataloader, cost_func, model,dev)\n",
        "  print(f\"\\nTotal Time for Training in {name}: {time.time() - start_time}\")\n",
        "  evaluateCNN(test_dataloader, cost_func, model,dev)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Total Time for Training in CPU: 4293.326610088348\n",
        "Avg Accuracy: 56.758721%, Avg loss: 1.080199\n",
        "F1 score is: 0.2610119650963434\n",
        "Total Time for Training in GPU: 120.61348843574524\n",
        "Avg Accuracy: 55.087209%, Avg loss: 1.196429\n",
        "F1 score is: 0.236123317679347\n",
        "\n",
        "Overall the training results are very bad. This can be explained by the absence of pooling and padding.\n",
        "Without pooling, the feature maps retain redundant information, which makes learning less efficient and slower. This is especially visible in the \n",
        "disparity between CPU and GPU time(in contrast to part 1, CPU time sky rockets due to computational complexity redundant features).\n",
        "Without padding, we run the risk of losing important edge-related features, as the convolution operation near the edges of the image has fewer neighboring pixels to process.\n",
        "This explains the especially low F1 score we got. Adding both pooling and padding to our model greatly improves performance as seen below.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmPBJWQU9F1o"
      },
      "source": [
        "## Step 4 Pooling and Padding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4lgGJ3nz7DSa",
        "outputId": "2922cace-5a1a-4f19-fcee-3b596b9bde5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "current epoch: 0\n",
            "\n",
            "Avg Accuracy: 25.250000%, Avg loss: 1.373792\n",
            "F1 score is: 0.12182024221867323\n",
            "Confusion Matrix:\n",
            "[[  4   0   0 196]\n",
            " [  7  11   0 182]\n",
            " [  3  18   0 179]\n",
            " [ 10   3   0 187]]\n",
            "current epoch: 1\n",
            "\n",
            "Avg Accuracy: 30.500000%, Avg loss: 1.367544\n",
            "F1 score is: 0.19627639040350914\n",
            "Confusion Matrix:\n",
            "[[ 72   0   0 128]\n",
            " [ 24   0   0 176]\n",
            " [ 13   2   0 185]\n",
            " [ 28   0   0 172]]\n",
            "current epoch: 2\n",
            "\n",
            "Avg Accuracy: 27.500000%, Avg loss: 1.360741\n",
            "F1 score is: 0.14218721825629474\n",
            "Confusion Matrix:\n",
            "[[ 22   0   0 178]\n",
            " [  9   0   0 191]\n",
            " [  0   0   0 200]\n",
            " [  2   0   0 198]]\n",
            "current epoch: 3\n",
            "\n",
            "Avg Accuracy: 40.125000%, Avg loss: 1.355033\n",
            "F1 score is: 0.2772618062794209\n",
            "Confusion Matrix:\n",
            "[[132   0   0  68]\n",
            " [ 34   1   0 165]\n",
            " [  9   0   0 191]\n",
            " [ 12   0   0 188]]\n",
            "current epoch: 4\n",
            "\n",
            "Avg Accuracy: 37.875000%, Avg loss: 1.348113\n",
            "F1 score is: 0.2590839923918247\n",
            "Confusion Matrix:\n",
            "[[104   0   0  96]\n",
            " [ 20   0   0 180]\n",
            " [  4   0   0 196]\n",
            " [  1   0   0 199]]\n",
            "current epoch: 5\n",
            "\n",
            "Avg Accuracy: 37.750000%, Avg loss: 1.340876\n",
            "F1 score is: 0.2524603056907654\n",
            "Confusion Matrix:\n",
            "[[100   1   0  99]\n",
            " [ 16   2   0 182]\n",
            " [  4   0   0 196]\n",
            " [  0   0   0 200]]\n",
            "current epoch: 6\n",
            "\n",
            "Avg Accuracy: 46.000000%, Avg loss: 1.334330\n",
            "F1 score is: 0.328724727332592\n",
            "Confusion Matrix:\n",
            "[[177   3   0  20]\n",
            " [ 48  12   0 140]\n",
            " [ 19  17   0 164]\n",
            " [ 18   3   0 179]]\n",
            "current epoch: 7\n",
            "\n",
            "Avg Accuracy: 43.500000%, Avg loss: 1.325740\n",
            "F1 score is: 0.3081185194849968\n",
            "Confusion Matrix:\n",
            "[[145   4   0  51]\n",
            " [ 29   7   0 164]\n",
            " [  8   7   0 185]\n",
            " [  2   2   0 196]]\n",
            "current epoch: 8\n",
            "\n",
            "Avg Accuracy: 44.500000%, Avg loss: 1.316812\n",
            "F1 score is: 0.31119418621063233\n",
            "Confusion Matrix:\n",
            "[[160   0   0  40]\n",
            " [ 33   1   0 166]\n",
            " [ 10   1   0 189]\n",
            " [  5   0   0 195]]\n",
            "current epoch: 9\n",
            "\n",
            "Avg Accuracy: 45.875000%, Avg loss: 1.307499\n",
            "F1 score is: 0.3180063804984093\n",
            "Confusion Matrix:\n",
            "[[179   0   0  21]\n",
            " [ 50   0   0 150]\n",
            " [ 23   0   0 177]\n",
            " [ 12   0   0 188]]\n",
            "current epoch: 10\n",
            "\n",
            "Avg Accuracy: 46.125000%, Avg loss: 1.296363\n",
            "F1 score is: 0.323298180103302\n",
            "Confusion Matrix:\n",
            "[[178   1   0  21]\n",
            " [ 50   7   0 143]\n",
            " [ 22   7   0 171]\n",
            " [ 13   3   0 184]]\n",
            "current epoch: 11\n",
            "\n",
            "Avg Accuracy: 46.125000%, Avg loss: 1.284919\n",
            "F1 score is: 0.3137546992301941\n",
            "Confusion Matrix:\n",
            "[[178   0   0  22]\n",
            " [ 51   5   0 144]\n",
            " [ 22   5   1 172]\n",
            " [ 13   2   0 185]]\n",
            "current epoch: 12\n",
            "\n",
            "Avg Accuracy: 47.125000%, Avg loss: 1.270643\n",
            "F1 score is: 0.342336388528347\n",
            "Confusion Matrix:\n",
            "[[187   0   0  13]\n",
            " [ 65  15   0 120]\n",
            " [ 27  17   0 156]\n",
            " [ 21   4   0 175]]\n",
            "current epoch: 13\n",
            "\n",
            "Avg Accuracy: 47.000000%, Avg loss: 1.257546\n",
            "F1 score is: 0.3388886868953705\n",
            "Confusion Matrix:\n",
            "[[195   0   0   5]\n",
            " [ 94  15   0  91]\n",
            " [ 40  19   6 135]\n",
            " [ 37   3   0 160]]\n",
            "current epoch: 14\n",
            "\n",
            "Avg Accuracy: 49.000000%, Avg loss: 1.243062\n",
            "F1 score is: 0.37529519349336626\n",
            "Confusion Matrix:\n",
            "[[196   0   0   4]\n",
            " [108  13   1  78]\n",
            " [ 45   9  28 118]\n",
            " [ 37   4   4 155]]\n",
            "current epoch: 15\n",
            "\n",
            "Avg Accuracy: 50.125000%, Avg loss: 1.222808\n",
            "F1 score is: 0.39769540503621104\n",
            "Confusion Matrix:\n",
            "[[185   1   0  14]\n",
            " [ 69  28   2 101]\n",
            " [ 26  16  24 134]\n",
            " [ 21   8   7 164]]\n",
            "current epoch: 16\n",
            "\n",
            "Avg Accuracy: 49.875000%, Avg loss: 1.203784\n",
            "F1 score is: 0.4043560430407524\n",
            "Confusion Matrix:\n",
            "[[169  16   0  15]\n",
            " [ 48  63   0  89]\n",
            " [ 19  57   0 124]\n",
            " [ 13  20   0 167]]\n",
            "current epoch: 17\n",
            "\n",
            "Avg Accuracy: 51.375000%, Avg loss: 1.187127\n",
            "F1 score is: 0.4139029759168625\n",
            "Confusion Matrix:\n",
            "[[185   9   0   6]\n",
            " [ 60  69   1  70]\n",
            " [ 24  72   6  98]\n",
            " [ 22  25   2 151]]\n",
            "current epoch: 18\n",
            "\n",
            "Avg Accuracy: 50.250000%, Avg loss: 1.176557\n",
            "F1 score is: 0.41326903223991396\n",
            "Confusion Matrix:\n",
            "[[170  27   0   3]\n",
            " [ 48 130   0  22]\n",
            " [ 17 136   2  45]\n",
            " [ 13  86   1 100]]\n",
            "current epoch: 19\n",
            "\n",
            "Avg Accuracy: 50.250000%, Avg loss: 1.148755\n",
            "F1 score is: 0.39955752551555634\n",
            "Confusion Matrix:\n",
            "[[185   2   0  13]\n",
            " [ 64  36   2  98]\n",
            " [ 28  19  15 138]\n",
            " [ 21   8   5 166]]\n",
            "current epoch: 20\n",
            "\n",
            "Avg Accuracy: 50.250000%, Avg loss: 1.139208\n",
            "F1 score is: 0.43006212890148166\n",
            "Confusion Matrix:\n",
            "[[158  37   0   5]\n",
            " [ 38 117   1  44]\n",
            " [ 13 109   0  78]\n",
            " [  5  68   0 127]]\n",
            "current epoch: 21\n",
            "\n",
            "Avg Accuracy: 53.750000%, Avg loss: 1.134265\n",
            "F1 score is: 0.48401481136679647\n",
            "Confusion Matrix:\n",
            "[[166  29   2   3]\n",
            " [ 43 126  13  18]\n",
            " [ 16 102  47  35]\n",
            " [  7  84  18  91]]\n",
            "current epoch: 22\n",
            "\n",
            "Avg Accuracy: 51.625000%, Avg loss: 1.102694\n",
            "F1 score is: 0.4244030964374542\n",
            "Confusion Matrix:\n",
            "[[181   4   6   9]\n",
            " [ 57  34   7 102]\n",
            " [ 22  12  29 137]\n",
            " [ 16   8   7 169]]\n",
            "current epoch: 23\n",
            "\n",
            "Avg Accuracy: 53.250000%, Avg loss: 1.104952\n",
            "F1 score is: 0.49810334503650666\n",
            "Confusion Matrix:\n",
            "[[140  53   4   3]\n",
            " [ 30 118  20  32]\n",
            " [ 10  83  55  52]\n",
            " [  5  62  20 113]]\n",
            "current epoch: 24\n",
            "\n",
            "Avg Accuracy: 46.625000%, Avg loss: 1.110462\n",
            "F1 score is: 0.38740599125623704\n",
            "Confusion Matrix:\n",
            "[[157  41   1   1]\n",
            " [ 40 149   0  11]\n",
            " [ 14 170   0  16]\n",
            " [  5 128   0  67]]\n",
            "current epoch: 25\n",
            "\n",
            "Avg Accuracy: 52.750000%, Avg loss: 1.072562\n",
            "F1 score is: 0.4823756164312363\n",
            "Confusion Matrix:\n",
            "[[145  51   1   3]\n",
            " [ 34 102  11  53]\n",
            " [ 11  76  32  81]\n",
            " [  5  40  12 143]]\n",
            "current epoch: 26\n",
            "\n",
            "Avg Accuracy: 55.625000%, Avg loss: 1.069729\n",
            "F1 score is: 0.4759586027264595\n",
            "Confusion Matrix:\n",
            "[[197   1   1   1]\n",
            " [ 88  57  14  41]\n",
            " [ 35  44  54  67]\n",
            " [ 27  22  14 137]]\n",
            "current epoch: 27\n",
            "\n",
            "Avg Accuracy: 55.375000%, Avg loss: 1.049963\n",
            "F1 score is: 0.4683769419789314\n",
            "Confusion Matrix:\n",
            "[[178   1  19   2]\n",
            " [ 56  16  63  65]\n",
            " [ 20   6  95  79]\n",
            " [ 14   3  29 154]]\n",
            "current epoch: 28\n",
            "\n",
            "Avg Accuracy: 54.750000%, Avg loss: 1.035371\n",
            "F1 score is: 0.4543235784769058\n",
            "Confusion Matrix:\n",
            "[[192   2   4   2]\n",
            " [ 75  37  11  77]\n",
            " [ 28  12  46 114]\n",
            " [ 18   8  11 163]]\n",
            "current epoch: 29\n",
            "\n",
            "Avg Accuracy: 52.000000%, Avg loss: 1.029489\n",
            "F1 score is: 0.42119184374809265\n",
            "Confusion Matrix:\n",
            "[[177   8   7   8]\n",
            " [ 48  29  11 112]\n",
            " [ 17  12  29 142]\n",
            " [  6   8   5 181]]\n",
            "best epoch is: 23\n",
            "\n",
            "Total Time for Training in CPU with pooling and padding: 257.3441216945648\n",
            "Avg Accuracy: 49.854651%, Avg loss: 1.043344\n",
            "F1 score is: 0.3274279066989588\n",
            "Confusion Matrix:\n",
            "[[257  10   5  25]\n",
            " [ 38  49  45 192]\n",
            " [ 36  33  47 283]\n",
            " [ 11   6   6 333]]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(1.0433437999251276,\n",
              " 0.3274279066989588,\n",
              " 0.498546511627907,\n",
              " tensor([[257,  10,   5,  25],\n",
              "         [ 38,  49,  45, 192],\n",
              "         [ 36,  33,  47, 283],\n",
              "         [ 11,   6,   6, 333]]))"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = \"cpu\"\n",
        "model = LeNet(padding=2, pooling=nn.MaxPool2d(kernel_size=2)).to(device)\n",
        "cost_func = nn.CrossEntropyLoss()\n",
        "learning_rate = 0.0001  # Start with a lower learning rate\n",
        "#when using learning rate 0.002 we encountered the exploding gradients problem...\n",
        "num_epochs = 30\n",
        "batch_size = 16\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
        "start_time = time.time()\n",
        "model = train_and_evaluateCNN(num_epochs, optimizer, train_dataloader,val_dataloader,cost_func, model,device)\n",
        "print(f\"\\nTotal Time for Training in CPU with pooling and padding: {time.time() - start_time}\")\n",
        "evaluateCNN(test_dataloader, cost_func, model,device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Total Time for Training in CPU with pooling and padding: 257.3441216945648\n",
        "Avg Accuracy: 49.854651%, Avg loss: 1.043344\n",
        "F1 score is: 0.3274279066989588\n",
        "\n",
        "As expected pooling and padding greatly reduce training times and increase F1 score as many featurs are dropped due to pooling and at the same time important information is preserved with the help of padding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFe7KEdX9R8n"
      },
      "source": [
        "##Step 5 Using the ReLU Activation Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9p3MM729Rfn",
        "outputId": "0ed58b31-c101-4c90-ef1f-e72bf41d869b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "current epoch: 0\n",
            "\n",
            "Avg Accuracy: 25.250000%, Avg loss: 1.380531\n",
            "F1 score is: 0.1032032086700201\n",
            "Confusion Matrix:\n",
            "[[  0   2 195   3]\n",
            " [  0   0 200   0]\n",
            " [  0   0 199   1]\n",
            " [  0   1 196   3]]\n",
            "current epoch: 1\n",
            "\n",
            "Avg Accuracy: 43.250000%, Avg loss: 1.377421\n",
            "F1 score is: 0.2850545761734247\n",
            "Confusion Matrix:\n",
            "[[  0   4  88 108]\n",
            " [  0   1 123  76]\n",
            " [  0   0 176  24]\n",
            " [  0   0  31 169]]\n",
            "current epoch: 2\n",
            "\n",
            "Avg Accuracy: 36.875000%, Avg loss: 1.375229\n",
            "F1 score is: 0.2543098609149456\n",
            "Confusion Matrix:\n",
            "[[  0   2  18 180]\n",
            " [  0   2  22 176]\n",
            " [  0   0  94 106]\n",
            " [  0   0   1 199]]\n",
            "current epoch: 3\n",
            "\n",
            "Avg Accuracy: 28.625000%, Avg loss: 1.373528\n",
            "F1 score is: 0.15270504932850598\n",
            "Confusion Matrix:\n",
            "[[  0   0   2 198]\n",
            " [  0   0   1 199]\n",
            " [  0   0  29 171]\n",
            " [  0   0   0 200]]\n",
            "current epoch: 4\n",
            "\n",
            "Avg Accuracy: 25.875000%, Avg loss: 1.372214\n",
            "F1 score is: 0.11313771177083254\n",
            "Confusion Matrix:\n",
            "[[  0   0   0 200]\n",
            " [  0   0   0 200]\n",
            " [  0   0   7 193]\n",
            " [  0   0   0 200]]\n",
            "current epoch: 5\n",
            "\n",
            "Avg Accuracy: 25.250000%, Avg loss: 1.371097\n",
            "F1 score is: 0.10089328218251467\n",
            "Confusion Matrix:\n",
            "[[  0   0   0 200]\n",
            " [  0   0   0 200]\n",
            " [  0   0   2 198]\n",
            " [  0   0   0 200]]\n",
            "current epoch: 6\n",
            "\n",
            "Avg Accuracy: 25.125000%, Avg loss: 1.370146\n",
            "F1 score is: 0.09908356718719005\n",
            "Confusion Matrix:\n",
            "[[  0   0   0 200]\n",
            " [  0   0   0 200]\n",
            " [  0   0   1 199]\n",
            " [  0   0   0 200]]\n",
            "current epoch: 7\n",
            "\n",
            "Avg Accuracy: 25.125000%, Avg loss: 1.369337\n",
            "F1 score is: 0.09921303514391183\n",
            "Confusion Matrix:\n",
            "[[  0   0   0 200]\n",
            " [  0   0   0 200]\n",
            " [  0   0   1 199]\n",
            " [  0   0   0 200]]\n",
            "current epoch: 8\n",
            "\n",
            "Avg Accuracy: 25.000000%, Avg loss: 1.368515\n",
            "F1 score is: 0.0980501051619649\n",
            "Confusion Matrix:\n",
            "[[  0   0   0 200]\n",
            " [  0   0   0 200]\n",
            " [  0   0   0 200]\n",
            " [  0   0   0 200]]\n",
            "current epoch: 9\n",
            "\n",
            "Avg Accuracy: 25.000000%, Avg loss: 1.367772\n",
            "F1 score is: 0.099075216203928\n",
            "Confusion Matrix:\n",
            "[[  0   0   0 200]\n",
            " [  0   0   0 200]\n",
            " [  1   0   0 199]\n",
            " [  0   0   0 200]]\n",
            "current epoch: 10\n",
            "\n",
            "Avg Accuracy: 25.000000%, Avg loss: 1.367037\n",
            "F1 score is: 0.09877199321985244\n",
            "Confusion Matrix:\n",
            "[[  0   0   0 200]\n",
            " [  0   0   0 200]\n",
            " [  1   0   0 199]\n",
            " [  0   0   0 200]]\n",
            "current epoch: 11\n",
            "\n",
            "Avg Accuracy: 25.000000%, Avg loss: 1.366251\n",
            "F1 score is: 0.09786063387989997\n",
            "Confusion Matrix:\n",
            "[[  0   0   0 200]\n",
            " [  0   0   0 200]\n",
            " [  0   0   0 200]\n",
            " [  0   0   0 200]]\n",
            "current epoch: 12\n",
            "\n",
            "Avg Accuracy: 25.000000%, Avg loss: 1.365444\n",
            "F1 score is: 0.0981482706964016\n",
            "Confusion Matrix:\n",
            "[[  0   0   0 200]\n",
            " [  0   0   0 200]\n",
            " [  1   0   0 199]\n",
            " [  0   0   0 200]]\n",
            "current epoch: 13\n",
            "\n",
            "Avg Accuracy: 25.375000%, Avg loss: 1.364617\n",
            "F1 score is: 0.10616004262119531\n",
            "Confusion Matrix:\n",
            "[[  3   0   0 197]\n",
            " [  1   0   0 199]\n",
            " [  2   0   0 198]\n",
            " [  0   0   0 200]]\n",
            "current epoch: 14\n",
            "\n",
            "Avg Accuracy: 25.375000%, Avg loss: 1.363823\n",
            "F1 score is: 0.1026379656791687\n",
            "Confusion Matrix:\n",
            "[[  3   0   0 197]\n",
            " [  1   0   0 199]\n",
            " [  2   0   0 198]\n",
            " [  0   0   0 200]]\n",
            "current epoch: 15\n",
            "\n",
            "Avg Accuracy: 30.625000%, Avg loss: 1.362856\n",
            "F1 score is: 0.17842315461486577\n",
            "Confusion Matrix:\n",
            "[[ 45   0   0 155]\n",
            " [  6   0   0 194]\n",
            " [  5   0   0 195]\n",
            " [  0   0   0 200]]\n",
            "current epoch: 16\n",
            "\n",
            "Avg Accuracy: 28.625000%, Avg loss: 1.361705\n",
            "F1 score is: 0.14584549512714148\n",
            "Confusion Matrix:\n",
            "[[ 29   0   0 171]\n",
            " [  4   0   0 196]\n",
            " [  2   0   0 198]\n",
            " [  0   0   0 200]]\n",
            "current epoch: 17\n",
            "\n",
            "Avg Accuracy: 31.125000%, Avg loss: 1.360748\n",
            "F1 score is: 0.18415149323642255\n",
            "Confusion Matrix:\n",
            "[[ 49   0   0 151]\n",
            " [  8   0   0 192]\n",
            " [  4   0   0 196]\n",
            " [  0   0   0 200]]\n",
            "current epoch: 18\n",
            "\n",
            "Avg Accuracy: 34.875000%, Avg loss: 1.359909\n",
            "F1 score is: 0.2173633871972561\n",
            "Confusion Matrix:\n",
            "[[ 80   0   0 120]\n",
            " [ 16   0   0 184]\n",
            " [  8   0   0 192]\n",
            " [  1   0   0 199]]\n",
            "current epoch: 19\n",
            "\n",
            "Avg Accuracy: 35.000000%, Avg loss: 1.358703\n",
            "F1 score is: 0.23280615787953138\n",
            "Confusion Matrix:\n",
            "[[ 82   0   0 118]\n",
            " [ 17   0   0 183]\n",
            " [  9   0   0 191]\n",
            " [  2   0   0 198]]\n",
            "current epoch: 20\n",
            "\n",
            "Avg Accuracy: 35.375000%, Avg loss: 1.357498\n",
            "F1 score is: 0.2352529438585043\n",
            "Confusion Matrix:\n",
            "[[ 85   0   0 115]\n",
            " [ 17   0   0 183]\n",
            " [  9   0   0 191]\n",
            " [  2   0   0 198]]\n",
            "current epoch: 21\n",
            "\n",
            "Avg Accuracy: 34.625000%, Avg loss: 1.356264\n",
            "F1 score is: 0.22577501252293586\n",
            "Confusion Matrix:\n",
            "[[ 79   0   0 121]\n",
            " [ 16   0   0 184]\n",
            " [  7   0   0 193]\n",
            " [  2   0   0 198]]\n",
            "current epoch: 22\n",
            "\n",
            "Avg Accuracy: 38.000000%, Avg loss: 1.355168\n",
            "F1 score is: 0.2591143339127302\n",
            "Confusion Matrix:\n",
            "[[108   0   0  92]\n",
            " [ 26   0   0 174]\n",
            " [ 14   0   0 186]\n",
            " [  4   0   0 196]]\n",
            "current epoch: 23\n",
            "\n",
            "Avg Accuracy: 38.125000%, Avg loss: 1.353692\n",
            "F1 score is: 0.26153714433312414\n",
            "Confusion Matrix:\n",
            "[[110   0   0  90]\n",
            " [ 26   0   0 174]\n",
            " [ 15   0   0 185]\n",
            " [  5   0   0 195]]\n",
            "current epoch: 24\n",
            "\n",
            "Avg Accuracy: 37.000000%, Avg loss: 1.351709\n",
            "F1 score is: 0.2543828597664833\n",
            "Confusion Matrix:\n",
            "[[100   0   0 100]\n",
            " [ 24   0   0 176]\n",
            " [ 12   0   0 188]\n",
            " [  4   0   0 196]]\n",
            "current epoch: 25\n",
            "\n",
            "Avg Accuracy: 39.125000%, Avg loss: 1.350117\n",
            "F1 score is: 0.26904895171523097\n",
            "Confusion Matrix:\n",
            "[[118   0   0  82]\n",
            " [ 31   0   0 169]\n",
            " [ 17   0   0 183]\n",
            " [  5   0   0 195]]\n",
            "current epoch: 26\n",
            "\n",
            "Avg Accuracy: 40.375000%, Avg loss: 1.348512\n",
            "F1 score is: 0.27862015157938\n",
            "Confusion Matrix:\n",
            "[[131   0   0  69]\n",
            " [ 41   0   0 159]\n",
            " [ 21   0   0 179]\n",
            " [  8   0   0 192]]\n",
            "current epoch: 27\n",
            "\n",
            "Avg Accuracy: 38.875000%, Avg loss: 1.346386\n",
            "F1 score is: 0.2695601963996887\n",
            "Confusion Matrix:\n",
            "[[116   0   0  84]\n",
            " [ 29   0   0 171]\n",
            " [ 17   0   0 183]\n",
            " [  5   0   0 195]]\n",
            "current epoch: 28\n",
            "\n",
            "Avg Accuracy: 37.125000%, Avg loss: 1.344389\n",
            "F1 score is: 0.24138152584433556\n",
            "Confusion Matrix:\n",
            "[[101   0   0  99]\n",
            " [ 24   0   0 176]\n",
            " [ 12   0   0 188]\n",
            " [  4   0   0 196]]\n",
            "current epoch: 29\n",
            "\n",
            "Avg Accuracy: 40.000000%, Avg loss: 1.342215\n",
            "F1 score is: 0.2635254323482513\n",
            "Confusion Matrix:\n",
            "[[128   0   0  72]\n",
            " [ 37   0   0 163]\n",
            " [ 18   0   0 182]\n",
            " [  8   0   0 192]]\n",
            "best epoch is: 1\n",
            "Avg Accuracy: 38.808140%, Avg loss: 1.346909\n",
            "F1 score is: 0.3238936476842609\n",
            "Confusion Matrix:\n",
            "[[184   0   0 113]\n",
            " [ 44   0   0 280]\n",
            " [ 35   0   0 364]\n",
            " [  6   0   0 350]]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(1.3469085804251737,\n",
              " 0.3238936476842609,\n",
              " 0.38808139534883723,\n",
              " tensor([[184,   0,   0, 113],\n",
              "         [ 44,   0,   0, 280],\n",
              "         [ 35,   0,   0, 364],\n",
              "         [  6,   0,   0, 350]]))"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = \"cpu\"\n",
        "model = LeNet(padding=2, pooling=nn.MaxPool2d(kernel_size=2), activation_function=nn.ReLU()).to(device)\n",
        "cost_func = nn.CrossEntropyLoss()\n",
        "learning_rate = 0.0001  # Start with a lower learning rate\n",
        "#when using learning rate 0.002 we encountered the exploding gradients problem...\n",
        "num_epochs = 30\n",
        "batch_size = 16\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
        "model = train_and_evaluateCNN(num_epochs, optimizer, train_dataloader,val_dataloader,cost_func, model,device)\n",
        "evaluateCNN(test_dataloader, cost_func, model,device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Avg Accuracy: 38.808140%, Avg loss: 1.346909\n",
        "F1 score is: 0.3238936476842609\n",
        "\n",
        "The introduction of the ReLU activation function seems to affect the performance of our score negatively, which is something \n",
        "not expected. We attribute this unexpected behavior to the reduce learning rate we introduced above, as a compromise in order to \n",
        "avoid the exploding gradients problem we encountered during some runs.\n",
        "Normally, we would expect improved training performance, as ctivation functions contribute to a network's ability to generalize from the training data to unseen data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Id4jElye2D65"
      },
      "source": [
        "# Part 3 Improving Performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJ4pu937-7Kc"
      },
      "source": [
        "## Reproducibility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "XUrZTdFJdjPa"
      },
      "outputs": [],
      "source": [
        "meltraindata = np.load('/content/drive/MyDrive/Machine_Learning/music_genre_data_di/train/melgrams/X.npy')\n",
        "meltrainlabels = np.load('/content/drive/MyDrive/Machine_Learning/music_genre_data_di/train/melgrams/labels.npy')\n",
        "\n",
        "meltestdata = np.load('/content/drive/MyDrive/Machine_Learning/music_genre_data_di/test/melgrams/X.npy')\n",
        "meltestlabels = np.load('/content/drive/MyDrive/Machine_Learning/music_genre_data_di/test/melgrams/labels.npy')\n",
        "\n",
        "melvaldata = np.load('/content/drive/MyDrive/Machine_Learning/music_genre_data_di/val/melgrams/X.npy')\n",
        "melvallabels = np.load('/content/drive/MyDrive/Machine_Learning/music_genre_data_di/val/melgrams/labels.npy')\n",
        "\n",
        "\n",
        "training_data = dataset(meltraindata, meltrainlabels, labels_map, torch.tensor)\n",
        "val_data = dataset(melvaldata, melvallabels, labels_map, torch.tensor)\n",
        "test_data = dataset(meltestdata, meltestlabels, labels_map, torch.tensor)\n",
        "\n",
        "seed = 42 #the answer to life the universe and everything\n",
        "\n",
        "#Setting a seed method for dataloaders\n",
        "g_cpu = torch.Generator(device='cpu')\n",
        "train_dataloader = DataLoader(training_data, batch_size=16,shuffle=True, generator=g_cpu)\n",
        "val_dataloader = DataLoader(val_data, batch_size=16,shuffle=True,generator=g_cpu)\n",
        "# No need to seed the test loader\n",
        "test_dataloader = DataLoader(test_data, batch_size=16,shuffle=False)\n",
        "\n",
        "\n",
        "\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "\n",
        "#Define the set_epoch function\n",
        "def set_epoch(self, epoch):\n",
        "    self.epoch = epoch\n",
        "    if self.shuffle:\n",
        "        self.generator.manual_seed(2147483647 + self.epoch)\n",
        "\n",
        "#Bind the set_epoch function to the DataLoader instances\n",
        "train_dataloader.shuffle = True\n",
        "train_dataloader.set_epoch = types.MethodType(set_epoch, train_dataloader)\n",
        "val_dataloader.shuffle = True\n",
        "val_dataloader.set_epoch = types.MethodType(set_epoch, val_dataloader)\n",
        "\n",
        "\n",
        "\n",
        "#Ensure that the results are reproducible when using GPU operations with cuDNN\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_k4fOkrgcKH",
        "outputId": "ebb86a31-cf9d-45e1-8db8-f54253264f2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "current epoch: 0\n",
            "\n",
            "Sum of batch losses for epoch 0: 277.09355652332306\n",
            "current epoch: 1\n",
            "\n",
            "Sum of batch losses for epoch 1: 276.35214626789093\n",
            "current epoch: 2\n",
            "\n",
            "Sum of batch losses for epoch 2: 275.6562249660492\n",
            "current epoch: 3\n",
            "\n",
            "Sum of batch losses for epoch 3: 274.98939776420593\n",
            "current epoch: 4\n",
            "\n",
            "Sum of batch losses for epoch 4: 274.4251626729965\n",
            "Avg Accuracy: 37.281977%, Avg loss: 1.373262\n",
            "F1 score is: 0.31670496383205404\n",
            "Confusion Matrix:\n",
            "[[163   0   0 134]\n",
            " [ 33   0   0 291]\n",
            " [ 10   0   0 389]\n",
            " [  6   0   0 350]]\n",
            "current epoch: 0\n",
            "\n",
            "Sum of batch losses for epoch 0: 277.2186493873596\n",
            "current epoch: 1\n",
            "\n",
            "Sum of batch losses for epoch 1: 277.0618590116501\n",
            "current epoch: 2\n",
            "\n",
            "Sum of batch losses for epoch 2: 276.92426788806915\n",
            "current epoch: 3\n",
            "\n",
            "Sum of batch losses for epoch 3: 276.8097348213196\n",
            "current epoch: 4\n",
            "\n",
            "Sum of batch losses for epoch 4: 276.7086510658264\n",
            "Avg Accuracy: 21.947674%, Avg loss: 1.388278\n",
            "F1 score is: 0.1959238675704529\n",
            "Confusion Matrix:\n",
            "[[292   0   0   5]\n",
            " [324   0   0   0]\n",
            " [399   0   0   0]\n",
            " [346   0   0  10]]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(1.3882781655289407,\n",
              " 0.1959238675704529,\n",
              " 0.2194767441860465,\n",
              " tensor([[292,   0,   0,   5],\n",
              "         [324,   0,   0,   0],\n",
              "         [399,   0,   0,   0],\n",
              "         [346,   0,   0,  10]], device='cuda:0'))"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def trainCNN_reproductible(num_epochs, optimizer, dataloader,cost_func,model,device):\n",
        "  model.to(device)\n",
        "  model.train()\n",
        "  size = len(dataloader.dataset)\n",
        "  for epoch in range(num_epochs):\n",
        "    epoch_loss = 0\n",
        "    print(f\"current epoch: {epoch}\\n\")\n",
        "    dataloader.set_epoch(epoch)\n",
        "    for batch, (X,y) in enumerate(dataloader):\n",
        "      X = X.to(device)\n",
        "      y = y.to(device)\n",
        "      X = torch.unsqueeze(X,1)\n",
        "      pred = model(X.float())\n",
        "      loss = cost_func(pred,y)\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      epoch_loss += loss.item()\n",
        "    print(f\"Sum of batch losses for epoch {epoch}: {epoch_loss}\")\n",
        "  return model\n",
        "\n",
        "\n",
        "#Initialize the model with a manual seed\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "cost_func = nn.CrossEntropyLoss()\n",
        "learning_rate = 0.0001\n",
        "num_epochs = 5\n",
        "batch_size = 16\n",
        "\n",
        "model = LeNet(padding=2, pooling=nn.MaxPool2d(kernel_size=2), activation_function=nn.ReLU()).to(device)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
        "model = trainCNN_reproductible(num_epochs, optimizer, train_dataloader, cost_func, model,device)\n",
        "evaluateCNN(test_dataloader,cost_func,model,device)\n",
        "\n",
        "model = LeNet(padding=2, pooling=nn.MaxPool2d(kernel_size=2), activation_function=nn.ReLU()).to(device)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
        "model = trainCNN_reproductible(num_epochs, optimizer, train_dataloader, cost_func, model, device)\n",
        "evaluateCNN(test_dataloader,cost_func,model,device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We didn't manage to achieve reproductibility despite introducing custom seeds for all the libraries we used.\n",
        "Maybe there are other parameters we didn't take into account...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "le7ZQZ9---Bb"
      },
      "source": [
        "## Using Different Optimizers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33lzdURrn-sv",
        "outputId": "a9c02958-fea0-4f4b-8206-40ce57c85519"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training with Adadelta...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epochs: 100%|██████████| 30/30 [00:51<00:00,  1.73s/it, loss=1.37, f1=0.384, accuracy=0.484, best_f1=0.398, best_epoch=28]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best epoch is: 28\n",
            "Training with Adagrad...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epochs: 100%|██████████| 30/30 [00:49<00:00,  1.64s/it, loss=1.12, f1=0.578, accuracy=0.62, best_f1=0.578, best_epoch=29]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best epoch is: 29\n",
            "Training with Adam...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epochs: 100%|██████████| 30/30 [00:50<00:00,  1.67s/it, loss=0.00071, f1=0.739, accuracy=0.764, best_f1=0.742, best_epoch=25]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best epoch is: 25\n",
            "Training with AdamW...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epochs: 100%|██████████| 30/30 [00:50<00:00,  1.69s/it, loss=0.000697, f1=0.737, accuracy=0.765, best_f1=0.748, best_epoch=10]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best epoch is: 10\n",
            "Training with Adamax...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epochs: 100%|██████████| 30/30 [00:50<00:00,  1.68s/it, loss=0.619, f1=0.739, accuracy=0.76, best_f1=0.739, best_epoch=29]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best epoch is: 29\n",
            "Training with ASGD...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epochs: 100%|██████████| 30/30 [00:50<00:00,  1.69s/it, loss=0.865, f1=0.734, accuracy=0.756, best_f1=0.737, best_epoch=27]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best epoch is: 27\n",
            "Training with NAdam...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epochs: 100%|██████████| 30/30 [00:51<00:00,  1.72s/it, loss=0.000124, f1=0.751, accuracy=0.78, best_f1=0.759, best_epoch=24]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best epoch is: 24\n",
            "Training with RAdam...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epochs: 100%|██████████| 30/30 [00:51<00:00,  1.72s/it, loss=0.361, f1=0.632, accuracy=0.68, best_f1=0.752, best_epoch=28]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best epoch is: 28\n",
            "Training with RMSprop...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epochs: 100%|██████████| 30/30 [00:50<00:00,  1.68s/it, loss=0.00104, f1=0.709, accuracy=0.745, best_f1=0.735, best_epoch=14]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best epoch is: 14\n",
            "Training with Rprop...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epochs: 100%|██████████| 30/30 [00:53<00:00,  1.80s/it, loss=1.19, f1=0.596, accuracy=0.624, best_f1=0.61, best_epoch=26]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best epoch is: 26\n",
            "Training with SGD...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epochs: 100%|██████████| 30/30 [00:48<00:00,  1.60s/it, loss=1.28, f1=0.285, accuracy=0.42, best_f1=0.316, best_epoch=14]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best epoch is: 14\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "   Optimizer  F1 Score  Accuracy\n",
            "0   Adadelta  0.279180  0.449855\n",
            "1    Adagrad  0.310233  0.563227\n",
            "2       Adam  0.370985  0.724564\n",
            "3      AdamW  0.370639  0.741279\n",
            "4     Adamax  0.429098  0.742733\n",
            "5       ASGD  0.437783  0.747820\n",
            "6      NAdam  0.402230  0.742006\n",
            "7      RAdam  0.373598  0.676599\n",
            "8    RMSprop  0.381771  0.740552\n",
            "9      Rprop  0.263940  0.576308\n",
            "10       SGD  0.317775  0.402616\n"
          ]
        }
      ],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "\n",
        "def train_and_evaluateCNN(num_epochs, optimizer,train_dataloader,valid_dataloader,cost_func,model,device):\n",
        "  bestmodel = None\n",
        "  bestf1 = -1\n",
        "  bestepoch = -1\n",
        "  size = len(train_dataloader.dataset)\n",
        "  model.to(device)\n",
        "  epoch_progress = tqdm(range(num_epochs), desc=\"Epochs\")\n",
        "  for epoch in epoch_progress:\n",
        "    #print(f\"current epoch: {epoch}\\n\")\n",
        "    for batch, (X,y) in enumerate(train_dataloader):\n",
        "      X = X.to(device)\n",
        "      y = y.to(device)\n",
        "      X = torch.unsqueeze(X,1)\n",
        "      pred = model(X.float())\n",
        "      loss = cost_func(pred,y)\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      #Disable printing for this step, to save time\n",
        "      #loss, current = loss.item(), batch * len(X)\n",
        "      #print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "    tl,f1, acc, conf = evaluateCNN(valid_dataloader, cost_func, model,device)\n",
        "    if f1 > bestf1:\n",
        "      bestmodel = model\n",
        "      bestf1 = f1\n",
        "      bestepoch = epoch\n",
        "    # Update the tqdm progress bar with the current loss and metrics\n",
        "    epoch_progress.set_postfix({\n",
        "        \"loss\": loss.item(),\n",
        "        \"f1\": f1,\n",
        "        \"accuracy\": acc,\n",
        "        \"best_f1\": bestf1,\n",
        "        \"best_epoch\": bestepoch\n",
        "    })\n",
        "\n",
        "  print(f\"best epoch is: {bestepoch}\")\n",
        "  return bestmodel\n",
        "\n",
        "\n",
        "def evaluateCNN(dataloader, cost_func, model, device):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    test_loss = 0.0\n",
        "    test_f1 = 0.0\n",
        "    test_acc = 0.0\n",
        "    correct = 0\n",
        "    size = len(dataloader.dataset)\n",
        "    accuracy = Accuracy(task=\"multiclass\", num_classes=4).to(device)\n",
        "    f1 = F1Score(task=\"multiclass\", num_classes=4, average=\"macro\").to(device)\n",
        "    confmatrix = ConfusionMatrix(task=\"multiclass\", num_classes=4).to(device)\n",
        "\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch, (X, y) in enumerate(dataloader):\n",
        "            X = X.to(device).float()\n",
        "            y = y.to(device)\n",
        "            X = torch.unsqueeze(X,1)\n",
        "            pred = model(X)\n",
        "            all_preds.append(pred)\n",
        "            all_labels.append(y)\n",
        "\n",
        "            test_loss += cost_func(pred, y).item() * X.size(0)\n",
        "            test_acc += accuracy(pred, y).item() * X.size(0)\n",
        "            test_f1 += f1(pred, y).item() * X.size(0)\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    test_loss /= size\n",
        "    test_acc /= size\n",
        "    test_f1 /= size\n",
        "\n",
        "    all_preds = torch.cat(all_preds)\n",
        "    all_labels = torch.cat(all_labels)\n",
        "    confusion_matrix = confmatrix(all_preds.argmax(dim=1), all_labels)\n",
        "\n",
        "    # print(f\"Avg Accuracy: {100*test_acc:>8f}%, Avg loss: {test_loss:>8f}\")\n",
        "    # print(f\"F1 score is: {test_f1}\")\n",
        "    # print(\"Confusion Matrix:\")\n",
        "    # print(confusion_matrix.cpu().numpy())\n",
        "\n",
        "    return test_loss, test_f1, test_acc, confusion_matrix\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model = LeNet(padding=2, pooling=nn.MaxPool2d(kernel_size=2), activation_function=nn.ReLU()).to(device)\n",
        "\n",
        "# Save the initial state, we will use it later\n",
        "initial_state = model.state_dict()\n",
        "torch.save(initial_state, 'initial_state.pth')\n",
        "\n",
        "\n",
        "learning_rate = 0.0001  # Start with a lower learning rate\n",
        "cost_func = nn.CrossEntropyLoss()\n",
        "num_epochs = 30\n",
        "batch_size = 16\n",
        "\n",
        "\n",
        "# Define the optimizers to test\n",
        "optimizers = {\n",
        "  \"Adadelta\": torch.optim.Adadelta(model.parameters(), lr=learning_rate),\n",
        "  \"Adagrad\" : torch.optim.Adagrad(model.parameters(),lr=learning_rate),\n",
        "  \"Adam\": torch.optim.Adam(model.parameters(),lr=learning_rate),\n",
        "  \"AdamW\": torch.optim.AdamW(model.parameters(),lr=learning_rate),\n",
        "  \"Adamax\": torch.optim.Adamax(model.parameters(),lr=learning_rate),\n",
        "  \"ASGD\": torch.optim.Adamax(model.parameters(),lr=learning_rate),\n",
        "  \"NAdam\": torch.optim.NAdam(model.parameters(),lr=learning_rate),\n",
        "  \"RAdam\": torch.optim.RAdam(model.parameters(),lr=learning_rate),\n",
        "  \"RMSprop\": torch.optim.RMSprop(model.parameters(),lr=learning_rate),\n",
        "  \"RMSprop\": torch.optim.RMSprop(model.parameters(),lr=learning_rate),\n",
        "  \"Rprop\": torch.optim.Rprop(model.parameters(), lr = learning_rate),\n",
        "  \"SGD\": torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "}\n",
        "\n",
        "# Dictionary to store results\n",
        "results = {\n",
        "  \"Optimizer\": [],\n",
        "  \"F1 Score\": [],\n",
        "  \"Accuracy\": []\n",
        "}\n",
        "\n",
        "cost_func = nn.CrossEntropyLoss()\n",
        "for optimizer_name, optimizer in optimizers.items():\n",
        "  print(f\"Training with {optimizer_name}...\")\n",
        "  model = train_and_evaluateCNN(num_epochs, optimizer, train_dataloader,val_dataloader,cost_func, model,device)\n",
        "  test_loss, test_f1, test_acc, confmatrix = evaluateCNN(test_dataloader,cost_func,model,device)\n",
        "  results[\"Optimizer\"].append(optimizer_name)\n",
        "  results[\"F1 Score\"].append(test_f1)\n",
        "  results[\"Accuracy\"].append(test_acc)\n",
        "\n",
        "  # Restore the initial state\n",
        "  initial_state = torch.load('initial_state.pth')\n",
        "  model.load_state_dict(initial_state)\n",
        "\n",
        "import pandas as pd\n",
        "print(\"\\n\\n\\n\")\n",
        "df = pd.DataFrame(results)\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We observe big differences in both F1 score and accuracy by using different optimizers. We conclude that choosing the correct optimizer is extremely important for the performance of our model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZoX2hQ8c_POd"
      },
      "source": [
        "## Batch Normalization and Regularization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5FcdmJif_qkT",
        "outputId": "af1aec44-47eb-464d-b650-38414e46f68e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LeNet(\n",
            "  (conv1): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv2): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv4): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (bn4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (linear1): Linear(in_features=71680, out_features=1024, bias=True)\n",
            "  (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "  (linear3): Linear(in_features=256, out_features=32, bias=True)\n",
            "  (linear4): Linear(in_features=32, out_features=4, bias=True)\n",
            "  (dropout): Dropout(p=0, inplace=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "\n",
        "class LeNet(nn.Module):\n",
        "  def __init__(self,per=0, padding=0, pooling=None, activation_function=None):\n",
        "      super(LeNet, self).__init__()\n",
        "      channels, height, width = (1,21, 128) #Mel Spectrograms input dimensions\n",
        "      self.activation_function = activation_function\n",
        "      self.pooling = pooling\n",
        "      self._conv_shape = None\n",
        "\n",
        "      self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5, stride=1, padding=padding)\n",
        "      self.bn1 = nn.BatchNorm2d(16)\n",
        "      self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=padding)\n",
        "      self.bn2 = nn.BatchNorm2d(32)\n",
        "      self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5, stride=1, padding=padding)\n",
        "      self.bn3 = nn.BatchNorm2d(64)\n",
        "      self.conv4 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=5, stride=1, padding=padding)\n",
        "      self.bn4 = nn.BatchNorm2d(128)\n",
        "\n",
        "      self._conv_layer_output_shape((channels, height, width)) # Calculate Convolutional Layer output\n",
        "\n",
        "      self.linear1 = nn.Linear(self._conv_shape, 1024)\n",
        "      self.linear2 = nn.Linear(1024, 256)\n",
        "      self.linear3 = nn.Linear(256,32)\n",
        "      self.linear4 = nn.Linear(32,4)\n",
        "      self.dropout = nn.Dropout(p=per)\n",
        "\n",
        "\n",
        "  def _conv_layer_output_shape(self, shape):\n",
        "    x = torch.rand(1, *shape) # dummy\n",
        "    x = self._convolutional_layers(x)\n",
        "    self._conv_shape = x.numel()\n",
        "\n",
        "  def _convolutional_layers(self,x):\n",
        "    x = self.conv1(x)\n",
        "    x = self.bn1(x)\n",
        "    if self.activation_function:\n",
        "        x = self.activation_function(x)\n",
        "    if self.pooling:\n",
        "        x = self.pooling(x)\n",
        "\n",
        "    x = self.conv2(x)\n",
        "    x = self.bn2(x)\n",
        "    if self.activation_function:\n",
        "        x = self.activation_function(x)\n",
        "    if self.pooling:\n",
        "        x = self.pooling(x)\n",
        "\n",
        "    x = self.conv3(x)\n",
        "    x = self.bn3(x)\n",
        "    if self.activation_function:\n",
        "        x = self.activation_function(x)\n",
        "    if self.pooling:\n",
        "        x = self.pooling(x)\n",
        "\n",
        "    x = self.conv4(x)\n",
        "    x = self.bn4(x)\n",
        "    if self.activation_function:\n",
        "        x = self.activation_function(x)\n",
        "    if self.pooling:\n",
        "        x = self.pooling(x)\n",
        "    return x\n",
        "\n",
        "  def _linear_layers(self,x):\n",
        "    x = self.linear1(x)\n",
        "    if self.activation_function:\n",
        "      x = self.activation_function(x)\n",
        "    x = self.dropout(x)\n",
        "    x = self.linear2(x)\n",
        "    if self.activation_function:\n",
        "      x = self.activation_function(x)\n",
        "    x = self.dropout(x)\n",
        "    x = self.linear3(x)\n",
        "    if self.activation_function:\n",
        "      x = self.activation_function(x)\n",
        "    x = self.dropout(x)\n",
        "    x = self.linear4(x)\n",
        "    x = self.dropout(x)\n",
        "    return x\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self._convolutional_layers(x)\n",
        "    x = torch.flatten(x,1)\n",
        "    x = self._linear_layers(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "model = LeNet().to(device)\n",
        "print(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0yExcsVPM0w8",
        "outputId": "6d6693a8-5ee0-4d2a-b79d-1cafac479a26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epochs: 30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epochs: 100%|██████████| 30/30 [01:00<00:00,  2.03s/it, loss=0.721, f1=0.613, accuracy=0.667, best_f1=0.614, best_epoch=28]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best epoch is: 28\n",
            "For weight decay = 1e-08 and dropout = 0.0 we got:Accuracy for test: 0.6693313953488372, f1_score for test: 0.39178195963938567, Accuracy for val: 0.6675, f1_score for val: 0.6183820700645447\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epochs: 100%|██████████| 30/30 [00:59<00:00,  2.00s/it, loss=1.08, f1=0.596, accuracy=0.623, best_f1=0.596, best_epoch=29]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best epoch is: 29\n",
            "For weight decay = 1e-05 and dropout = 0.0 we got:Accuracy for test: 0.6366279069767442, f1_score for test: 0.32259735575508935, Accuracy for val: 0.6225, f1_score for val: 0.5976405817270279\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epochs: 100%|██████████| 30/30 [01:00<00:00,  2.02s/it, loss=1.07, f1=0.528, accuracy=0.585, best_f1=0.55, best_epoch=28]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best epoch is: 28\n",
            "For weight decay = 0.001 and dropout = 0.0 we got:Accuracy for test: 0.6053779069767442, f1_score for test: 0.2645465076446187, Accuracy for val: 0.585, f1_score for val: 0.5406985175609589\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epochs: 100%|██████████| 30/30 [01:00<00:00,  2.02s/it, loss=0.716, f1=0.621, accuracy=0.671, best_f1=0.621, best_epoch=29]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best epoch is: 29\n",
            "For weight decay = 1e-08 and dropout = 0.1 we got:Accuracy for test: 0.6613372093023255, f1_score for test: 0.37024985223488754, Accuracy for val: 0.67125, f1_score for val: 0.6277568119764328\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epochs: 100%|██████████| 30/30 [01:00<00:00,  2.03s/it, loss=0.797, f1=0.646, accuracy=0.698, best_f1=0.646, best_epoch=29]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best epoch is: 29\n",
            "For weight decay = 1e-05 and dropout = 0.1 we got:Accuracy for test: 0.6729651162790697, f1_score for test: 0.36567638554545334, Accuracy for val: 0.6975, f1_score for val: 0.6474262821674347\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epochs: 100%|██████████| 30/30 [01:00<00:00,  2.03s/it, loss=1.08, f1=0.541, accuracy=0.581, best_f1=0.545, best_epoch=28]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best epoch is: 28\n",
            "For weight decay = 0.001 and dropout = 0.1 we got:Accuracy for test: 0.6010174418604651, f1_score for test: 0.28639277493104687, Accuracy for val: 0.58125, f1_score for val: 0.5492351526021957\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epochs: 100%|██████████| 30/30 [01:00<00:00,  2.02s/it, loss=0.741, f1=0.601, accuracy=0.646, best_f1=0.601, best_epoch=29]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best epoch is: 29\n",
            "For weight decay = 1e-08 and dropout = 0.2 we got:Accuracy for test: 0.6526162790697675, f1_score for test: 0.3567135846025722, Accuracy for val: 0.64625, f1_score for val: 0.6114558529853821\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epochs: 100%|██████████| 30/30 [01:01<00:00,  2.03s/it, loss=0.959, f1=0.608, accuracy=0.631, best_f1=0.608, best_epoch=29]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best epoch is: 29\n",
            "For weight decay = 1e-05 and dropout = 0.2 we got:Accuracy for test: 0.6329941860465116, f1_score for test: 0.3315702095454515, Accuracy for val: 0.63125, f1_score for val: 0.5752386248111725\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epochs: 100%|██████████| 30/30 [01:00<00:00,  2.02s/it, loss=0.849, f1=0.581, accuracy=0.639, best_f1=0.591, best_epoch=27]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best epoch is: 27\n",
            "For weight decay = 0.001 and dropout = 0.2 we got:Accuracy for test: 0.627906976744186, f1_score for test: 0.3134835013513302, Accuracy for val: 0.63875, f1_score for val: 0.5852128410339356\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epochs: 100%|██████████| 30/30 [01:00<00:00,  2.03s/it, loss=0.72, f1=0.549, accuracy=0.614, best_f1=0.573, best_epoch=28]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best epoch is: 28\n",
            "For weight decay = 1e-08 and dropout = 0.3 we got:Accuracy for test: 0.625, f1_score for test: 0.31121536976722786, Accuracy for val: 0.61375, f1_score for val: 0.5777793949842454\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epochs: 100%|██████████| 30/30 [01:00<00:00,  2.03s/it, loss=0.907, f1=0.512, accuracy=0.573, best_f1=0.558, best_epoch=27]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best epoch is: 27\n",
            "For weight decay = 1e-05 and dropout = 0.3 we got:Accuracy for test: 0.6206395348837209, f1_score for test: 0.2960311759826402, Accuracy for val: 0.5725, f1_score for val: 0.5212267524003983\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epochs: 100%|██████████| 30/30 [01:00<00:00,  2.02s/it, loss=0.739, f1=0.53, accuracy=0.584, best_f1=0.53, best_epoch=29]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best epoch is: 29\n",
            "For weight decay = 0.001 and dropout = 0.3 we got:Accuracy for test: 0.6090116279069767, f1_score for test: 0.28805788729859644, Accuracy for val: 0.58375, f1_score for val: 0.5329331606626511\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epochs: 100%|██████████| 30/30 [01:00<00:00,  2.03s/it, loss=1.05, f1=0.654, accuracy=0.685, best_f1=0.654, best_epoch=29]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best epoch is: 29\n",
            "For weight decay = 1e-08 and dropout = 0.4 we got:Accuracy for test: 0.6475290697674418, f1_score for test: 0.37337052659673053, Accuracy for val: 0.685, f1_score for val: 0.6358650410175324\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epochs: 100%|██████████| 30/30 [01:00<00:00,  2.02s/it, loss=0.871, f1=0.543, accuracy=0.599, best_f1=0.555, best_epoch=28]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best epoch is: 28\n",
            "For weight decay = 1e-05 and dropout = 0.4 we got:Accuracy for test: 0.6293604651162791, f1_score for test: 0.2896480547090949, Accuracy for val: 0.59875, f1_score for val: 0.5490181016921997\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epochs: 100%|██████████| 30/30 [01:00<00:00,  2.03s/it, loss=0.739, f1=0.603, accuracy=0.634, best_f1=0.603, best_epoch=29]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best epoch is: 29\n",
            "For weight decay = 0.001 and dropout = 0.4 we got:Accuracy for test: 0.6162790697674418, f1_score for test: 0.290662839922101, Accuracy for val: 0.63375, f1_score for val: 0.6060764318704606\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epochs: 100%|██████████| 30/30 [01:00<00:00,  2.00s/it, loss=1.06, f1=0.536, accuracy=0.593, best_f1=0.545, best_epoch=28]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best epoch is: 28\n",
            "For weight decay = 0.0 and dropout = 0.1 we got:Accuracy for test: 0.5930232558139535, f1_score for test: 0.2697461252209059, Accuracy for val: 0.5925, f1_score for val: 0.5530495554208755\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epochs: 100%|██████████| 30/30 [01:00<00:00,  2.01s/it, loss=0.924, f1=0.564, accuracy=0.604, best_f1=0.574, best_epoch=28]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best epoch is: 28\n",
            "For weight decay = 0.0 and dropout = 0.2 we got:Accuracy for test: 0.6177325581395349, f1_score for test: 0.3000917559229704, Accuracy for val: 0.60375, f1_score for val: 0.5684790730476379\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epochs: 100%|██████████| 30/30 [01:00<00:00,  2.02s/it, loss=0.951, f1=0.593, accuracy=0.652, best_f1=0.593, best_epoch=29]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best epoch is: 29\n",
            "For weight decay = 0.0 and dropout = 0.3 we got:Accuracy for test: 0.6380813953488372, f1_score for test: 0.32397186175681825, Accuracy for val: 0.6525, f1_score for val: 0.5956343364715576\n",
            "epochs: 60\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epochs: 100%|██████████| 60/60 [02:01<00:00,  2.02s/it, loss=0.691, f1=0.708, accuracy=0.741, best_f1=0.74, best_epoch=49]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best epoch is: 49\n",
            "For weight decay = 1e-08 and dropout = 0.0 we got:Accuracy for test: 0.7049418604651163, f1_score for test: 0.4052187853687724, Accuracy for val: 0.74125, f1_score for val: 0.7153729444742203\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epochs: 100%|██████████| 60/60 [02:01<00:00,  2.02s/it, loss=0.725, f1=0.728, accuracy=0.761, best_f1=0.747, best_epoch=54]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best epoch is: 54\n",
            "For weight decay = 1e-05 and dropout = 0.0 we got:Accuracy for test: 0.6954941860465116, f1_score for test: 0.3898842571259931, Accuracy for val: 0.76125, f1_score for val: 0.7269630038738251\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epochs: 100%|██████████| 60/60 [02:01<00:00,  2.03s/it, loss=0.402, f1=0.741, accuracy=0.771, best_f1=0.741, best_epoch=59]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best epoch is: 59\n",
            "For weight decay = 0.001 and dropout = 0.0 we got:Accuracy for test: 0.7267441860465116, f1_score for test: 0.4078585099168988, Accuracy for val: 0.77125, f1_score for val: 0.7369599843025207\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epochs: 100%|██████████| 60/60 [02:01<00:00,  2.03s/it, loss=0.48, f1=0.733, accuracy=0.767, best_f1=0.752, best_epoch=57]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best epoch is: 57\n",
            "For weight decay = 1e-08 and dropout = 0.1 we got:Accuracy for test: 0.7034883720930233, f1_score for test: 0.4009015319522384, Accuracy for val: 0.7675, f1_score for val: 0.7417752522230149\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epochs: 100%|██████████| 60/60 [02:01<00:00,  2.03s/it, loss=0.52, f1=0.726, accuracy=0.749, best_f1=0.726, best_epoch=59]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best epoch is: 59\n",
            "For weight decay = 1e-05 and dropout = 0.1 we got:Accuracy for test: 0.7063953488372093, f1_score for test: 0.3950659596729417, Accuracy for val: 0.74875, f1_score for val: 0.7304193311929703\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epochs: 100%|██████████| 60/60 [02:00<00:00,  2.01s/it, loss=0.626, f1=0.7, accuracy=0.739, best_f1=0.703, best_epoch=49]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best epoch is: 49\n",
            "For weight decay = 0.001 and dropout = 0.1 we got:Accuracy for test: 0.7027616279069767, f1_score for test: 0.42496346645490374, Accuracy for val: 0.73875, f1_score for val: 0.6990505504608154\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epochs: 100%|██████████| 60/60 [02:00<00:00,  2.01s/it, loss=0.811, f1=0.708, accuracy=0.741, best_f1=0.729, best_epoch=57]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best epoch is: 57\n",
            "For weight decay = 1e-08 and dropout = 0.2 we got:Accuracy for test: 0.7165697674418605, f1_score for test: 0.40434726550741945, Accuracy for val: 0.74125, f1_score for val: 0.6991897594928741\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epochs: 100%|██████████| 60/60 [02:00<00:00,  2.01s/it, loss=0.343, f1=0.698, accuracy=0.751, best_f1=0.752, best_epoch=56]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best epoch is: 56\n",
            "For weight decay = 1e-05 and dropout = 0.2 we got:Accuracy for test: 0.7194767441860465, f1_score for test: 0.41977493971759494, Accuracy for val: 0.75125, f1_score for val: 0.7122550356388092\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epochs: 100%|██████████| 60/60 [02:01<00:00,  2.02s/it, loss=0.606, f1=0.732, accuracy=0.762, best_f1=0.747, best_epoch=52]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best epoch is: 52\n",
            "For weight decay = 0.001 and dropout = 0.2 we got:Accuracy for test: 0.7325581395348837, f1_score for test: 0.40692081185447615, Accuracy for val: 0.7625, f1_score for val: 0.7211674565076828\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epochs: 100%|██████████| 60/60 [02:00<00:00,  2.02s/it, loss=0.316, f1=0.75, accuracy=0.769, best_f1=0.75, best_epoch=59]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best epoch is: 59\n",
            "For weight decay = 1e-08 and dropout = 0.3 we got:Accuracy for test: 0.7194767441860465, f1_score for test: 0.40356062112246144, Accuracy for val: 0.76875, f1_score for val: 0.7403958261013031\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epochs: 100%|██████████| 60/60 [02:01<00:00,  2.02s/it, loss=0.47, f1=0.653, accuracy=0.716, best_f1=0.735, best_epoch=58]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best epoch is: 58\n",
            "For weight decay = 1e-05 and dropout = 0.3 we got:Accuracy for test: 0.684593023255814, f1_score for test: 0.40068858121196893, Accuracy for val: 0.71625, f1_score for val: 0.6485803550481797\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epochs: 100%|██████████| 60/60 [02:00<00:00,  2.01s/it, loss=0.803, f1=0.718, accuracy=0.761, best_f1=0.724, best_epoch=48]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best epoch is: 48\n",
            "For weight decay = 0.001 and dropout = 0.3 we got:Accuracy for test: 0.7136627906976745, f1_score for test: 0.39630415091334387, Accuracy for val: 0.76125, f1_score for val: 0.7212527120113372\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epochs: 100%|██████████| 60/60 [02:00<00:00,  2.01s/it, loss=0.766, f1=0.697, accuracy=0.73, best_f1=0.704, best_epoch=57]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best epoch is: 57\n",
            "For weight decay = 1e-08 and dropout = 0.4 we got:Accuracy for test: 0.6954941860465116, f1_score for test: 0.37719668885476365, Accuracy for val: 0.73, f1_score for val: 0.6898975163698197\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epochs: 100%|██████████| 60/60 [02:00<00:00,  2.02s/it, loss=0.401, f1=0.709, accuracy=0.738, best_f1=0.756, best_epoch=58]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best epoch is: 58\n",
            "For weight decay = 1e-05 and dropout = 0.4 we got:Accuracy for test: 0.7151162790697675, f1_score for test: 0.392798954707592, Accuracy for val: 0.7375, f1_score for val: 0.7151490688323975\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epochs: 100%|██████████| 60/60 [02:00<00:00,  2.01s/it, loss=0.487, f1=0.725, accuracy=0.762, best_f1=0.744, best_epoch=55]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best epoch is: 55\n",
            "For weight decay = 0.001 and dropout = 0.4 we got:Accuracy for test: 0.7180232558139535, f1_score for test: 0.41224935581517774, Accuracy for val: 0.7625, f1_score for val: 0.7295380383729935\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epochs: 100%|██████████| 60/60 [01:59<00:00,  2.00s/it, loss=0.71, f1=0.683, accuracy=0.709, best_f1=0.728, best_epoch=58]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best epoch is: 58\n",
            "For weight decay = 0.0 and dropout = 0.1 we got:Accuracy for test: 0.6875, f1_score for test: 0.4078753732231467, Accuracy for val: 0.70875, f1_score for val: 0.6704675191640854\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epochs: 100%|██████████| 60/60 [01:59<00:00,  2.00s/it, loss=0.494, f1=0.731, accuracy=0.759, best_f1=0.743, best_epoch=51]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best epoch is: 51\n",
            "For weight decay = 0.0 and dropout = 0.2 we got:Accuracy for test: 0.7056686046511628, f1_score for test: 0.4159141554128985, Accuracy for val: 0.75875, f1_score for val: 0.7173565196990966\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epochs: 100%|██████████| 60/60 [01:59<00:00,  2.00s/it, loss=0.802, f1=0.643, accuracy=0.7, best_f1=0.696, best_epoch=56]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best epoch is: 56\n",
            "For weight decay = 0.0 and dropout = 0.3 we got:Accuracy for test: 0.6933139534883721, f1_score for test: 0.39210464528133704, Accuracy for val: 0.7, f1_score for val: 0.6451506596803666\n"
          ]
        }
      ],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "cost_func = nn.CrossEntropyLoss()\n",
        "for num_epochs in [30,60]:\n",
        "  print(f\"epochs: {num_epochs}\")\n",
        "  for (per,wd) in [(0.0,1e-8),(0.0,1e-5),(0.0,1e-3),(0.1,1e-8), (0.1,1e-5), (0.1,1e-3), (0.2,1e-8), (0.2,1e-5), (0.2,1e-3), (0.3,1e-8),(0.3,1e-5), (0.3,1e-3),(0.4,1e-8), (0.4,1e-5), (0.4,1e-3), (0.1,0.0), (0.2,0.0), (0.3,0.0)]:\n",
        "    model = LeNet(per=per,padding=2, pooling=nn.MaxPool2d(kernel_size=2), activation_function=nn.ReLU()).to(device)\n",
        "    optimizer = torch.optim.ASGD(model.parameters(), lr = learning_rate, weight_decay= wd)\n",
        "    model = train_and_evaluateCNN(num_epochs, optimizer, train_dataloader,val_dataloader,cost_func,model, device)\n",
        "    test_loss, f1_t, acc_t, confmatrix = evaluateCNN(test_dataloader, cost_func, model,device)\n",
        "    test_loss, f1_v, acc_v, confmatrix = evaluateCNN(val_dataloader, cost_func, model,device)\n",
        "    print(f\"For weight decay = {wd} and dropout = {per} we got:Accuracy for test: {acc_t}, f1_score for test: {f1_t}, Accuracy for val: {acc_v}, f1_score for val: {f1_v}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tl-hy8qovRIj"
      },
      "source": [
        "\n",
        "Introducing dropout and weight decay to our model significantly boosts performance on the test data, as it increases our models ability to generalize to new data.\n",
        "When using those two techniques we reached an accuracy of 66% on the test data for 30 epoch training and an accuracy of 70% on the test data for 60 epochs.\n",
        "\n",
        "For weight decay = 0.001 and dropout = 0.1 we got:Accuracy for test: 0.7027616279069767, f1_score for test: 0.42496346645490374, Accuracy for val: 0.73875, f1_score for val: 0.6990505504608154\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpdYUEJSAFn7"
      },
      "source": [
        "# Part 4 Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3s7pX4CVDuTM"
      },
      "source": [
        "## Using Previous Parts to Select and Train a CNN model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_HvRr4mD8td",
        "outputId": "b60b6493-978a-4817-e89f-f01fa6d3a4fa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epochs: 100%|██████████| 60/60 [02:02<00:00,  2.04s/it, loss=0.545, f1=0.693, accuracy=0.739, best_f1=0.715, best_epoch=58]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best epoch is: 58\n"
          ]
        }
      ],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = LeNet(per=0.1,padding=2, pooling=nn.MaxPool2d(kernel_size=2), activation_function=nn.ReLU()).to(device)\n",
        "optimizer = torch.optim.ASGD(model.parameters(), lr = 0.0001, weight_decay=0.001)\n",
        "cost_func = nn.CrossEntropyLoss()\n",
        "model = train_and_evaluateCNN(60, optimizer, train_dataloader,val_dataloader,cost_func,model, device)\n",
        "test_loss, f1_t, acc_t, confmatrix = evaluateCNN(test_dataloader, cost_func, model,device)\n",
        "test_loss, f1_v, acc_v, confmatrix = evaluateCNN(val_dataloader, cost_func, model,device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AyKm19izBV4M"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x661iBuzAJeX",
        "outputId": "ac3da31d-a4e1-4d7c-eb00-bb8244f5e942"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'hiphop', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'hiphop', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'blues', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'hiphop', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'hiphop', 'classical', 'classical', 'classical', 'classical', 'classical', 'blues', 'classical', 'hiphop', 'classical', 'hiphop', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'hiphop', 'classical', 'classical', 'blues', 'classical', 'classical', 'classical', 'hiphop', 'classical', 'classical', 'classical', 'classical', 'hiphop', 'hiphop', 'classical', 'classical', 'hiphop', 'classical', 'classical', 'classical', 'classical', 'hiphop', 'classical', 'classical', 'classical', 'classical', 'classical', 'hiphop', 'classical', 'classical', 'classical', 'blues', 'classical', 'classical', 'classical', 'hiphop', 'classical', 'hiphop', 'classical', 'classical', 'hiphop', 'classical', 'classical', 'hiphop', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'hiphop', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical', 'classical']\n"
          ]
        }
      ],
      "source": [
        "def infrence(dataloader, model):\n",
        "  predictions = []\n",
        "  with torch.no_grad():\n",
        "    for batch, (X,y) in enumerate(dataloader):\n",
        "      X = X.to(device)\n",
        "      y = y.to(device)\n",
        "      X = torch.unsqueeze(X,1)\n",
        "      pred = model(X)\n",
        "      for i in range(pred.size(0)):  # tensor.size(0) gives the number of rows\n",
        "        row = pred[i]\n",
        "        predictions.append(opp_map[row.argmax().item()])\n",
        "\n",
        "  return predictions\n",
        "\n",
        "print(infrence(test_dataloader, model))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEIECk5GBpB7"
      },
      "source": [
        "## Downloading Music From Youtube"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gWs4PVQegsQM",
        "outputId": "5db83dc6-f98d-435a-a8ef-d6e5261ffb08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytube in /usr/local/lib/python3.10/dist-packages (15.0.0)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (0.25.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install pytube pydub\n",
        "import nbformat\n",
        "from pytube import YouTube\n",
        "from pydub import AudioSegment\n",
        "import librosa\n",
        "\n",
        "#Note: slightly modified the provided youtube_to_melgram.ipynb file so i can import\n",
        "#the functions to my notebook. I did not however modify the functions in any way\n",
        "\n",
        "\n",
        "# Path to the notebook file in your Google Drive\n",
        "notebook_path = '/content/drive/MyDrive/Machine_Learning/youtube_to_melgram.ipynb'\n",
        "\n",
        "# Load the notebook\n",
        "with open(notebook_path, 'r', encoding='utf-8') as f:\n",
        "    notebook = nbformat.read(f, as_version=4)\n",
        "function_names = ['load_wav','melspectrogram','get_melgrams','download_youtube','get_melgrams','youtube_to_melgram']\n",
        "for cell in notebook.cells:\n",
        "    if cell.cell_type == 'code':\n",
        "        code = cell.source\n",
        "        for name in function_names:\n",
        "          if name in code:\n",
        "              exec(code)  # Execute the function definition\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQJLKvicByCE"
      },
      "source": [
        "## Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        },
        "id": "J6vIWMlBGKHB",
        "outputId": "1681aa42-51e3-4a52-b32e-440b8f8bf43d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "classical\n",
            "Accuracy on Test dataloader is: 0.010638297872340425 and f1 score is: 0.008249427938292213\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "youtube_to_melgram(\"https://www.youtube.com/watch?v=TZCfydWF48c\")\n",
        "data = np.load('/content/youtube_melgrams.npy')\n",
        "labels = data.shape[0]*['blues'] #dummy value\n",
        "test = dataset(data, labels, labels_map, torch.tensor)\n",
        "test_dataloader = DataLoader(test, batch_size=16,shuffle=False)\n",
        "labels = infrence(test_dataloader, model)\n",
        "\n",
        "data = np.load('/content/youtube_melgrams.npy')\n",
        "actual = dataset(data, labels, labels_map, torch.tensor)\n",
        "actual_dataloader = DataLoader(actual, batch_size=16,shuffle=False)\n",
        "\n",
        "print(\"classical\")\n",
        "test_loss, f1_, acc_, confmatrix = evaluateCNN(test_dataloader,cost_func,model,device)\n",
        "plt.plot(infrence(actual_dataloader, model))\n",
        "print(f\"Accuracy on Test dataloader is: {acc_} and f1 score is: {f1_}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        },
        "id": "FVcyTsj_rocU",
        "outputId": "470fdbb0-d928-4e99-f734-c0bef9c03a30"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hiphop\n",
            "Accuracy on Test dataloader is: 0.018691588785046728 and f1 score is: 0.011728056123323529\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "data = youtube_to_melgram(\"https://www.youtube.com/watch?v=4EJzOk9csek\")\n",
        "data = np.load('/content/youtube_melgrams.npy')\n",
        "labels = data.shape[0]*['blues'] #dummy value\n",
        "test = dataset(data, labels, labels_map, torch.tensor)\n",
        "test_dataloader = DataLoader(test, batch_size=16,shuffle=False)\n",
        "labels = infrence(test_dataloader, model)\n",
        "\n",
        "data = np.load('/content/youtube_melgrams.npy')\n",
        "actual = dataset(data, labels, labels_map, torch.tensor)\n",
        "actual_dataloader = DataLoader(actual, batch_size=16,shuffle=False)\n",
        "\n",
        "print(\"hiphop\")\n",
        "test_loss, f1_, acc_, confmatrix = evaluateCNN(test_dataloader,cost_func,model,device)\n",
        "plt.plot(infrence(actual_dataloader, model))\n",
        "print(f\"Accuracy on Test dataloader is: {acc_} and f1 score is: {f1_}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        },
        "id": "UeaWyyD5GLkN",
        "outputId": "b43e31c9-af82-4198-a475-59458484513d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "rock\n",
            "Accuracy on Test dataloader is: 0.006369426751592357 and f1 score is: 0.005994754612066184\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "data = youtube_to_melgram(\"https://www.youtube.com/watch?v=L7UMubmfbH0\")\n",
        "data = np.load('/content/youtube_melgrams.npy')\n",
        "labels = data.shape[0]*['blues'] #dummy value\n",
        "test = dataset(data, labels, labels_map, torch.tensor)\n",
        "test_dataloader = DataLoader(test, batch_size=16,shuffle=False)\n",
        "labels = infrence(test_dataloader, model)\n",
        "\n",
        "data = np.load('/content/youtube_melgrams.npy')\n",
        "actual = dataset(data, labels, labels_map, torch.tensor)\n",
        "actual_dataloader = DataLoader(actual, batch_size=16,shuffle=False)\n",
        "\n",
        "print(\"rock\")\n",
        "test_loss, f1_, acc_, confmatrix = evaluateCNN(test_dataloader,cost_func,model,device)\n",
        "plt.plot(infrence(actual_dataloader, model))\n",
        "print(f\"Accuracy on Test dataloader is: {acc_} and f1 score is: {f1_}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        },
        "id": "RDE4LRXYGL2A",
        "outputId": "19c2a26f-4cac-41ec-9f22-aafb81e7d04d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "blues\n",
            "Accuracy on Test dataloader is: 0.012738853503184714 and f1 score is: 0.008992132107922985\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "data = youtube_to_melgram(\"https://www.youtube.com/watch?v=l45f28PzfCI\")\n",
        "data = np.load('/content/youtube_melgrams.npy')\n",
        "labels = data.shape[0]*['blues'] #dummy value\n",
        "test = dataset(data, labels, labels_map, torch.tensor)\n",
        "test_dataloader = DataLoader(test, batch_size=16,shuffle=False)\n",
        "labels = infrence(test_dataloader, model)\n",
        "\n",
        "data = np.load('/content/youtube_melgrams.npy')\n",
        "actual = dataset(data, labels, labels_map, torch.tensor)\n",
        "actual_dataloader = DataLoader(actual, batch_size=16,shuffle=False)\n",
        "\n",
        "print(\"blues\")\n",
        "test_loss, f1_, acc_, confmatrix = evaluateCNN(test_dataloader,cost_func,model,device)\n",
        "plt.plot(infrence(actual_dataloader, model))\n",
        "print(f\"Accuracy on Test dataloader is: {acc_} and f1 score is: {f1_}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Overall we observe that our model is predesposed to predicting \"classical\" over any other genre of music for the sample songs provided.\n",
        "This can be attributed to bias during training or the overall modest performace a network achieves when tested with real world data (poor generalization).\n",
        "Perhaps better regularisation would fix the issues as well as a bigger batch sizes.\n",
        "We can conclude with certainty that Vivaldi composes classical music.\n",
        "Another conclusion we reach is that Terror X Crew (TXC) incorporates a classical flair into their tunes, demonstrating an exquisite taste in their background sampling choices.\n",
        "Sadly, I don't listen to a lot of blues songs so I used the recpmmended sample.\n",
        "Lastly, we can safely conclude that Voodoo Child is a \"classical\" song more than a rock song, truly a masterpiece for the ages."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
